{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepPulse Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOSSi4sTafnlTV5PJyMOG9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielray54/DeepPulse/blob/main/DeepPulse_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggw0W9-jwABw"
      },
      "source": [
        "# DeepPulse Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlvbszNNwY60",
        "outputId": "4c5b790a-475c-442f-ce64-4e61bc01117c"
      },
      "source": [
        "!pip install pickle5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pickle5 in /opt/conda/lib/python3.7/site-packages (0.0.12)\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAcl7MIQwlBS"
      },
      "source": [
        "#Import Necessary Packages\n",
        "import pickle5\n",
        "import tensorflow_probability as tfp \n",
        "tfd = tfp.distributions\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import random\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import scipy.signal as sci_sig\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from sklearn import preprocessing "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRmRsGqQwupV",
        "outputId": "a9792ec3-3bf8-4460-822e-760423af5811"
      },
      "source": [
        "print(\"Tensorflow Version: \", tf.__version__)\n",
        "print(\"Built with CUDA: \", tf.test.is_built_with_cuda())\n",
        "print(\"CPUs: \", tf.config.list_physical_devices('CPU'))\n",
        "print(\"GPUs: \", tf.config.list_physical_devices('GPU'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow Version:  2.7.0\n",
            "Built with CUDA:  True\n",
            "CPUs:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "GPUs:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-07 05:55:33.609186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-07 05:55:33.719392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-07 05:55:33.720291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5GSRRm5w5L9"
      },
      "source": [
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = sci_sig.butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = sci_sig.lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "def resample_signal(signal_data, curr_fs, new_fs):\n",
        "    num_secs_in_signal = len(signal_data)/curr_fs \n",
        "    num_samples_to_resample = round(num_secs_in_signal*new_fs)  \n",
        "    return sci_sig.resample(signal_data, num_samples_to_resample)\n",
        "\n",
        "def slidingWindow(sequence,winSize,step=1):\n",
        "    \"\"\"Returns a generator that will iterate through the defined chunks of input sequence. Input sequence must be sliceable.\"\"\"\n",
        "\n",
        "    # Verify the inputs\n",
        "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
        "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
        "    if step > winSize:\n",
        "        raise Exception(\"**ERROR** step must not be larger than winSize.\") \n",
        "    if winSize > len(sequence):\n",
        "        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n",
        "\n",
        "    # Pre-compute number of chunks to emit\n",
        "    numOfChunks = ((len(sequence)-winSize)/step)+1\n",
        "    # Do the work\n",
        "    for i in range(0,int(numOfChunks)*step,step):\n",
        "        yield sequence[i:i+winSize]\n",
        "\n",
        "def preprocess(signal_data, fs):\n",
        "  filtr_sig = butter_bandpass_filter(signal_data, 0.5, 4.5, fs, order=2)\n",
        "  filtd_sig_resample = resample_signal(filtr_sig, fs, 64)\n",
        "  normalise_sig = stats.zscore(filtd_sig_resample)\n",
        "  return normalise_sig\n",
        "\n",
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "\n",
        "def split_list(alist, wanted_parts=1):     \n",
        "  length = len(alist)     \n",
        "  return [alist[i*length // wanted_parts: (i+1)*length // wanted_parts]  for i in range(wanted_parts)]\n",
        "\n",
        "def loso(data):\n",
        "  SUBJECT = list(range(len(data)))\n",
        "  random.shuffle(SUBJECT, random.random)\n",
        "  FOLDS = len(SUBJECT)//3\n",
        "  sub_folds = split_list(SUBJECT, wanted_parts=FOLDS)\n",
        "  train_set = []\n",
        "  val_set = []\n",
        "  test_set = []\n",
        "  for x in range(len(sub_folds)):\n",
        "    train_val_test = sub_folds\n",
        "    val_test = train_val_test[x]\n",
        "    train = [num for num in train_val_test if num != val_test]\n",
        "    train = flatten(train)\n",
        "    for z in range(len(val_test)):\n",
        "      test = val_test[z]\n",
        "      val = [num for num in val_test if num != test]\n",
        "      train = random.sample(train, len(train))\n",
        "      train_set.append(train)\n",
        "      val_set.append(val)\n",
        "      test_set.append([test])\n",
        "  return train_set, val_set, test_set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzN1aNQyw7MV"
      },
      "source": [
        "infile = open(\"/home/jupyter/DaLia_Data\",'rb')\n",
        "DaLia_Data = pickle5.load(infile)\n",
        "infile.close()\n",
        "infile = open(\"/home/jupyter/BAMI_Data\",'rb')\n",
        "BAMI_Data = pickle5.load(infile)\n",
        "infile.close()\n",
        "infile = open(\"/home/jupyter/IEEE_Train_Data\",'rb')\n",
        "IEEE_Train_Data = pickle5.load(infile)\n",
        "infile.close()\n",
        "infile = open(\"/home/jupyter/IEEE_Test_Data\",'rb')\n",
        "IEEE_Test_Data = pickle5.load(infile)\n",
        "infile.close()\n",
        "IEEE_Data = IEEE_Train_Data + IEEE_Test_Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SehjX1eA0N3s"
      },
      "source": [
        "def preprocess_dataset(dataset):\n",
        "  preprocessed_inputs_ACC = []\n",
        "  preprocessed_inputs_PPG = []\n",
        "  truth = []\n",
        "  fs_dataset_PPG = dataset[0][\"PPG_fs\"]\n",
        "  fs_dataset_ACC = dataset[0][\"ACC_fs\"]\n",
        "  for idx, subject in enumerate(range(len(dataset))):\n",
        "    ppg1_signal_windowed = []\n",
        "    accx_signal_windowed = []\n",
        "    accy_signal_windowed = []\n",
        "    accz_signal_windowed = []\n",
        "\n",
        "    #---------------PPG------------------------------------------\n",
        "    ppg_signal = preprocess(dataset[subject][\"Raw_PPG_1\"], fs_dataset_PPG)\n",
        "    for i in slidingWindow(ppg_signal, 8*64, 2*64):\n",
        "      ppg1_signal_windowed.append(i)\n",
        "\n",
        "    accx_signal = preprocess(dataset[subject][\"Raw ACC_X\"], fs_dataset_ACC)\n",
        "    for i in slidingWindow(accx_signal, 8*64, 2*64):\n",
        "      accx_signal_windowed.append(i)\n",
        "\n",
        "    accy_signal = preprocess(dataset[subject][\"Raw ACC_Y\"], fs_dataset_ACC)\n",
        "    for i in slidingWindow(accy_signal, 8*64, 2*64):\n",
        "      accy_signal_windowed.append(i)\n",
        "\n",
        "    accz_signal = preprocess(dataset[subject][\"Raw ACC_Z\"], fs_dataset_ACC)\n",
        "    for i in slidingWindow(accz_signal, 8*64, 2*64):\n",
        "      accz_signal_windowed.append(i)\n",
        "\n",
        "    truth.append(dataset[subject][\"truth_values\"])\n",
        "    \n",
        "    ppg1_signal_windowed = np.array(ppg1_signal_windowed)\n",
        "    accx_signal_windowed = np.array(accx_signal_windowed)\n",
        "    accy_signal_windowed = np.array(accy_signal_windowed)\n",
        "    accz_signal_windowed = np.array(accz_signal_windowed)\n",
        "\n",
        "   \n",
        "    acc_signals_stacked = np.stack((accx_signal_windowed,\n",
        "                      accy_signal_windowed,accz_signal_windowed), axis=2)\n",
        "    ppg1_signal_windowed = np.expand_dims(ppg1_signal_windowed, axis=2)\n",
        "    preprocessed_inputs_ACC.append(acc_signals_stacked)\n",
        "    preprocessed_inputs_PPG.append(ppg1_signal_windowed)\n",
        "  return preprocessed_inputs_PPG, preprocessed_inputs_ACC, truth\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NLL(y, distr): \n",
        "  return -distr.log_prob(y) \n",
        "\n",
        "def logistic_sp(params): \n",
        "  return tfd.Logistic(loc=params[:,0:1], scale=1e-3 + tf.math.softplus(0.05 *params[:,1:2]))# both parameters are learnable"
      ],
      "metadata": {
        "id": "Hp2HEWigwvnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_DeepPulse():\n",
        "  keras.backend.clear_session()\n",
        "  DROP_RATE = 0.15\n",
        "  m = 0.9\n",
        "  MAX_POOL = True\n",
        "  training_do = True\n",
        "  paddingcnn = \"same\"\n",
        "  input_PPG = keras.Input(shape=(512, 1), name=\"PPG_Sigs\")\n",
        "  input_ACC = keras.Input(shape=(512, 3), name=\"ACC_Sigs\")\n",
        "  \n",
        "#=-==========================================================\n",
        "  x = layers.Conv1D(\n",
        "      64, \n",
        "      16, \n",
        "      use_bias=False,\n",
        "      padding=paddingcnn)(input_PPG)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.BatchNormalization(momentum=m)(x)\n",
        "  x = layers.MaxPool1D(2)(x)\n",
        "  x = layers.Dropout(DROP_RATE)(x, training=training_do)\n",
        "  \n",
        "  x = layers.Conv1D(\n",
        "      64, \n",
        "      16, \n",
        "      use_bias=False,\n",
        "     padding=paddingcnn)(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.BatchNormalization(momentum=m)(x)\n",
        "  x = layers.MaxPool1D(2)(x)\n",
        "  x = layers.Dropout(DROP_RATE)(x, training=training_do)\n",
        "  \n",
        "#=-==========================================================\n",
        "\n",
        "  y = layers.Conv1D(\n",
        "      64, \n",
        "      16,\n",
        "      use_bias=False,\n",
        "      padding=paddingcnn)(input_ACC)\n",
        "  y = layers.ReLU()(y)\n",
        "  y = layers.BatchNormalization(momentum=m)(y)\n",
        "  y = layers.MaxPool1D(2)(y)\n",
        "  y = layers.Dropout(DROP_RATE)(y, training=training_do)\n",
        "  \n",
        "  y = layers.Conv1D(\n",
        "      64, \n",
        "      16, \n",
        "      use_bias=False,\n",
        "      padding=paddingcnn)(y)\n",
        "  y = layers.ReLU()(y)\n",
        "  y = layers.BatchNormalization(momentum=m)(y)\n",
        "  y = layers.MaxPool1D(2)(y)\n",
        "  y = layers.Dropout(DROP_RATE)(y, training=training_do)\n",
        "  \n",
        "\n",
        "#=-==========================================================\n",
        "  x = layers.Concatenate(axis=2)([x, y])\n",
        "#=-==========================================================\n",
        "  \n",
        "  x = layers.Conv1D(\n",
        "      128, \n",
        "      16, \n",
        "      use_bias=False,\n",
        "      padding=paddingcnn)(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.BatchNormalization(momentum=m)(x)\n",
        "  x = layers.MaxPool1D(2)(x)\n",
        "  x = layers.Dropout(DROP_RATE)(x, training=training_do)\n",
        "\n",
        "  x = layers.Conv1D(\n",
        "      128, \n",
        "      16,\n",
        "      use_bias=False,\n",
        "      padding=paddingcnn)(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.BatchNormalization(momentum=m)(x)\n",
        "  x = layers.Dropout(DROP_RATE)(x, training=training_do)\n",
        "\n",
        "#=-==========================================================\n",
        "  x = layers.Bidirectional(layers.LSTM(32, time_major=False,\n",
        "      return_sequences=True))(x) \n",
        "  x = layers.Bidirectional(layers.LSTM(32,  time_major=False,\n",
        "      return_sequences=True))(x) \n",
        "#=-==========================================================\n",
        "\n",
        "  x = layers.Conv1D(\n",
        "       1, \n",
        "       16,\n",
        "       padding=paddingcnn)(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  \n",
        "  x = layers.Flatten()(x)\n",
        " \n",
        "  params = layers.Dense(2)(x)\n",
        "  \n",
        "  dist = tfp.layers.DistributionLambda(logistic_sp)(params)\n",
        "  OPT = tf.optimizers.Nadam()\n",
        "  DeepPulse = keras.Model(inputs = [input_PPG, input_ACC], outputs = dist, name=\"DeepPulse\")\n",
        " # model_monotoic_sd_mean = keras.Model(inputs=[input_PPG, input_ACC], outputs=dist.mean())\n",
        "  #model_monotoic_sd_sd = keras.Model(inputs=[input_PPG, input_ACC], outputs=dist.stddev())\n",
        "  DeepPulse.compile(loss=NLL, optimizer=OPT, metrics = ['mae'])\n",
        "  return DeepPulse"
      ],
      "metadata": {
        "id": "kAiNxZ5d8cq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_DeepPulse().summary()"
      ],
      "metadata": {
        "id": "DKKkQf5Tn3W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aUa2e2T1iHe"
      },
      "source": [
        "def train_model_loso(DATASET, PPG_DATA, ACC_DATA, TRUTH, FOLDER):\n",
        "  eval_res = []\n",
        "  train_res = []\n",
        "  val_res = []\n",
        "  train_set_indexes, val_set_indexes, test_set_indexes = loso(DATASET)\n",
        "  for fold in range(len(train_set_indexes)):\n",
        "    train_PPG = [PPG_DATA[index] for index in train_set_indexes[fold]]\n",
        "    train_PPG = np.concatenate(train_PPG, axis=0)\n",
        "\n",
        "    train_ACC = [ACC_DATA[index] for index in train_set_indexes[fold]]\n",
        "    train_ACC = np.concatenate(train_ACC, axis=0)\n",
        "\n",
        "    train_truth = [TRUTH[index] for index in train_set_indexes[fold]]\n",
        "    train_truth = np.concatenate(train_truth, axis=0)\n",
        "\n",
        "    val_PPG = [PPG_DATA[valindex] for valindex in val_set_indexes[fold]]\n",
        "    val_PPG = np.concatenate(val_PPG, axis=0)\n",
        "\n",
        "    val_ACC = [ACC_DATA[valindex] for valindex in val_set_indexes[fold]]\n",
        "    val_ACC = np.concatenate(val_ACC, axis=0)\n",
        "\n",
        "    val_truth = [TRUTH[valindex] for valindex in val_set_indexes[fold]]\n",
        "    val_truth = np.concatenate(val_truth, axis=0)\n",
        "\n",
        "    test_PPG = [PPG_DATA[testindex] for testindex in test_set_indexes[fold]]\n",
        "    test_PPG = np.concatenate(test_PPG, axis=0)\n",
        "\n",
        "    test_ACC = [ACC_DATA[testindex] for testindex in test_set_indexes[fold]]\n",
        "    test_ACC = np.concatenate(test_ACC, axis=0)\n",
        "\n",
        "    test_truth = [TRUTH[testindex] for testindex in test_set_indexes[fold]]\n",
        "    test_truth = np.concatenate(test_truth, axis=0)\n",
        "\n",
        "    es=tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\", patience=30,\n",
        "                                     verbose=1,  restore_best_weights=True)\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        '/home/jupyter/'+str(FOLDER)+'/fold'+str(fold)+'.h5', \n",
        "        monitor='val_loss', verbose=1, \n",
        "        save_best_only=True, \n",
        "        mode='min')\n",
        "    rlronp=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", \n",
        "                                                factor=0.5, patience=2,\n",
        "                                                verbose=1)\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"/home/jupyter/\"+str(FOLDER)+\"_logs\"+\"/\"+str(fold)+\"/\", histogram_freq=1)\n",
        "    model = make_DeepPulse()\n",
        "    print(\"Fit model on training data fold \", fold)\n",
        "    history = model.fit(\n",
        "        [train_PPG, train_ACC],\n",
        "        train_truth,\n",
        "        batch_size=32,\n",
        "        epochs=200,\n",
        "        validation_data=([val_PPG, val_ACC], val_truth),\n",
        "            verbose=1,\n",
        "          callbacks=[es, checkpoint_callback, rlronp, tensorboard_callback])\n",
        "    model.load_weights('/home/jupyter/'+str(FOLDER)+'/fold'+str(fold)+'.h5')\n",
        "    eval_history = model.evaluate(\n",
        "        [test_PPG, test_ACC],\n",
        "        test_truth,\n",
        "        batch_size=32,verbose=1)\n",
        "    train_res.append(history.history['mae'])\n",
        "    val_res.append(history.history['val_mae'])\n",
        "    eval_res.append([eval_history[1]]*len(history.history['mae']))\n",
        "    \n",
        "    del history\n",
        "    del eval_history\n",
        "    #model.save('/home/jupyter/'+str(FOLDER)+'/fold'+str(fold)+'.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "    del model  # deletes the existing model\n",
        "\n",
        "  train_res = flatten(train_res)\n",
        "  val_res = flatten(val_res)\n",
        "  eval_res = flatten(eval_res)\n",
        "  plt.figure(figsize=(30,10))\n",
        "  plt.plot(train_res)\n",
        "  plt.plot(val_res)\n",
        "  plt.plot(eval_res)\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation', 'test'], loc='upper left')\n",
        "  #plt.yscale('log')\n",
        "  #plt.ylim([0, 25])\n",
        "  \n",
        "  plt.show()\n",
        "  print(\"Eval Mean\", np.mean(eval_res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IEEE_Data_PPG, IEEE_Data_ACC, IEEE_Data_truth = preprocess_dataset(IEEE_Data)\n",
        "train_model_loso(IEEE_Data, IEEE_Data_PPG, IEEE_Data_ACC, IEEE_Data_truth, \"IEEE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IB8qgPYpZftg",
        "outputId": "051a64a3-7048-4478-bdb8-b61c661b7766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fit model on training data fold  0\n",
            "Epoch 1/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 43.3337 - mae: 75.6397\n",
            "Epoch 00001: val_loss improved from inf to 4.53871, saving model to /home/jupyter/IEEE/fold0.h5\n",
            "84/84 [==============================] - 22s 134ms/step - loss: 43.3337 - mae: 75.6397 - val_loss: 4.5387 - val_mae: 21.0094 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 5.1129 - mae: 25.2565\n",
            "Epoch 00002: val_loss did not improve from 4.53871\n",
            "84/84 [==============================] - 8s 89ms/step - loss: 5.1129 - mae: 25.2565 - val_loss: 4.8485 - val_mae: 20.6867 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.5537 - mae: 20.8156\n",
            "Epoch 00003: val_loss improved from 4.53871 to 4.49164, saving model to /home/jupyter/IEEE/fold0.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 4.5537 - mae: 20.8156 - val_loss: 4.4916 - val_mae: 21.5726 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.3563 - mae: 19.2369\n",
            "Epoch 00004: val_loss improved from 4.49164 to 4.34224, saving model to /home/jupyter/IEEE/fold0.h5\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 4.3563 - mae: 19.2369 - val_loss: 4.3422 - val_mae: 18.4167 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.2281 - mae: 18.4685\n",
            "Epoch 00005: val_loss improved from 4.34224 to 4.30482, saving model to /home/jupyter/IEEE/fold0.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 4.2281 - mae: 18.4685 - val_loss: 4.3048 - val_mae: 18.2624 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.0450 - mae: 15.6980\n",
            "Epoch 00006: val_loss improved from 4.30482 to 4.21653, saving model to /home/jupyter/IEEE/fold0.h5\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 4.0450 - mae: 15.6980 - val_loss: 4.2165 - val_mae: 16.8602 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.0107 - mae: 15.6317\n",
            "Epoch 00007: val_loss did not improve from 4.21653\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 4.0107 - mae: 15.6317 - val_loss: 4.4969 - val_mae: 18.6821 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.9277 - mae: 14.6221\n",
            "Epoch 00008: val_loss improved from 4.21653 to 4.11172, saving model to /home/jupyter/IEEE/fold0.h5\n",
            "84/84 [==============================] - 8s 93ms/step - loss: 3.9277 - mae: 14.6221 - val_loss: 4.1117 - val_mae: 16.3276 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.7379 - mae: 12.5707\n",
            "Epoch 00009: val_loss improved from 4.11172 to 4.01011, saving model to /home/jupyter/IEEE/fold0.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.7379 - mae: 12.5707 - val_loss: 4.0101 - val_mae: 14.7234 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.7112 - mae: 11.8373\n",
            "Epoch 00010: val_loss did not improve from 4.01011\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 3.7112 - mae: 11.8373 - val_loss: 4.1329 - val_mae: 15.1691 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.6208 - mae: 10.9476\n",
            "Epoch 00011: val_loss did not improve from 4.01011\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.6208 - mae: 10.9476 - val_loss: 4.1232 - val_mae: 13.4013 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.3767 - mae: 9.1545\n",
            "Epoch 00012: val_loss did not improve from 4.01011\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.3767 - mae: 9.1545 - val_loss: 4.2509 - val_mae: 13.3406 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.3193 - mae: 8.4594\n",
            "Epoch 00013: val_loss did not improve from 4.01011\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.3193 - mae: 8.4594 - val_loss: 4.4940 - val_mae: 13.3261 - lr: 5.0000e-04\n",
            "Epoch 14/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.1867 - mae: 7.5142\n",
            "Epoch 00014: val_loss improved from 4.01011 to 3.89961, saving model to /home/jupyter/IEEE/fold0.h5\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 3.1867 - mae: 7.5142 - val_loss: 3.8996 - val_mae: 10.6278 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.1053 - mae: 7.2971\n",
            "Epoch 00015: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.1053 - mae: 7.2971 - val_loss: 4.1445 - val_mae: 11.2957 - lr: 2.5000e-04\n",
            "Epoch 16/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0442 - mae: 6.7927\n",
            "Epoch 00016: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.0442 - mae: 6.7927 - val_loss: 4.2318 - val_mae: 11.8003 - lr: 2.5000e-04\n",
            "Epoch 17/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9547 - mae: 6.1554\n",
            "Epoch 00017: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.9547 - mae: 6.1554 - val_loss: 4.3150 - val_mae: 11.1204 - lr: 1.2500e-04\n",
            "Epoch 18/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8932 - mae: 5.8412\n",
            "Epoch 00018: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.8932 - mae: 5.8412 - val_loss: 4.3710 - val_mae: 10.3147 - lr: 1.2500e-04\n",
            "Epoch 19/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8728 - mae: 5.7668\n",
            "Epoch 00019: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.8728 - mae: 5.7668 - val_loss: 4.5357 - val_mae: 11.1885 - lr: 6.2500e-05\n",
            "Epoch 20/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8234 - mae: 5.4420\n",
            "Epoch 00020: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.8234 - mae: 5.4420 - val_loss: 4.2699 - val_mae: 10.2808 - lr: 6.2500e-05\n",
            "Epoch 21/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8072 - mae: 5.4663\n",
            "Epoch 00021: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.8072 - mae: 5.4663 - val_loss: 4.4039 - val_mae: 10.2137 - lr: 3.1250e-05\n",
            "Epoch 22/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7684 - mae: 5.3122\n",
            "Epoch 00022: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7684 - mae: 5.3122 - val_loss: 4.4945 - val_mae: 10.5962 - lr: 3.1250e-05\n",
            "Epoch 23/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7408 - mae: 5.0599\n",
            "Epoch 00023: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.7408 - mae: 5.0599 - val_loss: 4.3746 - val_mae: 10.6703 - lr: 1.5625e-05\n",
            "Epoch 24/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7554 - mae: 5.1770\n",
            "Epoch 00024: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.7554 - mae: 5.1770 - val_loss: 4.4232 - val_mae: 10.1940 - lr: 1.5625e-05\n",
            "Epoch 25/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7447 - mae: 5.1964\n",
            "Epoch 00025: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7447 - mae: 5.1964 - val_loss: 4.4056 - val_mae: 9.4324 - lr: 7.8125e-06\n",
            "Epoch 26/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7309 - mae: 5.2157\n",
            "Epoch 00026: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.7309 - mae: 5.2157 - val_loss: 4.3942 - val_mae: 9.6003 - lr: 7.8125e-06\n",
            "Epoch 27/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7391 - mae: 5.0805\n",
            "Epoch 00027: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7391 - mae: 5.0805 - val_loss: 4.5190 - val_mae: 10.8457 - lr: 3.9063e-06\n",
            "Epoch 28/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7165 - mae: 5.0284\n",
            "Epoch 00028: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7165 - mae: 5.0284 - val_loss: 4.2753 - val_mae: 9.7349 - lr: 3.9063e-06\n",
            "Epoch 29/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7222 - mae: 4.9276\n",
            "Epoch 00029: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.7222 - mae: 4.9276 - val_loss: 4.4166 - val_mae: 10.3378 - lr: 1.9531e-06\n",
            "Epoch 30/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7190 - mae: 4.9475\n",
            "Epoch 00030: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7190 - mae: 4.9475 - val_loss: 4.3798 - val_mae: 9.6874 - lr: 1.9531e-06\n",
            "Epoch 31/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7240 - mae: 4.8635\n",
            "Epoch 00031: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7240 - mae: 4.8635 - val_loss: 4.6058 - val_mae: 10.1916 - lr: 9.7656e-07\n",
            "Epoch 32/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7303 - mae: 4.9883\n",
            "Epoch 00032: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.7303 - mae: 4.9883 - val_loss: 4.7808 - val_mae: 11.1812 - lr: 9.7656e-07\n",
            "Epoch 33/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7015 - mae: 4.8935\n",
            "Epoch 00033: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7015 - mae: 4.8935 - val_loss: 4.4787 - val_mae: 9.5520 - lr: 4.8828e-07\n",
            "Epoch 34/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7181 - mae: 5.0346\n",
            "Epoch 00034: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.7181 - mae: 5.0346 - val_loss: 4.6120 - val_mae: 10.4250 - lr: 4.8828e-07\n",
            "Epoch 35/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7136 - mae: 4.8991\n",
            "Epoch 00035: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7136 - mae: 4.8991 - val_loss: 4.5171 - val_mae: 10.5792 - lr: 2.4414e-07\n",
            "Epoch 36/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7107 - mae: 4.9945\n",
            "Epoch 00036: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.7107 - mae: 4.9945 - val_loss: 4.4496 - val_mae: 10.1296 - lr: 2.4414e-07\n",
            "Epoch 37/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7103 - mae: 4.9668\n",
            "Epoch 00037: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7103 - mae: 4.9668 - val_loss: 4.5396 - val_mae: 10.5920 - lr: 1.2207e-07\n",
            "Epoch 38/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7215 - mae: 4.9587\n",
            "Epoch 00038: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7215 - mae: 4.9587 - val_loss: 4.4374 - val_mae: 10.2889 - lr: 1.2207e-07\n",
            "Epoch 39/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7186 - mae: 4.9310\n",
            "Epoch 00039: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.7186 - mae: 4.9310 - val_loss: 4.5124 - val_mae: 10.5830 - lr: 6.1035e-08\n",
            "Epoch 40/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7143 - mae: 4.9921\n",
            "Epoch 00040: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7143 - mae: 4.9921 - val_loss: 4.2810 - val_mae: 9.8656 - lr: 6.1035e-08\n",
            "Epoch 41/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7097 - mae: 5.0851\n",
            "Epoch 00041: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.7097 - mae: 5.0851 - val_loss: 4.5049 - val_mae: 9.8681 - lr: 3.0518e-08\n",
            "Epoch 42/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7317 - mae: 4.9700\n",
            "Epoch 00042: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.7317 - mae: 4.9700 - val_loss: 4.6422 - val_mae: 10.6919 - lr: 3.0518e-08\n",
            "Epoch 43/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7048 - mae: 4.8288\n",
            "Epoch 00043: val_loss did not improve from 3.89961\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7048 - mae: 4.8288 - val_loss: 4.5871 - val_mae: 10.5743 - lr: 1.5259e-08\n",
            "Epoch 44/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7108 - mae: 4.9781Restoring model weights from the end of the best epoch: 14.\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 3.89961\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.7108 - mae: 4.9781 - val_loss: 4.3719 - val_mae: 10.0897 - lr: 1.5259e-08\n",
            "Epoch 00044: early stopping\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 3.0327 - mae: 7.5831\n",
            "Fit model on training data fold  1\n",
            "Epoch 1/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 83.6330 - mae: 105.1250\n",
            "Epoch 00001: val_loss improved from inf to 4.41591, saving model to /home/jupyter/IEEE/fold1.h5\n",
            "84/84 [==============================] - 21s 135ms/step - loss: 83.6330 - mae: 105.1250 - val_loss: 4.4159 - val_mae: 17.4101 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.2914 - mae: 17.1332\n",
            "Epoch 00002: val_loss improved from 4.41591 to 3.87724, saving model to /home/jupyter/IEEE/fold1.h5\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 4.2914 - mae: 17.1332 - val_loss: 3.8772 - val_mae: 14.3888 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.9201 - mae: 14.1821\n",
            "Epoch 00003: val_loss improved from 3.87724 to 3.76461, saving model to /home/jupyter/IEEE/fold1.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.9201 - mae: 14.1821 - val_loss: 3.7646 - val_mae: 12.6149 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.7390 - mae: 11.9899\n",
            "Epoch 00004: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.7390 - mae: 11.9899 - val_loss: 3.7700 - val_mae: 10.9018 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.5943 - mae: 10.2874\n",
            "Epoch 00005: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.5943 - mae: 10.2874 - val_loss: 3.8382 - val_mae: 9.9129 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.2368 - mae: 7.8504\n",
            "Epoch 00006: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.2368 - mae: 7.8504 - val_loss: 4.1359 - val_mae: 8.4096 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.1372 - mae: 6.8910\n",
            "Epoch 00007: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 3.1372 - mae: 6.8910 - val_loss: 3.9365 - val_mae: 7.8290 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9041 - mae: 6.0820\n",
            "Epoch 00008: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.9041 - mae: 6.0820 - val_loss: 4.3553 - val_mae: 7.1546 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8432 - mae: 5.4559\n",
            "Epoch 00009: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.8432 - mae: 5.4559 - val_loss: 4.0087 - val_mae: 6.8061 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.6671 - mae: 4.9162\n",
            "Epoch 00010: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.6671 - mae: 4.9162 - val_loss: 4.5250 - val_mae: 6.4315 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.6564 - mae: 4.7659\n",
            "Epoch 00011: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.6564 - mae: 4.7659 - val_loss: 4.5431 - val_mae: 6.4372 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5783 - mae: 4.5089\n",
            "Epoch 00012: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.5783 - mae: 4.5089 - val_loss: 4.5929 - val_mae: 6.5687 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4892 - mae: 4.2514\n",
            "Epoch 00013: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4892 - mae: 4.2514 - val_loss: 4.7142 - val_mae: 6.2160 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4508 - mae: 4.1120\n",
            "Epoch 00014: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4508 - mae: 4.1120 - val_loss: 4.7942 - val_mae: 6.2147 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4307 - mae: 4.0553\n",
            "Epoch 00015: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.4307 - mae: 4.0553 - val_loss: 4.9549 - val_mae: 6.5957 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4535 - mae: 4.0520\n",
            "Epoch 00016: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4535 - mae: 4.0520 - val_loss: 4.8110 - val_mae: 6.2192 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3988 - mae: 3.9432\n",
            "Epoch 00017: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.3988 - mae: 3.9432 - val_loss: 5.0608 - val_mae: 6.0125 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3956 - mae: 3.8804\n",
            "Epoch 00018: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.3956 - mae: 3.8804 - val_loss: 5.1078 - val_mae: 6.2969 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3969 - mae: 3.9096\n",
            "Epoch 00019: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.3969 - mae: 3.9096 - val_loss: 4.9961 - val_mae: 5.9049 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3856 - mae: 3.9604\n",
            "Epoch 00020: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.3856 - mae: 3.9604 - val_loss: 5.0045 - val_mae: 6.3474 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3687 - mae: 3.9758\n",
            "Epoch 00021: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.3687 - mae: 3.9758 - val_loss: 5.0301 - val_mae: 6.3145 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3844 - mae: 3.8927\n",
            "Epoch 00022: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.3844 - mae: 3.8927 - val_loss: 5.1864 - val_mae: 6.2522 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3626 - mae: 3.8715\n",
            "Epoch 00023: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.3626 - mae: 3.8715 - val_loss: 4.9530 - val_mae: 6.0870 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3730 - mae: 3.9846\n",
            "Epoch 00024: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.3730 - mae: 3.9846 - val_loss: 4.9253 - val_mae: 6.0428 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3478 - mae: 3.8441\n",
            "Epoch 00025: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.3478 - mae: 3.8441 - val_loss: 5.1142 - val_mae: 5.4203 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3450 - mae: 3.9040\n",
            "Epoch 00026: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.3450 - mae: 3.9040 - val_loss: 5.1532 - val_mae: 6.4557 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3576 - mae: 3.8341\n",
            "Epoch 00027: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.3576 - mae: 3.8341 - val_loss: 5.1369 - val_mae: 6.3732 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3574 - mae: 3.8911\n",
            "Epoch 00028: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.3574 - mae: 3.8911 - val_loss: 5.1000 - val_mae: 6.4581 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3746 - mae: 3.8420\n",
            "Epoch 00029: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.3746 - mae: 3.8420 - val_loss: 5.1539 - val_mae: 6.0096 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3505 - mae: 3.8339\n",
            "Epoch 00030: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.3505 - mae: 3.8339 - val_loss: 5.1166 - val_mae: 6.1606 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3474 - mae: 3.8726\n",
            "Epoch 00031: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.3474 - mae: 3.8726 - val_loss: 5.0833 - val_mae: 6.1767 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3703 - mae: 3.8111\n",
            "Epoch 00032: val_loss did not improve from 3.76461\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.3703 - mae: 3.8111 - val_loss: 5.1875 - val_mae: 6.2083 - lr: 6.1035e-08\n",
            "Epoch 33/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3728 - mae: 3.8548Restoring model weights from the end of the best epoch: 3.\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 3.76461\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.3728 - mae: 3.8548 - val_loss: 5.1739 - val_mae: 6.2111 - lr: 6.1035e-08\n",
            "Epoch 00033: early stopping\n",
            "4/4 [==============================] - 0s 73ms/step - loss: 4.2602 - mae: 16.4895\n",
            "Fit model on training data fold  2\n",
            "Epoch 1/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 56.5405 - mae: 69.1194\n",
            "Epoch 00001: val_loss improved from inf to 4.42827, saving model to /home/jupyter/IEEE/fold2.h5\n",
            "84/84 [==============================] - 21s 130ms/step - loss: 56.5405 - mae: 69.1194 - val_loss: 4.4283 - val_mae: 17.3062 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.9926 - mae: 20.8239\n",
            "Epoch 00002: val_loss improved from 4.42827 to 4.24776, saving model to /home/jupyter/IEEE/fold2.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 4.9926 - mae: 20.8239 - val_loss: 4.2478 - val_mae: 16.7544 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.4357 - mae: 17.8249\n",
            "Epoch 00003: val_loss improved from 4.24776 to 3.99170, saving model to /home/jupyter/IEEE/fold2.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 4.4357 - mae: 17.8249 - val_loss: 3.9917 - val_mae: 15.1755 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.1854 - mae: 15.7330\n",
            "Epoch 00004: val_loss did not improve from 3.99170\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 4.1854 - mae: 15.7330 - val_loss: 4.2472 - val_mae: 17.2673 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.9468 - mae: 14.1821\n",
            "Epoch 00005: val_loss did not improve from 3.99170\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 3.9468 - mae: 14.1821 - val_loss: 4.2168 - val_mae: 14.9237 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.8417 - mae: 13.0439\n",
            "Epoch 00006: val_loss improved from 3.99170 to 3.93232, saving model to /home/jupyter/IEEE/fold2.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 3.8417 - mae: 13.0439 - val_loss: 3.9323 - val_mae: 13.8614 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.6775 - mae: 11.5167\n",
            "Epoch 00007: val_loss did not improve from 3.93232\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.6775 - mae: 11.5167 - val_loss: 4.0158 - val_mae: 14.0176 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.6066 - mae: 11.0287\n",
            "Epoch 00008: val_loss did not improve from 3.93232\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.6066 - mae: 11.0287 - val_loss: 3.9667 - val_mae: 12.7058 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.5627 - mae: 10.6706\n",
            "Epoch 00009: val_loss did not improve from 3.93232\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.5627 - mae: 10.6706 - val_loss: 3.9858 - val_mae: 12.9309 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.4778 - mae: 9.9970\n",
            "Epoch 00010: val_loss improved from 3.93232 to 3.92397, saving model to /home/jupyter/IEEE/fold2.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 3.4778 - mae: 9.9970 - val_loss: 3.9240 - val_mae: 12.3556 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.4388 - mae: 9.7706\n",
            "Epoch 00011: val_loss did not improve from 3.92397\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.4388 - mae: 9.7706 - val_loss: 3.9688 - val_mae: 12.3330 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.4034 - mae: 9.2416\n",
            "Epoch 00012: val_loss improved from 3.92397 to 3.92106, saving model to /home/jupyter/IEEE/fold2.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 3.4034 - mae: 9.2416 - val_loss: 3.9211 - val_mae: 11.4874 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.3097 - mae: 8.6208\n",
            "Epoch 00013: val_loss improved from 3.92106 to 3.87301, saving model to /home/jupyter/IEEE/fold2.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.3097 - mae: 8.6208 - val_loss: 3.8730 - val_mae: 10.7833 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.2602 - mae: 8.3322\n",
            "Epoch 00014: val_loss did not improve from 3.87301\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.2602 - mae: 8.3322 - val_loss: 4.0000 - val_mae: 11.2472 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.3308 - mae: 8.0616\n",
            "Epoch 00015: val_loss did not improve from 3.87301\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 3.3308 - mae: 8.0616 - val_loss: 3.9333 - val_mae: 10.7639 - lr: 2.5000e-04\n",
            "Epoch 16/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.1856 - mae: 7.2695\n",
            "Epoch 00016: val_loss did not improve from 3.87301\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.1856 - mae: 7.2695 - val_loss: 3.8890 - val_mae: 9.7427 - lr: 1.2500e-04\n",
            "Epoch 17/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0924 - mae: 7.1087\n",
            "Epoch 00017: val_loss did not improve from 3.87301\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.0924 - mae: 7.1087 - val_loss: 4.0150 - val_mae: 10.7568 - lr: 1.2500e-04\n",
            "Epoch 18/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0562 - mae: 6.9011\n",
            "Epoch 00018: val_loss did not improve from 3.87301\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.0562 - mae: 6.9011 - val_loss: 4.0802 - val_mae: 10.3965 - lr: 6.2500e-05\n",
            "Epoch 19/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0270 - mae: 6.8049\n",
            "Epoch 00019: val_loss did not improve from 3.87301\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.0270 - mae: 6.8049 - val_loss: 3.9746 - val_mae: 9.8999 - lr: 6.2500e-05\n",
            "Epoch 20/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9884 - mae: 6.4738\n",
            "Epoch 00020: val_loss did not improve from 3.87301\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.9884 - mae: 6.4738 - val_loss: 3.9732 - val_mae: 10.0555 - lr: 3.1250e-05\n",
            "Epoch 21/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9663 - mae: 6.2994\n",
            "Epoch 00021: val_loss improved from 3.87301 to 3.87051, saving model to /home/jupyter/IEEE/fold2.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.9663 - mae: 6.2994 - val_loss: 3.8705 - val_mae: 9.3458 - lr: 3.1250e-05\n",
            "Epoch 22/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9781 - mae: 6.3259\n",
            "Epoch 00022: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.9781 - mae: 6.3259 - val_loss: 4.2361 - val_mae: 10.4944 - lr: 3.1250e-05\n",
            "Epoch 23/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9488 - mae: 6.1502\n",
            "Epoch 00023: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.9488 - mae: 6.1502 - val_loss: 4.1479 - val_mae: 10.5340 - lr: 3.1250e-05\n",
            "Epoch 24/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9164 - mae: 6.0884\n",
            "Epoch 00024: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.9164 - mae: 6.0884 - val_loss: 4.0999 - val_mae: 10.3669 - lr: 1.5625e-05\n",
            "Epoch 25/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8837 - mae: 5.8054\n",
            "Epoch 00025: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.8837 - mae: 5.8054 - val_loss: 4.1867 - val_mae: 10.2664 - lr: 1.5625e-05\n",
            "Epoch 26/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8841 - mae: 5.9413\n",
            "Epoch 00026: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.8841 - mae: 5.9413 - val_loss: 4.1766 - val_mae: 10.4804 - lr: 7.8125e-06\n",
            "Epoch 27/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8946 - mae: 5.9195\n",
            "Epoch 00027: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.8946 - mae: 5.9195 - val_loss: 4.2457 - val_mae: 10.4716 - lr: 7.8125e-06\n",
            "Epoch 28/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8967 - mae: 5.8998\n",
            "Epoch 00028: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.8967 - mae: 5.8998 - val_loss: 4.3220 - val_mae: 10.7281 - lr: 3.9063e-06\n",
            "Epoch 29/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8849 - mae: 5.8043\n",
            "Epoch 00029: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.8849 - mae: 5.8043 - val_loss: 4.1140 - val_mae: 10.0034 - lr: 3.9063e-06\n",
            "Epoch 30/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9005 - mae: 5.8186\n",
            "Epoch 00030: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.9005 - mae: 5.8186 - val_loss: 4.3168 - val_mae: 10.5826 - lr: 1.9531e-06\n",
            "Epoch 31/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8879 - mae: 5.8174\n",
            "Epoch 00031: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.8879 - mae: 5.8174 - val_loss: 4.1330 - val_mae: 9.9362 - lr: 1.9531e-06\n",
            "Epoch 32/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8663 - mae: 5.5577\n",
            "Epoch 00032: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.8663 - mae: 5.5577 - val_loss: 4.2219 - val_mae: 10.5112 - lr: 9.7656e-07\n",
            "Epoch 33/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8741 - mae: 5.7613\n",
            "Epoch 00033: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.8741 - mae: 5.7613 - val_loss: 4.3035 - val_mae: 10.7670 - lr: 9.7656e-07\n",
            "Epoch 34/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8631 - mae: 5.7601\n",
            "Epoch 00034: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.8631 - mae: 5.7601 - val_loss: 4.2252 - val_mae: 10.7300 - lr: 4.8828e-07\n",
            "Epoch 35/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8701 - mae: 5.6937\n",
            "Epoch 00035: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.8701 - mae: 5.6937 - val_loss: 4.3276 - val_mae: 11.6643 - lr: 4.8828e-07\n",
            "Epoch 36/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8835 - mae: 5.8661\n",
            "Epoch 00036: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.8835 - mae: 5.8661 - val_loss: 4.2269 - val_mae: 10.4032 - lr: 2.4414e-07\n",
            "Epoch 37/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8753 - mae: 5.7890\n",
            "Epoch 00037: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.8753 - mae: 5.7890 - val_loss: 4.2779 - val_mae: 11.2122 - lr: 2.4414e-07\n",
            "Epoch 38/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8699 - mae: 5.7322\n",
            "Epoch 00038: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.8699 - mae: 5.7322 - val_loss: 4.2059 - val_mae: 10.1349 - lr: 1.2207e-07\n",
            "Epoch 39/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8826 - mae: 5.7839\n",
            "Epoch 00039: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.8826 - mae: 5.7839 - val_loss: 4.1191 - val_mae: 10.6377 - lr: 1.2207e-07\n",
            "Epoch 40/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8897 - mae: 5.8317\n",
            "Epoch 00040: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.8897 - mae: 5.8317 - val_loss: 4.0773 - val_mae: 9.4421 - lr: 6.1035e-08\n",
            "Epoch 41/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8711 - mae: 5.7049\n",
            "Epoch 00041: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.8711 - mae: 5.7049 - val_loss: 4.1974 - val_mae: 10.3184 - lr: 6.1035e-08\n",
            "Epoch 42/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8724 - mae: 5.8278\n",
            "Epoch 00042: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.8724 - mae: 5.8278 - val_loss: 4.1647 - val_mae: 10.1886 - lr: 3.0518e-08\n",
            "Epoch 43/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8737 - mae: 5.6904\n",
            "Epoch 00043: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.8737 - mae: 5.6904 - val_loss: 4.3433 - val_mae: 10.6878 - lr: 3.0518e-08\n",
            "Epoch 44/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8764 - mae: 5.7498\n",
            "Epoch 00044: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.8764 - mae: 5.7498 - val_loss: 4.2570 - val_mae: 10.4972 - lr: 1.5259e-08\n",
            "Epoch 45/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8900 - mae: 5.8762\n",
            "Epoch 00045: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.8900 - mae: 5.8762 - val_loss: 4.3099 - val_mae: 11.0011 - lr: 1.5259e-08\n",
            "Epoch 46/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8606 - mae: 5.6464\n",
            "Epoch 00046: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.8606 - mae: 5.6464 - val_loss: 4.1024 - val_mae: 10.0306 - lr: 7.6294e-09\n",
            "Epoch 47/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8827 - mae: 5.6787\n",
            "Epoch 00047: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.8827 - mae: 5.6787 - val_loss: 4.4080 - val_mae: 11.6586 - lr: 7.6294e-09\n",
            "Epoch 48/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8785 - mae: 5.5673\n",
            "Epoch 00048: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.8785 - mae: 5.5673 - val_loss: 4.3626 - val_mae: 10.4975 - lr: 3.8147e-09\n",
            "Epoch 49/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8790 - mae: 5.7296\n",
            "Epoch 00049: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.8790 - mae: 5.7296 - val_loss: 4.2395 - val_mae: 9.8756 - lr: 3.8147e-09\n",
            "Epoch 50/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8718 - mae: 5.8802\n",
            "Epoch 00050: val_loss did not improve from 3.87051\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.8718 - mae: 5.8802 - val_loss: 4.1441 - val_mae: 10.9472 - lr: 1.9073e-09\n",
            "Epoch 51/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8767 - mae: 5.6268Restoring model weights from the end of the best epoch: 21.\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 3.87051\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.8767 - mae: 5.6268 - val_loss: 4.2570 - val_mae: 10.8727 - lr: 1.9073e-09\n",
            "Epoch 00051: early stopping\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 3.7817 - mae: 10.3687\n",
            "Fit model on training data fold  3\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 72.6830 - mae: 77.7340\n",
            "Epoch 00001: val_loss improved from inf to 7.57418, saving model to /home/jupyter/IEEE/fold3.h5\n",
            "85/85 [==============================] - 22s 141ms/step - loss: 72.6830 - mae: 77.7340 - val_loss: 7.5742 - val_mae: 35.5374 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 4.3635 - mae: 16.3782\n",
            "Epoch 00002: val_loss improved from 7.57418 to 7.55753, saving model to /home/jupyter/IEEE/fold3.h5\n",
            "85/85 [==============================] - 8s 92ms/step - loss: 4.3635 - mae: 16.3782 - val_loss: 7.5575 - val_mae: 41.4562 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.9133 - mae: 14.2243\n",
            "Epoch 00003: val_loss improved from 7.55753 to 6.93630, saving model to /home/jupyter/IEEE/fold3.h5\n",
            "85/85 [==============================] - 8s 91ms/step - loss: 3.9133 - mae: 14.2243 - val_loss: 6.9363 - val_mae: 34.0021 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.7046 - mae: 11.8304\n",
            "Epoch 00004: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 3.7046 - mae: 11.8304 - val_loss: 7.6490 - val_mae: 33.5830 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.5243 - mae: 10.0783\n",
            "Epoch 00005: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "85/85 [==============================] - 8s 88ms/step - loss: 3.5243 - mae: 10.0783 - val_loss: 10.5415 - val_mae: 39.6074 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.2236 - mae: 7.8096\n",
            "Epoch 00006: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 3.2236 - mae: 7.8096 - val_loss: 9.5225 - val_mae: 32.6376 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.0989 - mae: 6.3284\n",
            "Epoch 00007: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "85/85 [==============================] - 8s 88ms/step - loss: 3.0989 - mae: 6.3284 - val_loss: 12.9062 - val_mae: 37.1055 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.9073 - mae: 5.3855\n",
            "Epoch 00008: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.9073 - mae: 5.3855 - val_loss: 13.7095 - val_mae: 34.5158 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.7984 - mae: 4.6847\n",
            "Epoch 00009: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.7984 - mae: 4.6847 - val_loss: 14.9312 - val_mae: 33.3042 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.6293 - mae: 3.9829\n",
            "Epoch 00010: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.6293 - mae: 3.9829 - val_loss: 16.2268 - val_mae: 32.2804 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.5303 - mae: 3.6673\n",
            "Epoch 00011: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.5303 - mae: 3.6673 - val_loss: 17.7514 - val_mae: 33.9134 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.4620 - mae: 3.4162\n",
            "Epoch 00012: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.4620 - mae: 3.4162 - val_loss: 17.3481 - val_mae: 32.5970 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.4031 - mae: 3.2813\n",
            "Epoch 00013: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.4031 - mae: 3.2813 - val_loss: 19.7495 - val_mae: 34.3301 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.3645 - mae: 3.1408\n",
            "Epoch 00014: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.3645 - mae: 3.1408 - val_loss: 19.0550 - val_mae: 32.4121 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.3570 - mae: 3.0534\n",
            "Epoch 00015: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.3570 - mae: 3.0534 - val_loss: 19.3612 - val_mae: 33.0837 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2994 - mae: 2.9340\n",
            "Epoch 00016: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2994 - mae: 2.9340 - val_loss: 19.7798 - val_mae: 31.8777 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.3061 - mae: 2.9144\n",
            "Epoch 00017: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.3061 - mae: 2.9144 - val_loss: 20.4865 - val_mae: 33.2435 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2744 - mae: 2.9375\n",
            "Epoch 00018: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2744 - mae: 2.9375 - val_loss: 20.4479 - val_mae: 33.6961 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2571 - mae: 2.8039\n",
            "Epoch 00019: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2571 - mae: 2.8039 - val_loss: 20.4423 - val_mae: 32.6504 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2582 - mae: 2.8713\n",
            "Epoch 00020: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.2582 - mae: 2.8713 - val_loss: 21.3019 - val_mae: 33.1483 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2485 - mae: 2.9040\n",
            "Epoch 00021: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2485 - mae: 2.9040 - val_loss: 20.8109 - val_mae: 32.9774 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2503 - mae: 2.7665\n",
            "Epoch 00022: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2503 - mae: 2.7665 - val_loss: 20.9375 - val_mae: 33.8651 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2908 - mae: 2.9077\n",
            "Epoch 00023: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.2908 - mae: 2.9077 - val_loss: 21.2687 - val_mae: 33.5293 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2500 - mae: 2.7785\n",
            "Epoch 00024: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2500 - mae: 2.7785 - val_loss: 21.0978 - val_mae: 33.5958 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2147 - mae: 2.7609\n",
            "Epoch 00025: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2147 - mae: 2.7609 - val_loss: 21.0711 - val_mae: 33.0565 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2494 - mae: 2.8710\n",
            "Epoch 00026: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2494 - mae: 2.8710 - val_loss: 21.5593 - val_mae: 33.2045 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2465 - mae: 2.8196\n",
            "Epoch 00027: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2465 - mae: 2.8196 - val_loss: 20.9591 - val_mae: 33.5235 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2403 - mae: 2.7304\n",
            "Epoch 00028: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.2403 - mae: 2.7304 - val_loss: 20.8527 - val_mae: 32.7646 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2512 - mae: 2.8427\n",
            "Epoch 00029: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2512 - mae: 2.8427 - val_loss: 20.7824 - val_mae: 32.4179 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2440 - mae: 2.8219\n",
            "Epoch 00030: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2440 - mae: 2.8219 - val_loss: 21.0638 - val_mae: 33.1709 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2227 - mae: 2.7341\n",
            "Epoch 00031: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2227 - mae: 2.7341 - val_loss: 21.5548 - val_mae: 33.7100 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2494 - mae: 2.7703\n",
            "Epoch 00032: val_loss did not improve from 6.93630\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2494 - mae: 2.7703 - val_loss: 21.1923 - val_mae: 32.3510 - lr: 6.1035e-08\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2316 - mae: 2.6896Restoring model weights from the end of the best epoch: 3.\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 6.93630\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "85/85 [==============================] - 8s 91ms/step - loss: 2.2316 - mae: 2.6896 - val_loss: 20.9569 - val_mae: 32.8475 - lr: 6.1035e-08\n",
            "Epoch 00033: early stopping\n",
            "4/4 [==============================] - 0s 63ms/step - loss: 3.9173 - mae: 9.8115\n",
            "Fit model on training data fold  4\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 69.0613 - mae: 70.9014\n",
            "Epoch 00001: val_loss improved from inf to 7.78984, saving model to /home/jupyter/IEEE/fold4.h5\n",
            "85/85 [==============================] - 21s 130ms/step - loss: 69.0613 - mae: 70.9014 - val_loss: 7.7898 - val_mae: 34.0607 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 4.7111 - mae: 16.4348\n",
            "Epoch 00002: val_loss improved from 7.78984 to 6.04326, saving model to /home/jupyter/IEEE/fold4.h5\n",
            "85/85 [==============================] - 9s 104ms/step - loss: 4.7111 - mae: 16.4348 - val_loss: 6.0433 - val_mae: 23.3436 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.9280 - mae: 12.7841\n",
            "Epoch 00003: val_loss improved from 6.04326 to 5.51578, saving model to /home/jupyter/IEEE/fold4.h5\n",
            "85/85 [==============================] - 9s 101ms/step - loss: 3.9280 - mae: 12.7841 - val_loss: 5.5158 - val_mae: 21.4079 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.6128 - mae: 10.9961\n",
            "Epoch 00004: val_loss did not improve from 5.51578\n",
            "85/85 [==============================] - 8s 97ms/step - loss: 3.6128 - mae: 10.9961 - val_loss: 6.7336 - val_mae: 24.2123 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.4638 - mae: 9.6722\n",
            "Epoch 00005: val_loss improved from 5.51578 to 5.31338, saving model to /home/jupyter/IEEE/fold4.h5\n",
            "85/85 [==============================] - 8s 100ms/step - loss: 3.4638 - mae: 9.6722 - val_loss: 5.3134 - val_mae: 18.1322 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.3865 - mae: 8.6680\n",
            "Epoch 00006: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 8s 96ms/step - loss: 3.3865 - mae: 8.6680 - val_loss: 6.5558 - val_mae: 20.3394 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.5164 - mae: 9.1207\n",
            "Epoch 00007: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "85/85 [==============================] - 8s 94ms/step - loss: 3.5164 - mae: 9.1207 - val_loss: 7.5747 - val_mae: 23.7427 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.1067 - mae: 6.9934\n",
            "Epoch 00008: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 8s 93ms/step - loss: 3.1067 - mae: 6.9934 - val_loss: 6.6235 - val_mae: 19.0698 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.9653 - mae: 6.0394\n",
            "Epoch 00009: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.9653 - mae: 6.0394 - val_loss: 13.4933 - val_mae: 35.2157 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.8152 - mae: 5.5135\n",
            "Epoch 00010: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 7s 88ms/step - loss: 2.8152 - mae: 5.5135 - val_loss: 9.2046 - val_mae: 22.4922 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.6954 - mae: 4.7159\n",
            "Epoch 00011: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.6954 - mae: 4.7159 - val_loss: 9.5923 - val_mae: 22.1073 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.6244 - mae: 4.4678\n",
            "Epoch 00012: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 8s 88ms/step - loss: 2.6244 - mae: 4.4678 - val_loss: 10.7061 - val_mae: 22.6997 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.5416 - mae: 4.1479\n",
            "Epoch 00013: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.5416 - mae: 4.1479 - val_loss: 10.5794 - val_mae: 21.9366 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.4742 - mae: 3.9264\n",
            "Epoch 00014: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.4742 - mae: 3.9264 - val_loss: 11.0092 - val_mae: 21.6597 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.4473 - mae: 3.7143\n",
            "Epoch 00015: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.4473 - mae: 3.7143 - val_loss: 9.1522 - val_mae: 17.3025 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.3802 - mae: 3.6312\n",
            "Epoch 00016: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.3802 - mae: 3.6312 - val_loss: 9.9969 - val_mae: 18.5738 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.3768 - mae: 3.4678\n",
            "Epoch 00017: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "85/85 [==============================] - 7s 88ms/step - loss: 2.3768 - mae: 3.4678 - val_loss: 10.6036 - val_mae: 19.7402 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.3541 - mae: 3.3638\n",
            "Epoch 00018: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 7s 88ms/step - loss: 2.3541 - mae: 3.3638 - val_loss: 12.3640 - val_mae: 21.9210 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.3288 - mae: 3.4146\n",
            "Epoch 00019: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "85/85 [==============================] - 7s 88ms/step - loss: 2.3288 - mae: 3.4146 - val_loss: 11.7124 - val_mae: 21.0756 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.3167 - mae: 3.2612\n",
            "Epoch 00020: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.3167 - mae: 3.2612 - val_loss: 11.8131 - val_mae: 21.0319 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.3013 - mae: 3.2082\n",
            "Epoch 00021: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.3013 - mae: 3.2082 - val_loss: 11.0858 - val_mae: 19.5775 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.3121 - mae: 3.2967\n",
            "Epoch 00022: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 7s 88ms/step - loss: 2.3121 - mae: 3.2967 - val_loss: 12.0821 - val_mae: 20.7541 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2951 - mae: 3.2225\n",
            "Epoch 00023: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "85/85 [==============================] - 7s 88ms/step - loss: 2.2951 - mae: 3.2225 - val_loss: 10.9167 - val_mae: 19.1809 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2917 - mae: 3.1789\n",
            "Epoch 00024: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 7s 87ms/step - loss: 2.2917 - mae: 3.1789 - val_loss: 11.6734 - val_mae: 20.2214 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2826 - mae: 3.2324\n",
            "Epoch 00025: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "85/85 [==============================] - 7s 88ms/step - loss: 2.2826 - mae: 3.2324 - val_loss: 11.3695 - val_mae: 20.0034 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2847 - mae: 3.2278\n",
            "Epoch 00026: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 7s 88ms/step - loss: 2.2847 - mae: 3.2278 - val_loss: 10.5454 - val_mae: 18.4910 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2835 - mae: 3.1829\n",
            "Epoch 00027: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "85/85 [==============================] - 7s 88ms/step - loss: 2.2835 - mae: 3.1829 - val_loss: 9.8009 - val_mae: 16.8295 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2954 - mae: 3.1236\n",
            "Epoch 00028: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 7s 88ms/step - loss: 2.2954 - mae: 3.1236 - val_loss: 10.6272 - val_mae: 18.4069 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2861 - mae: 3.1882\n",
            "Epoch 00029: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.2861 - mae: 3.1882 - val_loss: 10.8315 - val_mae: 18.7266 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2852 - mae: 3.1929\n",
            "Epoch 00030: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 7s 88ms/step - loss: 2.2852 - mae: 3.1929 - val_loss: 11.9131 - val_mae: 20.3792 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2837 - mae: 3.1341\n",
            "Epoch 00031: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "85/85 [==============================] - 7s 88ms/step - loss: 2.2837 - mae: 3.1341 - val_loss: 11.6678 - val_mae: 20.6341 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2804 - mae: 3.1965\n",
            "Epoch 00032: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 7s 87ms/step - loss: 2.2804 - mae: 3.1965 - val_loss: 10.8698 - val_mae: 19.1884 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2864 - mae: 3.2235\n",
            "Epoch 00033: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "85/85 [==============================] - 7s 87ms/step - loss: 2.2864 - mae: 3.2235 - val_loss: 10.8478 - val_mae: 18.3887 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2834 - mae: 3.2038\n",
            "Epoch 00034: val_loss did not improve from 5.31338\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.2834 - mae: 3.2038 - val_loss: 11.4409 - val_mae: 20.0451 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2947 - mae: 3.1588Restoring model weights from the end of the best epoch: 5.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 5.31338\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.2947 - mae: 3.1588 - val_loss: 11.3537 - val_mae: 19.6127 - lr: 6.1035e-08\n",
            "Epoch 00035: early stopping\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 7.8362 - mae: 29.9613\n",
            "Fit model on training data fold  5\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 53.4331 - mae: 56.1619\n",
            "Epoch 00001: val_loss improved from inf to 12.07224, saving model to /home/jupyter/IEEE/fold5.h5\n",
            "85/85 [==============================] - 21s 134ms/step - loss: 53.4331 - mae: 56.1619 - val_loss: 12.0722 - val_mae: 42.5308 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 4.8049 - mae: 14.4025\n",
            "Epoch 00002: val_loss improved from 12.07224 to 9.70882, saving model to /home/jupyter/IEEE/fold5.h5\n",
            "85/85 [==============================] - 8s 100ms/step - loss: 4.8049 - mae: 14.4025 - val_loss: 9.7088 - val_mae: 35.7955 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.8624 - mae: 10.6701\n",
            "Epoch 00003: val_loss improved from 9.70882 to 7.92423, saving model to /home/jupyter/IEEE/fold5.h5\n",
            "85/85 [==============================] - 8s 97ms/step - loss: 3.8624 - mae: 10.6701 - val_loss: 7.9242 - val_mae: 29.1352 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.5448 - mae: 9.2599\n",
            "Epoch 00004: val_loss improved from 7.92423 to 7.60818, saving model to /home/jupyter/IEEE/fold5.h5\n",
            "85/85 [==============================] - 8s 96ms/step - loss: 3.5448 - mae: 9.2599 - val_loss: 7.6082 - val_mae: 27.4008 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.3653 - mae: 8.2704\n",
            "Epoch 00005: val_loss improved from 7.60818 to 7.05253, saving model to /home/jupyter/IEEE/fold5.h5\n",
            "85/85 [==============================] - 8s 95ms/step - loss: 3.3653 - mae: 8.2704 - val_loss: 7.0525 - val_mae: 21.1069 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.2981 - mae: 7.9267\n",
            "Epoch 00006: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 92ms/step - loss: 3.2981 - mae: 7.9267 - val_loss: 7.3451 - val_mae: 21.5438 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 3.1165 - mae: 6.7873\n",
            "Epoch 00007: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "85/85 [==============================] - 8s 93ms/step - loss: 3.1165 - mae: 6.7873 - val_loss: 7.5505 - val_mae: 20.0666 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.9238 - mae: 5.6842\n",
            "Epoch 00008: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 92ms/step - loss: 2.9238 - mae: 5.6842 - val_loss: 14.5013 - val_mae: 36.8025 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.8501 - mae: 5.2377\n",
            "Epoch 00009: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "85/85 [==============================] - 8s 93ms/step - loss: 2.8501 - mae: 5.2377 - val_loss: 12.6641 - val_mae: 30.4395 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.7070 - mae: 4.7405\n",
            "Epoch 00010: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 91ms/step - loss: 2.7070 - mae: 4.7405 - val_loss: 13.9174 - val_mae: 29.0459 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.6302 - mae: 4.2503\n",
            "Epoch 00011: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.6302 - mae: 4.2503 - val_loss: 13.6725 - val_mae: 27.5241 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.5411 - mae: 3.8761\n",
            "Epoch 00012: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.5411 - mae: 3.8761 - val_loss: 14.3029 - val_mae: 26.4106 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.4739 - mae: 3.7754\n",
            "Epoch 00013: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.4739 - mae: 3.7754 - val_loss: 14.8386 - val_mae: 25.2259 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.4108 - mae: 3.5243\n",
            "Epoch 00014: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.4108 - mae: 3.5243 - val_loss: 15.1959 - val_mae: 25.9637 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.3845 - mae: 3.4189\n",
            "Epoch 00015: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "85/85 [==============================] - 8s 91ms/step - loss: 2.3845 - mae: 3.4189 - val_loss: 17.7408 - val_mae: 28.5913 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.3258 - mae: 3.1699\n",
            "Epoch 00016: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.3258 - mae: 3.1699 - val_loss: 16.3275 - val_mae: 26.1924 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.3247 - mae: 3.2252\n",
            "Epoch 00017: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.3247 - mae: 3.2252 - val_loss: 17.4052 - val_mae: 27.5119 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.3164 - mae: 3.1596\n",
            "Epoch 00018: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 91ms/step - loss: 2.3164 - mae: 3.1596 - val_loss: 16.5631 - val_mae: 26.5559 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2919 - mae: 3.1475\n",
            "Epoch 00019: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "85/85 [==============================] - 8s 91ms/step - loss: 2.2919 - mae: 3.1475 - val_loss: 17.1391 - val_mae: 26.7392 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2747 - mae: 3.1054\n",
            "Epoch 00020: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 93ms/step - loss: 2.2747 - mae: 3.1054 - val_loss: 16.2840 - val_mae: 24.7187 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2659 - mae: 3.0498\n",
            "Epoch 00021: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "85/85 [==============================] - 8s 91ms/step - loss: 2.2659 - mae: 3.0498 - val_loss: 17.5748 - val_mae: 26.1408 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2435 - mae: 3.0080\n",
            "Epoch 00022: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 91ms/step - loss: 2.2435 - mae: 3.0080 - val_loss: 16.4855 - val_mae: 25.5511 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2622 - mae: 3.0076\n",
            "Epoch 00023: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "85/85 [==============================] - 8s 91ms/step - loss: 2.2622 - mae: 3.0076 - val_loss: 17.3954 - val_mae: 26.2510 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2607 - mae: 3.1047\n",
            "Epoch 00024: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 91ms/step - loss: 2.2607 - mae: 3.1047 - val_loss: 18.1083 - val_mae: 26.7686 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2765 - mae: 3.0711\n",
            "Epoch 00025: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2765 - mae: 3.0711 - val_loss: 17.5631 - val_mae: 26.9463 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2371 - mae: 3.0480\n",
            "Epoch 00026: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2371 - mae: 3.0480 - val_loss: 17.3012 - val_mae: 26.2019 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2471 - mae: 3.0624\n",
            "Epoch 00027: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2471 - mae: 3.0624 - val_loss: 16.8781 - val_mae: 25.7663 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2450 - mae: 2.9791\n",
            "Epoch 00028: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 88ms/step - loss: 2.2450 - mae: 2.9791 - val_loss: 17.9938 - val_mae: 27.7868 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2422 - mae: 2.9945\n",
            "Epoch 00029: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2422 - mae: 2.9945 - val_loss: 18.0826 - val_mae: 27.3285 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2414 - mae: 2.9357\n",
            "Epoch 00030: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2414 - mae: 2.9357 - val_loss: 17.5706 - val_mae: 25.9559 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2576 - mae: 3.0257\n",
            "Epoch 00031: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.2576 - mae: 3.0257 - val_loss: 18.5479 - val_mae: 27.2723 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2477 - mae: 3.0576\n",
            "Epoch 00032: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2477 - mae: 3.0576 - val_loss: 17.0711 - val_mae: 25.5868 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2624 - mae: 2.9256\n",
            "Epoch 00033: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.2624 - mae: 2.9256 - val_loss: 16.7914 - val_mae: 25.2296 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2562 - mae: 3.1008\n",
            "Epoch 00034: val_loss did not improve from 7.05253\n",
            "85/85 [==============================] - 8s 89ms/step - loss: 2.2562 - mae: 3.1008 - val_loss: 18.1944 - val_mae: 27.2759 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 2.2477 - mae: 2.9548Restoring model weights from the end of the best epoch: 5.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 7.05253\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "85/85 [==============================] - 8s 90ms/step - loss: 2.2477 - mae: 2.9548 - val_loss: 18.4409 - val_mae: 27.1808 - lr: 6.1035e-08\n",
            "Epoch 00035: early stopping\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 12.1042 - mae: 42.9513\n",
            "Fit model on training data fold  6\n",
            "Epoch 1/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 40.4286 - mae: 45.8030\n",
            "Epoch 00001: val_loss improved from inf to 6.29749, saving model to /home/jupyter/IEEE/fold6.h5\n",
            "84/84 [==============================] - 21s 134ms/step - loss: 40.4286 - mae: 45.8030 - val_loss: 6.2975 - val_mae: 20.9320 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.6142 - mae: 14.3062\n",
            "Epoch 00002: val_loss improved from 6.29749 to 4.81456, saving model to /home/jupyter/IEEE/fold6.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 4.6142 - mae: 14.3062 - val_loss: 4.8146 - val_mae: 16.3061 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.0609 - mae: 11.9133\n",
            "Epoch 00003: val_loss improved from 4.81456 to 4.31327, saving model to /home/jupyter/IEEE/fold6.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 4.0609 - mae: 11.9133 - val_loss: 4.3133 - val_mae: 14.5287 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.8420 - mae: 11.0703\n",
            "Epoch 00004: val_loss improved from 4.31327 to 4.27703, saving model to /home/jupyter/IEEE/fold6.h5\n",
            "84/84 [==============================] - 8s 89ms/step - loss: 3.8420 - mae: 11.0703 - val_loss: 4.2770 - val_mae: 14.1484 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.6335 - mae: 9.9203\n",
            "Epoch 00005: val_loss did not improve from 4.27703\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.6335 - mae: 9.9203 - val_loss: 4.5744 - val_mae: 14.7148 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.5217 - mae: 9.5857\n",
            "Epoch 00006: val_loss did not improve from 4.27703\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 3.5217 - mae: 9.5857 - val_loss: 4.4664 - val_mae: 13.7720 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.3628 - mae: 8.4476\n",
            "Epoch 00007: val_loss improved from 4.27703 to 4.12077, saving model to /home/jupyter/IEEE/fold6.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.3628 - mae: 8.4476 - val_loss: 4.1208 - val_mae: 12.0516 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.2274 - mae: 7.7864\n",
            "Epoch 00008: val_loss did not improve from 4.12077\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.2274 - mae: 7.7864 - val_loss: 4.1327 - val_mae: 11.8552 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 3.1273 - mae: 6.7996\n",
            "Epoch 00009: val_loss did not improve from 4.12077\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 3.1271 - mae: 6.8075 - val_loss: 4.1696 - val_mae: 11.5190 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9616 - mae: 6.3452\n",
            "Epoch 00010: val_loss improved from 4.12077 to 4.09133, saving model to /home/jupyter/IEEE/fold6.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.9616 - mae: 6.3452 - val_loss: 4.0913 - val_mae: 10.5833 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8671 - mae: 5.7929\n",
            "Epoch 00011: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.8671 - mae: 5.7929 - val_loss: 4.6074 - val_mae: 11.2665 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8111 - mae: 5.4153\n",
            "Epoch 00012: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.8111 - mae: 5.4153 - val_loss: 4.3127 - val_mae: 9.9897 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7207 - mae: 4.9176\n",
            "Epoch 00013: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.7207 - mae: 4.9176 - val_loss: 4.5795 - val_mae: 10.6403 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.6919 - mae: 4.7744\n",
            "Epoch 00014: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.6919 - mae: 4.7744 - val_loss: 4.2672 - val_mae: 9.2362 - lr: 1.2500e-04\n",
            "Epoch 15/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.6098 - mae: 4.5694\n",
            "Epoch 00015: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.6098 - mae: 4.5694 - val_loss: 4.7125 - val_mae: 10.5439 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5693 - mae: 4.3332\n",
            "Epoch 00016: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.5693 - mae: 4.3332 - val_loss: 4.5990 - val_mae: 9.7348 - lr: 6.2500e-05\n",
            "Epoch 17/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5426 - mae: 4.2314\n",
            "Epoch 00017: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.5426 - mae: 4.2314 - val_loss: 4.7718 - val_mae: 10.0632 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5186 - mae: 4.0862\n",
            "Epoch 00018: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.5186 - mae: 4.0862 - val_loss: 4.7498 - val_mae: 9.7120 - lr: 3.1250e-05\n",
            "Epoch 19/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4957 - mae: 4.0889\n",
            "Epoch 00019: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.4957 - mae: 4.0889 - val_loss: 4.7609 - val_mae: 10.1529 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4906 - mae: 4.1107\n",
            "Epoch 00020: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4906 - mae: 4.1107 - val_loss: 4.5241 - val_mae: 9.2669 - lr: 1.5625e-05\n",
            "Epoch 21/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4751 - mae: 4.0168\n",
            "Epoch 00021: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4751 - mae: 4.0168 - val_loss: 4.8620 - val_mae: 9.8657 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4649 - mae: 3.8711\n",
            "Epoch 00022: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4649 - mae: 3.8711 - val_loss: 4.7776 - val_mae: 9.9593 - lr: 7.8125e-06\n",
            "Epoch 23/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4647 - mae: 3.8698\n",
            "Epoch 00023: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4647 - mae: 3.8698 - val_loss: 4.6988 - val_mae: 9.6849 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4638 - mae: 3.9452\n",
            "Epoch 00024: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4638 - mae: 3.9452 - val_loss: 4.7224 - val_mae: 9.6264 - lr: 3.9063e-06\n",
            "Epoch 25/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4634 - mae: 3.9397\n",
            "Epoch 00025: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4634 - mae: 3.9397 - val_loss: 4.7061 - val_mae: 9.9328 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4613 - mae: 3.9725\n",
            "Epoch 00026: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.4613 - mae: 3.9725 - val_loss: 4.7127 - val_mae: 9.3271 - lr: 1.9531e-06\n",
            "Epoch 27/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4660 - mae: 3.9072\n",
            "Epoch 00027: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.4660 - mae: 3.9072 - val_loss: 4.7155 - val_mae: 9.4409 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4517 - mae: 3.8850\n",
            "Epoch 00028: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.4517 - mae: 3.8850 - val_loss: 4.6280 - val_mae: 9.5262 - lr: 9.7656e-07\n",
            "Epoch 29/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4518 - mae: 4.0271\n",
            "Epoch 00029: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.4518 - mae: 4.0271 - val_loss: 4.6518 - val_mae: 9.4188 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4390 - mae: 3.9324\n",
            "Epoch 00030: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.4390 - mae: 3.9324 - val_loss: 4.6742 - val_mae: 9.4013 - lr: 4.8828e-07\n",
            "Epoch 31/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4464 - mae: 3.8629\n",
            "Epoch 00031: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4464 - mae: 3.8629 - val_loss: 4.6441 - val_mae: 9.5271 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4453 - mae: 3.8617\n",
            "Epoch 00032: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4453 - mae: 3.8617 - val_loss: 4.6914 - val_mae: 9.8425 - lr: 2.4414e-07\n",
            "Epoch 33/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4544 - mae: 3.8783\n",
            "Epoch 00033: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4544 - mae: 3.8783 - val_loss: 4.7381 - val_mae: 9.7205 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4490 - mae: 3.8676\n",
            "Epoch 00034: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4490 - mae: 3.8676 - val_loss: 4.8980 - val_mae: 9.6373 - lr: 1.2207e-07\n",
            "Epoch 35/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4529 - mae: 3.8989\n",
            "Epoch 00035: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4529 - mae: 3.8989 - val_loss: 4.9246 - val_mae: 9.9064 - lr: 6.1035e-08\n",
            "Epoch 36/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4652 - mae: 3.9341\n",
            "Epoch 00036: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.4652 - mae: 3.9341 - val_loss: 4.5920 - val_mae: 9.3950 - lr: 6.1035e-08\n",
            "Epoch 37/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4496 - mae: 3.8024\n",
            "Epoch 00037: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.4496 - mae: 3.8024 - val_loss: 4.8241 - val_mae: 9.6777 - lr: 3.0518e-08\n",
            "Epoch 38/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4436 - mae: 3.8224\n",
            "Epoch 00038: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4436 - mae: 3.8224 - val_loss: 4.8244 - val_mae: 10.0838 - lr: 3.0518e-08\n",
            "Epoch 39/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4566 - mae: 3.8923\n",
            "Epoch 00039: val_loss did not improve from 4.09133\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.4566 - mae: 3.8923 - val_loss: 4.6899 - val_mae: 9.3341 - lr: 1.5259e-08\n",
            "Epoch 40/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4589 - mae: 3.8076Restoring model weights from the end of the best epoch: 10.\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 4.09133\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.4589 - mae: 3.8076 - val_loss: 4.5924 - val_mae: 9.3094 - lr: 1.5259e-08\n",
            "Epoch 00040: early stopping\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 6.5978 - mae: 17.6955\n",
            "Fit model on training data fold  7\n",
            "Epoch 1/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 53.5447 - mae: 66.7181\n",
            "Epoch 00001: val_loss improved from inf to 7.85055, saving model to /home/jupyter/IEEE/fold7.h5\n",
            "84/84 [==============================] - 20s 129ms/step - loss: 53.5447 - mae: 66.7181 - val_loss: 7.8506 - val_mae: 39.7067 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.6912 - mae: 18.2268\n",
            "Epoch 00002: val_loss improved from 7.85055 to 6.06122, saving model to /home/jupyter/IEEE/fold7.h5\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 4.6912 - mae: 18.2268 - val_loss: 6.0612 - val_mae: 26.0675 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.0399 - mae: 14.1257\n",
            "Epoch 00003: val_loss improved from 6.06122 to 4.66082, saving model to /home/jupyter/IEEE/fold7.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 4.0399 - mae: 14.1257 - val_loss: 4.6608 - val_mae: 16.6113 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.8671 - mae: 13.2389\n",
            "Epoch 00004: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.8671 - mae: 13.2389 - val_loss: 4.8248 - val_mae: 17.0447 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.6093 - mae: 11.0043\n",
            "Epoch 00005: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.6093 - mae: 11.0043 - val_loss: 5.5211 - val_mae: 19.0104 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.4288 - mae: 9.5560\n",
            "Epoch 00006: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.4288 - mae: 9.5560 - val_loss: 5.6723 - val_mae: 21.2256 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.2664 - mae: 8.3129\n",
            "Epoch 00007: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.2664 - mae: 8.3129 - val_loss: 5.3039 - val_mae: 16.0981 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.1253 - mae: 7.4849\n",
            "Epoch 00008: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.1253 - mae: 7.4849 - val_loss: 5.4083 - val_mae: 15.8688 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0516 - mae: 7.1453\n",
            "Epoch 00009: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.0516 - mae: 7.1453 - val_loss: 5.7073 - val_mae: 17.1448 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0863 - mae: 7.1466\n",
            "Epoch 00010: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.0863 - mae: 7.1466 - val_loss: 5.5588 - val_mae: 16.4171 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9568 - mae: 6.3813\n",
            "Epoch 00011: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.9568 - mae: 6.3813 - val_loss: 5.9495 - val_mae: 17.2133 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9028 - mae: 5.9771\n",
            "Epoch 00012: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.9028 - mae: 5.9771 - val_loss: 5.8518 - val_mae: 15.5869 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8687 - mae: 5.8003\n",
            "Epoch 00013: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.8687 - mae: 5.8003 - val_loss: 6.1769 - val_mae: 16.1690 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8238 - mae: 5.7786\n",
            "Epoch 00014: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.8238 - mae: 5.7786 - val_loss: 6.0601 - val_mae: 16.1639 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8227 - mae: 5.5410\n",
            "Epoch 00015: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.8227 - mae: 5.5410 - val_loss: 6.1617 - val_mae: 16.4351 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8008 - mae: 5.4643\n",
            "Epoch 00016: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.8008 - mae: 5.4643 - val_loss: 6.1957 - val_mae: 16.5246 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7904 - mae: 5.5092\n",
            "Epoch 00017: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.7904 - mae: 5.5092 - val_loss: 6.0763 - val_mae: 15.6977 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.7740 - mae: 5.3648\n",
            "Epoch 00018: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.7742 - mae: 5.3624 - val_loss: 6.2548 - val_mae: 16.1941 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7712 - mae: 5.3845\n",
            "Epoch 00019: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7712 - mae: 5.3845 - val_loss: 6.0404 - val_mae: 15.5765 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7771 - mae: 5.3282\n",
            "Epoch 00020: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.7771 - mae: 5.3282 - val_loss: 5.9515 - val_mae: 15.3764 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7638 - mae: 5.4491\n",
            "Epoch 00021: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.7638 - mae: 5.4491 - val_loss: 6.1310 - val_mae: 16.0145 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7644 - mae: 5.3605\n",
            "Epoch 00022: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.7644 - mae: 5.3605 - val_loss: 6.1070 - val_mae: 16.0580 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7571 - mae: 5.3733\n",
            "Epoch 00023: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7571 - mae: 5.3733 - val_loss: 6.1827 - val_mae: 15.8106 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7647 - mae: 5.4522\n",
            "Epoch 00024: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.7647 - mae: 5.4522 - val_loss: 6.0377 - val_mae: 15.8209 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.7604 - mae: 5.2511\n",
            "Epoch 00025: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.7599 - mae: 5.2509 - val_loss: 6.0953 - val_mae: 15.7565 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7605 - mae: 5.2934\n",
            "Epoch 00026: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7605 - mae: 5.2934 - val_loss: 6.1579 - val_mae: 15.9994 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7681 - mae: 5.3533\n",
            "Epoch 00027: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.7681 - mae: 5.3533 - val_loss: 6.2110 - val_mae: 15.8365 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7587 - mae: 5.3957\n",
            "Epoch 00028: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7587 - mae: 5.3957 - val_loss: 6.2000 - val_mae: 15.6929 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7618 - mae: 5.3468\n",
            "Epoch 00029: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7618 - mae: 5.3468 - val_loss: 6.1909 - val_mae: 15.7830 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7596 - mae: 5.3047\n",
            "Epoch 00030: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7596 - mae: 5.3047 - val_loss: 6.1553 - val_mae: 15.7669 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7594 - mae: 5.2818\n",
            "Epoch 00031: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7594 - mae: 5.2818 - val_loss: 6.0927 - val_mae: 15.4189 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7582 - mae: 5.3349\n",
            "Epoch 00032: val_loss did not improve from 4.66082\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7582 - mae: 5.3349 - val_loss: 6.0857 - val_mae: 15.9059 - lr: 6.1035e-08\n",
            "Epoch 33/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7582 - mae: 5.3071Restoring model weights from the end of the best epoch: 3.\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 4.66082\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.7582 - mae: 5.3071 - val_loss: 6.2596 - val_mae: 16.3983 - lr: 6.1035e-08\n",
            "Epoch 00033: early stopping\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 3.8426 - mae: 15.1249\n",
            "Fit model on training data fold  8\n",
            "Epoch 1/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 53.6654 - mae: 59.9817\n",
            "Epoch 00001: val_loss improved from inf to 8.16531, saving model to /home/jupyter/IEEE/fold8.h5\n",
            "84/84 [==============================] - 19s 121ms/step - loss: 53.6654 - mae: 59.9817 - val_loss: 8.1653 - val_mae: 36.2102 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.9995 - mae: 19.0665\n",
            "Epoch 00002: val_loss improved from 8.16531 to 5.49356, saving model to /home/jupyter/IEEE/fold8.h5\n",
            "84/84 [==============================] - 8s 95ms/step - loss: 4.9995 - mae: 19.0665 - val_loss: 5.4936 - val_mae: 21.9002 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.3722 - mae: 16.4981\n",
            "Epoch 00003: val_loss improved from 5.49356 to 4.67775, saving model to /home/jupyter/IEEE/fold8.h5\n",
            "84/84 [==============================] - 8s 95ms/step - loss: 4.3722 - mae: 16.4981 - val_loss: 4.6777 - val_mae: 18.9631 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.0791 - mae: 14.4381\n",
            "Epoch 00004: val_loss improved from 4.67775 to 3.93125, saving model to /home/jupyter/IEEE/fold8.h5\n",
            "84/84 [==============================] - 8s 94ms/step - loss: 4.0791 - mae: 14.4381 - val_loss: 3.9312 - val_mae: 12.3799 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.8674 - mae: 12.8753\n",
            "Epoch 00005: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 8s 93ms/step - loss: 3.8674 - mae: 12.8753 - val_loss: 4.0980 - val_mae: 13.8286 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.7890 - mae: 12.2891\n",
            "Epoch 00006: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 3.7890 - mae: 12.2891 - val_loss: 4.4564 - val_mae: 15.3454 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.5845 - mae: 10.6978\n",
            "Epoch 00007: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.5845 - mae: 10.6978 - val_loss: 4.1130 - val_mae: 12.8735 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.4642 - mae: 9.7582\n",
            "Epoch 00008: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 3.4642 - mae: 9.7582 - val_loss: 4.4086 - val_mae: 13.5468 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.3586 - mae: 8.9926\n",
            "Epoch 00009: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 3.3586 - mae: 8.9926 - val_loss: 4.2590 - val_mae: 12.9502 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.3008 - mae: 8.4071\n",
            "Epoch 00010: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.3008 - mae: 8.4071 - val_loss: 4.2195 - val_mae: 12.8595 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.2049 - mae: 7.9504\n",
            "Epoch 00011: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.2049 - mae: 7.9504 - val_loss: 4.3158 - val_mae: 12.9530 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.1739 - mae: 7.6051\n",
            "Epoch 00012: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.1739 - mae: 7.6051 - val_loss: 4.1590 - val_mae: 11.3940 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.1144 - mae: 7.3150\n",
            "Epoch 00013: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.1144 - mae: 7.3150 - val_loss: 4.2344 - val_mae: 11.8865 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.1088 - mae: 7.3751\n",
            "Epoch 00014: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 3.1088 - mae: 7.3751 - val_loss: 4.2382 - val_mae: 11.1395 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0701 - mae: 7.2759\n",
            "Epoch 00015: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.0701 - mae: 7.2759 - val_loss: 4.3787 - val_mae: 11.5710 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0517 - mae: 7.2403\n",
            "Epoch 00016: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.0517 - mae: 7.2403 - val_loss: 4.2920 - val_mae: 12.0387 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0609 - mae: 7.1302\n",
            "Epoch 00017: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.0609 - mae: 7.1302 - val_loss: 4.3268 - val_mae: 11.8455 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0318 - mae: 6.7417\n",
            "Epoch 00018: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.0318 - mae: 6.7417 - val_loss: 4.3089 - val_mae: 11.4701 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0225 - mae: 6.9269\n",
            "Epoch 00019: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.0225 - mae: 6.9269 - val_loss: 4.2400 - val_mae: 11.2149 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0208 - mae: 6.9076\n",
            "Epoch 00020: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.0208 - mae: 6.9076 - val_loss: 4.1856 - val_mae: 10.9269 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0128 - mae: 6.7654\n",
            "Epoch 00021: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.0128 - mae: 6.7654 - val_loss: 4.3753 - val_mae: 11.5920 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0051 - mae: 6.6820\n",
            "Epoch 00022: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.0051 - mae: 6.6820 - val_loss: 4.2614 - val_mae: 10.0629 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0206 - mae: 6.8963\n",
            "Epoch 00023: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.0206 - mae: 6.8963 - val_loss: 4.1503 - val_mae: 10.7556 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0117 - mae: 6.7107\n",
            "Epoch 00024: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.0117 - mae: 6.7107 - val_loss: 4.2875 - val_mae: 11.3416 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0111 - mae: 6.6741\n",
            "Epoch 00025: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.0111 - mae: 6.6741 - val_loss: 4.2376 - val_mae: 11.0668 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0100 - mae: 6.8293\n",
            "Epoch 00026: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.0100 - mae: 6.8293 - val_loss: 4.2539 - val_mae: 11.2518 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0113 - mae: 6.8525\n",
            "Epoch 00027: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.0113 - mae: 6.8525 - val_loss: 4.1803 - val_mae: 10.5541 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0180 - mae: 6.7826\n",
            "Epoch 00028: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.0180 - mae: 6.7826 - val_loss: 4.2953 - val_mae: 11.5228 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0118 - mae: 6.6582\n",
            "Epoch 00029: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.0118 - mae: 6.6582 - val_loss: 4.1275 - val_mae: 10.5854 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0053 - mae: 6.8155\n",
            "Epoch 00030: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.0053 - mae: 6.8155 - val_loss: 4.2804 - val_mae: 11.1448 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0118 - mae: 6.7612\n",
            "Epoch 00031: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.0118 - mae: 6.7612 - val_loss: 4.3517 - val_mae: 11.3850 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0133 - mae: 6.7744\n",
            "Epoch 00032: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.0133 - mae: 6.7744 - val_loss: 4.2220 - val_mae: 11.3166 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0161 - mae: 6.8776\n",
            "Epoch 00033: val_loss did not improve from 3.93125\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.0161 - mae: 6.8776 - val_loss: 4.2110 - val_mae: 11.1218 - lr: 6.1035e-08\n",
            "Epoch 34/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0046 - mae: 6.7111Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 3.93125\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.0046 - mae: 6.7111 - val_loss: 4.1532 - val_mae: 10.1469 - lr: 6.1035e-08\n",
            "Epoch 00034: early stopping\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 5.9573 - mae: 23.8441\n",
            "Fit model on training data fold  9\n",
            "Epoch 1/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 62.7762 - mae: 66.5010\n",
            "Epoch 00001: val_loss improved from inf to 6.48700, saving model to /home/jupyter/IEEE/fold9.h5\n",
            "83/83 [==============================] - 20s 133ms/step - loss: 62.7762 - mae: 66.5010 - val_loss: 6.4870 - val_mae: 22.3804 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 4.5953 - mae: 15.3013\n",
            "Epoch 00002: val_loss did not improve from 6.48700\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 4.5953 - mae: 15.3013 - val_loss: 8.8785 - val_mae: 35.6992 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.9712 - mae: 12.9252\n",
            "Epoch 00003: val_loss did not improve from 6.48700\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 3.9712 - mae: 12.9252 - val_loss: 8.0703 - val_mae: 32.0423 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.6356 - mae: 11.3231\n",
            "Epoch 00004: val_loss did not improve from 6.48700\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 3.6356 - mae: 11.3231 - val_loss: 7.4585 - val_mae: 29.9643 - lr: 5.0000e-04\n",
            "Epoch 5/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.5371 - mae: 10.2622\n",
            "Epoch 00005: val_loss improved from 6.48700 to 6.45530, saving model to /home/jupyter/IEEE/fold9.h5\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 3.5371 - mae: 10.2622 - val_loss: 6.4553 - val_mae: 23.8966 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.4000 - mae: 9.3049\n",
            "Epoch 00006: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 3.4000 - mae: 9.3049 - val_loss: 8.8353 - val_mae: 30.1063 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.3232 - mae: 8.3161\n",
            "Epoch 00007: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 3.3232 - mae: 8.3161 - val_loss: 10.4221 - val_mae: 32.2186 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.1884 - mae: 7.3007\n",
            "Epoch 00008: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 3.1884 - mae: 7.3007 - val_loss: 9.5820 - val_mae: 28.5364 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.0584 - mae: 6.7363\n",
            "Epoch 00009: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 3.0584 - mae: 6.7363 - val_loss: 9.6906 - val_mae: 26.6093 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.9563 - mae: 6.1855\n",
            "Epoch 00010: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.9563 - mae: 6.1855 - val_loss: 10.2221 - val_mae: 27.2114 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.8825 - mae: 5.6518\n",
            "Epoch 00011: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.8825 - mae: 5.6518 - val_loss: 11.7002 - val_mae: 29.4976 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.8332 - mae: 5.5803\n",
            "Epoch 00012: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.8332 - mae: 5.5803 - val_loss: 11.5673 - val_mae: 28.3560 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7810 - mae: 5.1432\n",
            "Epoch 00013: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.7810 - mae: 5.1432 - val_loss: 11.8368 - val_mae: 28.3003 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7375 - mae: 4.9988\n",
            "Epoch 00014: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.7375 - mae: 4.9988 - val_loss: 11.7722 - val_mae: 27.7277 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7037 - mae: 4.7369\n",
            "Epoch 00015: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.7037 - mae: 4.7369 - val_loss: 11.8197 - val_mae: 26.8882 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6803 - mae: 4.7727\n",
            "Epoch 00016: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.6803 - mae: 4.7727 - val_loss: 11.8798 - val_mae: 26.8687 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6687 - mae: 4.7322\n",
            "Epoch 00017: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.6687 - mae: 4.7322 - val_loss: 12.6315 - val_mae: 28.2430 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6573 - mae: 4.6126\n",
            "Epoch 00018: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.6573 - mae: 4.6126 - val_loss: 12.2390 - val_mae: 27.1114 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6433 - mae: 4.6110\n",
            "Epoch 00019: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6433 - mae: 4.6110 - val_loss: 12.1817 - val_mae: 27.1138 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6409 - mae: 4.5686\n",
            "Epoch 00020: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.6409 - mae: 4.5686 - val_loss: 12.7374 - val_mae: 27.7733 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6206 - mae: 4.5321\n",
            "Epoch 00021: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.6206 - mae: 4.5321 - val_loss: 12.4029 - val_mae: 27.3690 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6215 - mae: 4.4924\n",
            "Epoch 00022: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.6215 - mae: 4.4924 - val_loss: 12.5309 - val_mae: 27.8263 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6307 - mae: 4.5864\n",
            "Epoch 00023: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6307 - mae: 4.5864 - val_loss: 12.5059 - val_mae: 27.0294 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6189 - mae: 4.4833\n",
            "Epoch 00024: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6189 - mae: 4.4833 - val_loss: 12.5316 - val_mae: 27.0784 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6206 - mae: 4.2900\n",
            "Epoch 00025: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.6206 - mae: 4.2900 - val_loss: 12.8942 - val_mae: 28.0861 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6357 - mae: 4.6165\n",
            "Epoch 00026: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6357 - mae: 4.6165 - val_loss: 13.0033 - val_mae: 27.9331 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6196 - mae: 4.4472\n",
            "Epoch 00027: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.6196 - mae: 4.4472 - val_loss: 12.8275 - val_mae: 27.7402 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6154 - mae: 4.5697\n",
            "Epoch 00028: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6154 - mae: 4.5697 - val_loss: 12.2072 - val_mae: 26.5778 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6288 - mae: 4.4698\n",
            "Epoch 00029: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6288 - mae: 4.4698 - val_loss: 12.9660 - val_mae: 28.0111 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6271 - mae: 4.4803\n",
            "Epoch 00030: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.6271 - mae: 4.4803 - val_loss: 12.0549 - val_mae: 26.2541 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6107 - mae: 4.4371\n",
            "Epoch 00031: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6107 - mae: 4.4371 - val_loss: 13.0833 - val_mae: 28.2536 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6307 - mae: 4.5357\n",
            "Epoch 00032: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6307 - mae: 4.5357 - val_loss: 12.6761 - val_mae: 27.5223 - lr: 6.1035e-08\n",
            "Epoch 33/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6352 - mae: 4.5672\n",
            "Epoch 00033: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6352 - mae: 4.5672 - val_loss: 12.0524 - val_mae: 26.0572 - lr: 6.1035e-08\n",
            "Epoch 34/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6332 - mae: 4.5088\n",
            "Epoch 00034: val_loss did not improve from 6.45530\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.6332 - mae: 4.5088 - val_loss: 12.8643 - val_mae: 28.0092 - lr: 3.0518e-08\n",
            "Epoch 35/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6256 - mae: 4.4526Restoring model weights from the end of the best epoch: 5.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 6.45530\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6256 - mae: 4.4526 - val_loss: 12.9008 - val_mae: 27.7154 - lr: 3.0518e-08\n",
            "Epoch 00035: early stopping\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 3.3610 - mae: 9.6923\n",
            "Fit model on training data fold  10\n",
            "Epoch 1/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 30.2412 - mae: 48.3168\n",
            "Epoch 00001: val_loss improved from inf to 4.12234, saving model to /home/jupyter/IEEE/fold10.h5\n",
            "83/83 [==============================] - 19s 124ms/step - loss: 30.2412 - mae: 48.3168 - val_loss: 4.1223 - val_mae: 14.2826 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 4.4727 - mae: 17.2567\n",
            "Epoch 00002: val_loss did not improve from 4.12234\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 4.4727 - mae: 17.2567 - val_loss: 4.3241 - val_mae: 16.8478 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 4.0729 - mae: 14.8241\n",
            "Epoch 00003: val_loss improved from 4.12234 to 3.85666, saving model to /home/jupyter/IEEE/fold10.h5\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 4.0729 - mae: 14.8241 - val_loss: 3.8567 - val_mae: 12.9670 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.8352 - mae: 12.8274\n",
            "Epoch 00004: val_loss did not improve from 3.85666\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 3.8352 - mae: 12.8274 - val_loss: 3.9928 - val_mae: 14.6592 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.7194 - mae: 11.6222\n",
            "Epoch 00005: val_loss did not improve from 3.85666\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 3.7194 - mae: 11.6222 - val_loss: 4.0153 - val_mae: 13.9524 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.5328 - mae: 10.6715\n",
            "Epoch 00006: val_loss improved from 3.85666 to 3.82020, saving model to /home/jupyter/IEEE/fold10.h5\n",
            "83/83 [==============================] - 7s 90ms/step - loss: 3.5328 - mae: 10.6715 - val_loss: 3.8202 - val_mae: 12.3532 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.4136 - mae: 9.1876\n",
            "Epoch 00007: val_loss improved from 3.82020 to 3.71800, saving model to /home/jupyter/IEEE/fold10.h5\n",
            "83/83 [==============================] - 7s 90ms/step - loss: 3.4136 - mae: 9.1876 - val_loss: 3.7180 - val_mae: 11.0906 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.3427 - mae: 8.6823\n",
            "Epoch 00008: val_loss did not improve from 3.71800\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 3.3427 - mae: 8.6823 - val_loss: 4.4390 - val_mae: 13.2393 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.2649 - mae: 8.1107\n",
            "Epoch 00009: val_loss did not improve from 3.71800\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 3.2649 - mae: 8.1107 - val_loss: 3.9484 - val_mae: 10.2191 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.0772 - mae: 6.7632\n",
            "Epoch 00010: val_loss did not improve from 3.71800\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 3.0772 - mae: 6.7632 - val_loss: 3.8235 - val_mae: 9.4348 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.0341 - mae: 6.5882\n",
            "Epoch 00011: val_loss did not improve from 3.71800\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 3.0341 - mae: 6.5882 - val_loss: 5.1249 - val_mae: 14.1457 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.9487 - mae: 6.2067\n",
            "Epoch 00012: val_loss did not improve from 3.71800\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.9487 - mae: 6.2067 - val_loss: 3.9770 - val_mae: 9.1627 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.8533 - mae: 5.6140\n",
            "Epoch 00013: val_loss did not improve from 3.71800\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 2.8533 - mae: 5.6140 - val_loss: 4.0124 - val_mae: 8.8177 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7941 - mae: 5.1942\n",
            "Epoch 00014: val_loss improved from 3.71800 to 3.56304, saving model to /home/jupyter/IEEE/fold10.h5\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 2.7941 - mae: 5.1942 - val_loss: 3.5630 - val_mae: 7.7172 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7655 - mae: 5.2976\n",
            "Epoch 00015: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.7655 - mae: 5.2976 - val_loss: 3.9285 - val_mae: 8.7072 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7386 - mae: 5.0694\n",
            "Epoch 00016: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.7386 - mae: 5.0694 - val_loss: 3.9897 - val_mae: 8.9351 - lr: 6.2500e-05\n",
            "Epoch 17/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7151 - mae: 4.8336\n",
            "Epoch 00017: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.7151 - mae: 4.8336 - val_loss: 3.7873 - val_mae: 7.6620 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6662 - mae: 4.7561\n",
            "Epoch 00018: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.6662 - mae: 4.7561 - val_loss: 4.0784 - val_mae: 8.5908 - lr: 3.1250e-05\n",
            "Epoch 19/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6538 - mae: 4.6390\n",
            "Epoch 00019: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6538 - mae: 4.6390 - val_loss: 3.9151 - val_mae: 7.8589 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6394 - mae: 4.5148\n",
            "Epoch 00020: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6394 - mae: 4.5148 - val_loss: 3.8070 - val_mae: 7.5944 - lr: 1.5625e-05\n",
            "Epoch 21/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6248 - mae: 4.3640\n",
            "Epoch 00021: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6248 - mae: 4.3640 - val_loss: 4.0488 - val_mae: 8.2050 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6197 - mae: 4.6064\n",
            "Epoch 00022: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "83/83 [==============================] - 7s 86ms/step - loss: 2.6197 - mae: 4.6064 - val_loss: 3.8912 - val_mae: 7.8705 - lr: 7.8125e-06\n",
            "Epoch 23/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6176 - mae: 4.5453\n",
            "Epoch 00023: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6176 - mae: 4.5453 - val_loss: 3.9387 - val_mae: 7.8176 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6141 - mae: 4.3716\n",
            "Epoch 00024: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "83/83 [==============================] - 7s 86ms/step - loss: 2.6141 - mae: 4.3716 - val_loss: 3.9752 - val_mae: 7.7200 - lr: 3.9063e-06\n",
            "Epoch 25/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6058 - mae: 4.4104\n",
            "Epoch 00025: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6058 - mae: 4.4104 - val_loss: 4.0632 - val_mae: 8.2119 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6341 - mae: 4.5328\n",
            "Epoch 00026: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6341 - mae: 4.5328 - val_loss: 3.8860 - val_mae: 7.7405 - lr: 1.9531e-06\n",
            "Epoch 27/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.5942 - mae: 4.4396\n",
            "Epoch 00027: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.5942 - mae: 4.4396 - val_loss: 3.8842 - val_mae: 7.5455 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.5954 - mae: 4.3606\n",
            "Epoch 00028: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.5954 - mae: 4.3606 - val_loss: 3.9178 - val_mae: 8.0956 - lr: 9.7656e-07\n",
            "Epoch 29/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6206 - mae: 4.3096\n",
            "Epoch 00029: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6206 - mae: 4.3096 - val_loss: 3.9503 - val_mae: 7.8435 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6006 - mae: 4.3340\n",
            "Epoch 00030: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.6006 - mae: 4.3340 - val_loss: 3.9190 - val_mae: 7.7847 - lr: 4.8828e-07\n",
            "Epoch 31/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.5991 - mae: 4.4543\n",
            "Epoch 00031: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.5991 - mae: 4.4543 - val_loss: 3.8955 - val_mae: 8.0201 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6016 - mae: 4.3815\n",
            "Epoch 00032: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6016 - mae: 4.3815 - val_loss: 3.9927 - val_mae: 7.8639 - lr: 2.4414e-07\n",
            "Epoch 33/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6095 - mae: 4.3618\n",
            "Epoch 00033: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6095 - mae: 4.3618 - val_loss: 3.8806 - val_mae: 7.6494 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6155 - mae: 4.4109\n",
            "Epoch 00034: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6155 - mae: 4.4109 - val_loss: 3.8123 - val_mae: 7.7322 - lr: 1.2207e-07\n",
            "Epoch 35/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.5967 - mae: 4.3512\n",
            "Epoch 00035: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.5967 - mae: 4.3512 - val_loss: 3.8965 - val_mae: 7.6159 - lr: 6.1035e-08\n",
            "Epoch 36/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6021 - mae: 4.3074\n",
            "Epoch 00036: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.6021 - mae: 4.3074 - val_loss: 3.8325 - val_mae: 7.7000 - lr: 6.1035e-08\n",
            "Epoch 37/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.5913 - mae: 4.4051\n",
            "Epoch 00037: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.5913 - mae: 4.4051 - val_loss: 3.8808 - val_mae: 7.7222 - lr: 3.0518e-08\n",
            "Epoch 38/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6131 - mae: 4.4409\n",
            "Epoch 00038: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6131 - mae: 4.4409 - val_loss: 4.0056 - val_mae: 7.9408 - lr: 3.0518e-08\n",
            "Epoch 39/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6133 - mae: 4.4307\n",
            "Epoch 00039: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.6133 - mae: 4.4307 - val_loss: 3.9941 - val_mae: 8.0240 - lr: 1.5259e-08\n",
            "Epoch 40/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.5995 - mae: 4.4200\n",
            "Epoch 00040: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.5995 - mae: 4.4200 - val_loss: 4.0746 - val_mae: 8.2080 - lr: 1.5259e-08\n",
            "Epoch 41/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.5998 - mae: 4.2545\n",
            "Epoch 00041: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.5998 - mae: 4.2545 - val_loss: 4.2464 - val_mae: 8.6248 - lr: 7.6294e-09\n",
            "Epoch 42/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.6090 - mae: 4.3713\n",
            "Epoch 00042: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.6090 - mae: 4.3713 - val_loss: 4.0097 - val_mae: 7.9123 - lr: 7.6294e-09\n",
            "Epoch 43/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.5962 - mae: 4.4780\n",
            "Epoch 00043: val_loss did not improve from 3.56304\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.5962 - mae: 4.4780 - val_loss: 4.0620 - val_mae: 8.4029 - lr: 3.8147e-09\n",
            "Epoch 44/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.5947 - mae: 4.3827Restoring model weights from the end of the best epoch: 14.\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 3.56304\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.5947 - mae: 4.3827 - val_loss: 3.9607 - val_mae: 7.7418 - lr: 3.8147e-09\n",
            "Epoch 00044: early stopping\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 15.9294 - mae: 45.8601\n",
            "Fit model on training data fold  11\n",
            "Epoch 1/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 32.7032 - mae: 47.1595\n",
            "Epoch 00001: val_loss improved from inf to 7.29954, saving model to /home/jupyter/IEEE/fold11.h5\n",
            "83/83 [==============================] - 20s 133ms/step - loss: 32.7032 - mae: 47.1595 - val_loss: 7.2995 - val_mae: 27.8338 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 4.4218 - mae: 14.4838\n",
            "Epoch 00002: val_loss did not improve from 7.29954\n",
            "83/83 [==============================] - 8s 92ms/step - loss: 4.4218 - mae: 14.4838 - val_loss: 7.7293 - val_mae: 29.9924 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.8930 - mae: 12.0374\n",
            "Epoch 00003: val_loss improved from 7.29954 to 7.14091, saving model to /home/jupyter/IEEE/fold11.h5\n",
            "83/83 [==============================] - 8s 94ms/step - loss: 3.8930 - mae: 12.0374 - val_loss: 7.1409 - val_mae: 27.8432 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.7020 - mae: 11.0621\n",
            "Epoch 00004: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 8s 93ms/step - loss: 3.7020 - mae: 11.0621 - val_loss: 7.6926 - val_mae: 28.0005 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.5472 - mae: 9.5573\n",
            "Epoch 00005: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 3.5472 - mae: 9.5573 - val_loss: 7.6927 - val_mae: 27.4988 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.3203 - mae: 8.6572\n",
            "Epoch 00006: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 8s 90ms/step - loss: 3.3203 - mae: 8.6572 - val_loss: 9.6739 - val_mae: 31.3126 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.2473 - mae: 7.7999\n",
            "Epoch 00007: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "83/83 [==============================] - 7s 90ms/step - loss: 3.2473 - mae: 7.7999 - val_loss: 9.2179 - val_mae: 29.7193 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.1268 - mae: 7.4920\n",
            "Epoch 00008: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 3.1268 - mae: 7.4920 - val_loss: 10.1988 - val_mae: 29.7124 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 3.0540 - mae: 6.9816\n",
            "Epoch 00009: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 3.0540 - mae: 6.9816 - val_loss: 10.0801 - val_mae: 29.5795 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.9682 - mae: 6.4388\n",
            "Epoch 00010: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.9682 - mae: 6.4388 - val_loss: 9.4658 - val_mae: 27.1335 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.9282 - mae: 6.1185\n",
            "Epoch 00011: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 2.9282 - mae: 6.1185 - val_loss: 9.5085 - val_mae: 26.7565 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.8750 - mae: 5.9463\n",
            "Epoch 00012: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 2.8750 - mae: 5.9463 - val_loss: 9.0919 - val_mae: 25.3842 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.8563 - mae: 6.0056\n",
            "Epoch 00013: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 2.8563 - mae: 6.0056 - val_loss: 8.7371 - val_mae: 24.2061 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.8301 - mae: 5.6085\n",
            "Epoch 00014: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 2.8301 - mae: 5.6085 - val_loss: 9.3355 - val_mae: 24.9393 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.8077 - mae: 5.6367\n",
            "Epoch 00015: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.8077 - mae: 5.6367 - val_loss: 10.3352 - val_mae: 27.1222 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7932 - mae: 5.4618\n",
            "Epoch 00016: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 2.7932 - mae: 5.4618 - val_loss: 10.1062 - val_mae: 26.2937 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7887 - mae: 5.3770\n",
            "Epoch 00017: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.7887 - mae: 5.3770 - val_loss: 9.7935 - val_mae: 25.0150 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7782 - mae: 5.4566\n",
            "Epoch 00018: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 2.7782 - mae: 5.4566 - val_loss: 9.8405 - val_mae: 26.0283 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7689 - mae: 5.4360\n",
            "Epoch 00019: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 2.7689 - mae: 5.4360 - val_loss: 9.9858 - val_mae: 26.2021 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7702 - mae: 5.4796\n",
            "Epoch 00020: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.7702 - mae: 5.4796 - val_loss: 9.9801 - val_mae: 26.3690 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7689 - mae: 5.1941\n",
            "Epoch 00021: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 2.7689 - mae: 5.1941 - val_loss: 10.0511 - val_mae: 26.2405 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7697 - mae: 5.3048\n",
            "Epoch 00022: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 2.7697 - mae: 5.3048 - val_loss: 9.8907 - val_mae: 25.6392 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7679 - mae: 5.2926\n",
            "Epoch 00023: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "83/83 [==============================] - 7s 87ms/step - loss: 2.7679 - mae: 5.2926 - val_loss: 9.8003 - val_mae: 24.7922 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7739 - mae: 5.3921\n",
            "Epoch 00024: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 2.7739 - mae: 5.3921 - val_loss: 9.7938 - val_mae: 25.2904 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7638 - mae: 5.3665\n",
            "Epoch 00025: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.7638 - mae: 5.3665 - val_loss: 10.4915 - val_mae: 26.7285 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7671 - mae: 5.3589\n",
            "Epoch 00026: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.7671 - mae: 5.3589 - val_loss: 9.9917 - val_mae: 25.9059 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7625 - mae: 5.3538\n",
            "Epoch 00027: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.7625 - mae: 5.3538 - val_loss: 9.9232 - val_mae: 26.2813 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7660 - mae: 5.3644\n",
            "Epoch 00028: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.7660 - mae: 5.3644 - val_loss: 9.9413 - val_mae: 25.4801 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7627 - mae: 5.4832\n",
            "Epoch 00029: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.7627 - mae: 5.4832 - val_loss: 9.6753 - val_mae: 24.5777 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7514 - mae: 5.3555\n",
            "Epoch 00030: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.7514 - mae: 5.3555 - val_loss: 9.9357 - val_mae: 25.2656 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7552 - mae: 5.4003\n",
            "Epoch 00031: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 2.7552 - mae: 5.4003 - val_loss: 10.1077 - val_mae: 25.7034 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7535 - mae: 5.3654\n",
            "Epoch 00032: val_loss did not improve from 7.14091\n",
            "83/83 [==============================] - 7s 88ms/step - loss: 2.7535 - mae: 5.3654 - val_loss: 9.9615 - val_mae: 26.0116 - lr: 6.1035e-08\n",
            "Epoch 33/200\n",
            "83/83 [==============================] - ETA: 0s - loss: 2.7523 - mae: 5.3827Restoring model weights from the end of the best epoch: 3.\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 7.14091\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "83/83 [==============================] - 7s 89ms/step - loss: 2.7523 - mae: 5.3827 - val_loss: 9.9298 - val_mae: 25.7803 - lr: 6.1035e-08\n",
            "Epoch 00033: early stopping\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 3.9334 - mae: 12.2820\n",
            "Fit model on training data fold  12\n",
            "Epoch 1/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 45.6154 - mae: 63.7049\n",
            "Epoch 00001: val_loss improved from inf to 4.45229, saving model to /home/jupyter/IEEE/fold12.h5\n",
            "84/84 [==============================] - 20s 130ms/step - loss: 45.6154 - mae: 63.7049 - val_loss: 4.4523 - val_mae: 17.0355 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.7110 - mae: 18.7346\n",
            "Epoch 00002: val_loss improved from 4.45229 to 4.19716, saving model to /home/jupyter/IEEE/fold12.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 4.7110 - mae: 18.7346 - val_loss: 4.1972 - val_mae: 16.5527 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.2119 - mae: 15.8380\n",
            "Epoch 00003: val_loss did not improve from 4.19716\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 4.2119 - mae: 15.8380 - val_loss: 4.2148 - val_mae: 15.7652 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.9453 - mae: 13.7640\n",
            "Epoch 00004: val_loss improved from 4.19716 to 3.92240, saving model to /home/jupyter/IEEE/fold12.h5\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 3.9453 - mae: 13.7640 - val_loss: 3.9224 - val_mae: 12.9849 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.8259 - mae: 12.7581\n",
            "Epoch 00005: val_loss improved from 3.92240 to 3.88637, saving model to /home/jupyter/IEEE/fold12.h5\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 3.8259 - mae: 12.7581 - val_loss: 3.8864 - val_mae: 12.8125 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.6637 - mae: 11.3886\n",
            "Epoch 00006: val_loss did not improve from 3.88637\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.6637 - mae: 11.3886 - val_loss: 3.9206 - val_mae: 12.8630 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.5394 - mae: 10.4338\n",
            "Epoch 00007: val_loss did not improve from 3.88637\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.5394 - mae: 10.4338 - val_loss: 3.9779 - val_mae: 11.3979 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.3208 - mae: 8.6564\n",
            "Epoch 00008: val_loss did not improve from 3.88637\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 3.3208 - mae: 8.6564 - val_loss: 4.1178 - val_mae: 12.0233 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.2335 - mae: 7.9439\n",
            "Epoch 00009: val_loss did not improve from 3.88637\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 3.2335 - mae: 7.9439 - val_loss: 4.2599 - val_mae: 10.8961 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0632 - mae: 6.9185\n",
            "Epoch 00010: val_loss did not improve from 3.88637\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.0632 - mae: 6.9185 - val_loss: 4.0023 - val_mae: 9.2486 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0012 - mae: 6.6019\n",
            "Epoch 00011: val_loss improved from 3.88637 to 3.86818, saving model to /home/jupyter/IEEE/fold12.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 3.0012 - mae: 6.6019 - val_loss: 3.8682 - val_mae: 9.3684 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9147 - mae: 6.1118\n",
            "Epoch 00012: val_loss improved from 3.86818 to 3.81628, saving model to /home/jupyter/IEEE/fold12.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.9147 - mae: 6.1118 - val_loss: 3.8163 - val_mae: 8.7532 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8441 - mae: 5.8389\n",
            "Epoch 00013: val_loss improved from 3.81628 to 3.80511, saving model to /home/jupyter/IEEE/fold12.h5\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 2.8441 - mae: 5.8389 - val_loss: 3.8051 - val_mae: 8.8703 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8213 - mae: 5.3919\n",
            "Epoch 00014: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.8213 - mae: 5.3919 - val_loss: 4.0283 - val_mae: 7.8753 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.6922 - mae: 4.8014\n",
            "Epoch 00015: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.6922 - mae: 4.8014 - val_loss: 4.4523 - val_mae: 8.8452 - lr: 2.5000e-04\n",
            "Epoch 16/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.6014 - mae: 4.5379\n",
            "Epoch 00016: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.6014 - mae: 4.5379 - val_loss: 4.0936 - val_mae: 7.6921 - lr: 1.2500e-04\n",
            "Epoch 17/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5428 - mae: 4.1523\n",
            "Epoch 00017: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.5428 - mae: 4.1523 - val_loss: 4.0651 - val_mae: 7.0504 - lr: 1.2500e-04\n",
            "Epoch 18/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4702 - mae: 3.8557\n",
            "Epoch 00018: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.4702 - mae: 3.8557 - val_loss: 4.1536 - val_mae: 7.4356 - lr: 6.2500e-05\n",
            "Epoch 19/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.4288 - mae: 3.7208\n",
            "Epoch 00019: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.4288 - mae: 3.7208 - val_loss: 4.2772 - val_mae: 6.9378 - lr: 6.2500e-05\n",
            "Epoch 20/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3742 - mae: 3.5416\n",
            "Epoch 00020: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.3742 - mae: 3.5416 - val_loss: 4.1848 - val_mae: 6.6182 - lr: 3.1250e-05\n",
            "Epoch 21/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3483 - mae: 3.5345\n",
            "Epoch 00021: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.3483 - mae: 3.5345 - val_loss: 4.2064 - val_mae: 6.6178 - lr: 3.1250e-05\n",
            "Epoch 22/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3357 - mae: 3.3468\n",
            "Epoch 00022: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.3357 - mae: 3.3468 - val_loss: 4.2930 - val_mae: 7.0550 - lr: 1.5625e-05\n",
            "Epoch 23/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3128 - mae: 3.3554\n",
            "Epoch 00023: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.3128 - mae: 3.3554 - val_loss: 4.3904 - val_mae: 7.1792 - lr: 1.5625e-05\n",
            "Epoch 24/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3033 - mae: 3.3293\n",
            "Epoch 00024: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.3033 - mae: 3.3293 - val_loss: 4.3057 - val_mae: 6.9192 - lr: 7.8125e-06\n",
            "Epoch 25/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2910 - mae: 3.2916\n",
            "Epoch 00025: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.2910 - mae: 3.2916 - val_loss: 4.3948 - val_mae: 6.8749 - lr: 7.8125e-06\n",
            "Epoch 26/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2884 - mae: 3.1962\n",
            "Epoch 00026: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.2884 - mae: 3.1962 - val_loss: 4.3994 - val_mae: 6.7878 - lr: 3.9063e-06\n",
            "Epoch 27/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2741 - mae: 3.2287\n",
            "Epoch 00027: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.2741 - mae: 3.2287 - val_loss: 4.3776 - val_mae: 7.1452 - lr: 3.9063e-06\n",
            "Epoch 28/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2782 - mae: 3.2310\n",
            "Epoch 00028: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.2782 - mae: 3.2310 - val_loss: 4.3005 - val_mae: 6.9743 - lr: 1.9531e-06\n",
            "Epoch 29/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2795 - mae: 3.2872\n",
            "Epoch 00029: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.2795 - mae: 3.2872 - val_loss: 4.3962 - val_mae: 6.9707 - lr: 1.9531e-06\n",
            "Epoch 30/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2782 - mae: 3.3054\n",
            "Epoch 00030: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.2782 - mae: 3.3054 - val_loss: 4.3469 - val_mae: 6.9120 - lr: 9.7656e-07\n",
            "Epoch 31/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2754 - mae: 3.2420\n",
            "Epoch 00031: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.2754 - mae: 3.2420 - val_loss: 4.2492 - val_mae: 6.5313 - lr: 9.7656e-07\n",
            "Epoch 32/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2799 - mae: 3.1822\n",
            "Epoch 00032: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.2799 - mae: 3.1822 - val_loss: 4.3444 - val_mae: 6.5466 - lr: 4.8828e-07\n",
            "Epoch 33/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2752 - mae: 3.2514\n",
            "Epoch 00033: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 2.2752 - mae: 3.2514 - val_loss: 4.3463 - val_mae: 6.8746 - lr: 4.8828e-07\n",
            "Epoch 34/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2698 - mae: 3.2068\n",
            "Epoch 00034: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.2698 - mae: 3.2068 - val_loss: 4.5126 - val_mae: 7.3938 - lr: 2.4414e-07\n",
            "Epoch 35/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2656 - mae: 3.2556\n",
            "Epoch 00035: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.2656 - mae: 3.2556 - val_loss: 4.3723 - val_mae: 6.3738 - lr: 2.4414e-07\n",
            "Epoch 36/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2562 - mae: 3.1780\n",
            "Epoch 00036: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.2562 - mae: 3.1780 - val_loss: 4.3671 - val_mae: 6.7077 - lr: 1.2207e-07\n",
            "Epoch 37/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2646 - mae: 3.2111\n",
            "Epoch 00037: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.2646 - mae: 3.2111 - val_loss: 4.2347 - val_mae: 6.5266 - lr: 1.2207e-07\n",
            "Epoch 38/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2704 - mae: 3.2440\n",
            "Epoch 00038: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.2704 - mae: 3.2440 - val_loss: 4.3536 - val_mae: 6.8852 - lr: 6.1035e-08\n",
            "Epoch 39/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2835 - mae: 3.2000\n",
            "Epoch 00039: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "84/84 [==============================] - 8s 89ms/step - loss: 2.2835 - mae: 3.2000 - val_loss: 4.2794 - val_mae: 6.9498 - lr: 6.1035e-08\n",
            "Epoch 40/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2668 - mae: 3.2395\n",
            "Epoch 00040: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.2668 - mae: 3.2395 - val_loss: 4.4067 - val_mae: 7.2144 - lr: 3.0518e-08\n",
            "Epoch 41/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2789 - mae: 3.1140\n",
            "Epoch 00041: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.2789 - mae: 3.1140 - val_loss: 4.4353 - val_mae: 7.1502 - lr: 3.0518e-08\n",
            "Epoch 42/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2668 - mae: 3.1902\n",
            "Epoch 00042: val_loss did not improve from 3.80511\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.2668 - mae: 3.1902 - val_loss: 4.3806 - val_mae: 6.9781 - lr: 1.5259e-08\n",
            "Epoch 43/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2629 - mae: 3.1115Restoring model weights from the end of the best epoch: 13.\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 3.80511\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.2629 - mae: 3.1115 - val_loss: 4.4519 - val_mae: 6.8925 - lr: 1.5259e-08\n",
            "Epoch 00043: early stopping\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 3.5131 - mae: 7.4767\n",
            "Fit model on training data fold  13\n",
            "Epoch 1/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 66.4100 - mae: 72.1760\n",
            "Epoch 00001: val_loss improved from inf to 5.42084, saving model to /home/jupyter/IEEE/fold13.h5\n",
            "84/84 [==============================] - 21s 134ms/step - loss: 66.4100 - mae: 72.1760 - val_loss: 5.4208 - val_mae: 21.8473 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.9100 - mae: 20.0689\n",
            "Epoch 00002: val_loss improved from 5.42084 to 4.79994, saving model to /home/jupyter/IEEE/fold13.h5\n",
            "84/84 [==============================] - 9s 105ms/step - loss: 4.9100 - mae: 20.0689 - val_loss: 4.7999 - val_mae: 20.7232 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.4193 - mae: 17.7009\n",
            "Epoch 00003: val_loss improved from 4.79994 to 4.68064, saving model to /home/jupyter/IEEE/fold13.h5\n",
            "84/84 [==============================] - 9s 105ms/step - loss: 4.4193 - mae: 17.7009 - val_loss: 4.6806 - val_mae: 21.2671 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.1271 - mae: 16.0616\n",
            "Epoch 00004: val_loss improved from 4.68064 to 4.40536, saving model to /home/jupyter/IEEE/fold13.h5\n",
            "84/84 [==============================] - 9s 103ms/step - loss: 4.1271 - mae: 16.0616 - val_loss: 4.4054 - val_mae: 16.4737 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.8793 - mae: 13.5373\n",
            "Epoch 00005: val_loss improved from 4.40536 to 4.39275, saving model to /home/jupyter/IEEE/fold13.h5\n",
            "84/84 [==============================] - 9s 102ms/step - loss: 3.8793 - mae: 13.5373 - val_loss: 4.3927 - val_mae: 18.5785 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.7045 - mae: 12.0985\n",
            "Epoch 00006: val_loss improved from 4.39275 to 4.04127, saving model to /home/jupyter/IEEE/fold13.h5\n",
            "84/84 [==============================] - 8s 101ms/step - loss: 3.7045 - mae: 12.0985 - val_loss: 4.0413 - val_mae: 14.4413 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.5908 - mae: 10.6732\n",
            "Epoch 00007: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 8s 98ms/step - loss: 3.5908 - mae: 10.6732 - val_loss: 4.4294 - val_mae: 15.8098 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.4949 - mae: 9.8179\n",
            "Epoch 00008: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "84/84 [==============================] - 8s 95ms/step - loss: 3.4949 - mae: 9.8179 - val_loss: 4.2886 - val_mae: 13.4682 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.2403 - mae: 8.2019\n",
            "Epoch 00009: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 8s 95ms/step - loss: 3.2403 - mae: 8.2019 - val_loss: 4.2843 - val_mae: 12.6564 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0845 - mae: 7.1276\n",
            "Epoch 00010: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "84/84 [==============================] - 8s 93ms/step - loss: 3.0845 - mae: 7.1276 - val_loss: 4.2475 - val_mae: 11.1335 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9820 - mae: 6.4324\n",
            "Epoch 00011: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 8s 94ms/step - loss: 2.9820 - mae: 6.4324 - val_loss: 4.4603 - val_mae: 10.9028 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8787 - mae: 6.0104\n",
            "Epoch 00012: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "84/84 [==============================] - 8s 93ms/step - loss: 2.8787 - mae: 6.0104 - val_loss: 4.4310 - val_mae: 9.7899 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7824 - mae: 5.4170\n",
            "Epoch 00013: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 8s 93ms/step - loss: 2.7824 - mae: 5.4170 - val_loss: 4.4726 - val_mae: 9.8734 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7187 - mae: 5.0141\n",
            "Epoch 00014: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "84/84 [==============================] - 8s 94ms/step - loss: 2.7187 - mae: 5.0141 - val_loss: 4.4138 - val_mae: 9.4462 - lr: 1.2500e-04\n",
            "Epoch 15/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.6700 - mae: 4.8817\n",
            "Epoch 00015: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 8s 94ms/step - loss: 2.6700 - mae: 4.8817 - val_loss: 4.5986 - val_mae: 9.5279 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.6187 - mae: 4.6050\n",
            "Epoch 00016: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "84/84 [==============================] - 8s 93ms/step - loss: 2.6187 - mae: 4.6050 - val_loss: 4.5281 - val_mae: 9.1606 - lr: 6.2500e-05\n",
            "Epoch 17/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5976 - mae: 4.4584\n",
            "Epoch 00017: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 8s 95ms/step - loss: 2.5976 - mae: 4.4584 - val_loss: 4.5469 - val_mae: 9.4385 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5720 - mae: 4.4491\n",
            "Epoch 00018: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 2.5720 - mae: 4.4491 - val_loss: 4.5271 - val_mae: 9.1352 - lr: 3.1250e-05\n",
            "Epoch 19/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5555 - mae: 4.2492\n",
            "Epoch 00019: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 8s 93ms/step - loss: 2.5555 - mae: 4.2492 - val_loss: 4.5672 - val_mae: 9.0576 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5461 - mae: 4.2725\n",
            "Epoch 00020: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.5461 - mae: 4.2725 - val_loss: 4.6074 - val_mae: 8.6487 - lr: 1.5625e-05\n",
            "Epoch 21/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5407 - mae: 4.2153\n",
            "Epoch 00021: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 2.5407 - mae: 4.2153 - val_loss: 4.6332 - val_mae: 8.9564 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5287 - mae: 4.2133\n",
            "Epoch 00022: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "84/84 [==============================] - 8s 93ms/step - loss: 2.5287 - mae: 4.2133 - val_loss: 4.6289 - val_mae: 8.9700 - lr: 7.8125e-06\n",
            "Epoch 23/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5265 - mae: 4.2247\n",
            "Epoch 00023: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 2.5265 - mae: 4.2247 - val_loss: 4.6146 - val_mae: 8.7506 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5249 - mae: 4.1020\n",
            "Epoch 00024: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.5249 - mae: 4.1020 - val_loss: 4.6405 - val_mae: 8.9654 - lr: 3.9063e-06\n",
            "Epoch 25/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5176 - mae: 4.0838\n",
            "Epoch 00025: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.5176 - mae: 4.0838 - val_loss: 4.6163 - val_mae: 8.7978 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5178 - mae: 4.1602\n",
            "Epoch 00026: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.5178 - mae: 4.1602 - val_loss: 4.6539 - val_mae: 8.7327 - lr: 1.9531e-06\n",
            "Epoch 27/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5147 - mae: 4.1168\n",
            "Epoch 00027: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.5147 - mae: 4.1168 - val_loss: 4.6293 - val_mae: 8.8884 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5116 - mae: 4.2286\n",
            "Epoch 00028: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.5116 - mae: 4.2286 - val_loss: 4.5506 - val_mae: 8.8471 - lr: 9.7656e-07\n",
            "Epoch 29/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5000 - mae: 4.0452\n",
            "Epoch 00029: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.5000 - mae: 4.0452 - val_loss: 4.5969 - val_mae: 8.5889 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5204 - mae: 4.0599\n",
            "Epoch 00030: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.5204 - mae: 4.0599 - val_loss: 4.6440 - val_mae: 9.4144 - lr: 4.8828e-07\n",
            "Epoch 31/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5197 - mae: 4.1077\n",
            "Epoch 00031: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.5197 - mae: 4.1077 - val_loss: 4.5574 - val_mae: 9.0858 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5156 - mae: 4.2139\n",
            "Epoch 00032: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.5156 - mae: 4.2139 - val_loss: 4.5943 - val_mae: 8.7629 - lr: 2.4414e-07\n",
            "Epoch 33/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5034 - mae: 4.0963\n",
            "Epoch 00033: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.5034 - mae: 4.0963 - val_loss: 4.6442 - val_mae: 8.8463 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5089 - mae: 4.1872\n",
            "Epoch 00034: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.5089 - mae: 4.1872 - val_loss: 4.5703 - val_mae: 8.7732 - lr: 1.2207e-07\n",
            "Epoch 35/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5082 - mae: 4.1182\n",
            "Epoch 00035: val_loss did not improve from 4.04127\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.5082 - mae: 4.1182 - val_loss: 4.7785 - val_mae: 8.9688 - lr: 6.1035e-08\n",
            "Epoch 36/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5102 - mae: 4.1645Restoring model weights from the end of the best epoch: 6.\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 4.04127\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.5102 - mae: 4.1645 - val_loss: 4.5694 - val_mae: 8.8354 - lr: 6.1035e-08\n",
            "Epoch 00036: early stopping\n",
            "5/5 [==============================] - 2s 32ms/step - loss: 3.6372 - mae: 11.3562\n",
            "Fit model on training data fold  14\n",
            "Epoch 1/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 33.4928 - mae: 48.3914\n",
            "Epoch 00001: val_loss improved from inf to 4.94322, saving model to /home/jupyter/IEEE/fold14.h5\n",
            "84/84 [==============================] - 21s 137ms/step - loss: 33.4928 - mae: 48.3914 - val_loss: 4.9432 - val_mae: 19.0943 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.6299 - mae: 17.2661\n",
            "Epoch 00002: val_loss improved from 4.94322 to 4.34363, saving model to /home/jupyter/IEEE/fold14.h5\n",
            "84/84 [==============================] - 8s 101ms/step - loss: 4.6299 - mae: 17.2661 - val_loss: 4.3436 - val_mae: 16.0511 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.1720 - mae: 14.8839\n",
            "Epoch 00003: val_loss improved from 4.34363 to 4.05072, saving model to /home/jupyter/IEEE/fold14.h5\n",
            "84/84 [==============================] - 8s 96ms/step - loss: 4.1720 - mae: 14.8839 - val_loss: 4.0507 - val_mae: 14.4336 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.8878 - mae: 13.4010\n",
            "Epoch 00004: val_loss improved from 4.05072 to 3.86645, saving model to /home/jupyter/IEEE/fold14.h5\n",
            "84/84 [==============================] - 8s 94ms/step - loss: 3.8878 - mae: 13.4010 - val_loss: 3.8664 - val_mae: 13.0699 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.7307 - mae: 11.6979\n",
            "Epoch 00005: val_loss improved from 3.86645 to 3.79921, saving model to /home/jupyter/IEEE/fold14.h5\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 3.7307 - mae: 11.6979 - val_loss: 3.7992 - val_mae: 11.8299 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.5991 - mae: 10.5891\n",
            "Epoch 00006: val_loss improved from 3.79921 to 3.72658, saving model to /home/jupyter/IEEE/fold14.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 3.5991 - mae: 10.5891 - val_loss: 3.7266 - val_mae: 11.1808 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.5333 - mae: 9.7968\n",
            "Epoch 00007: val_loss did not improve from 3.72658\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 3.5333 - mae: 9.7968 - val_loss: 4.0435 - val_mae: 11.1204 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.4969 - mae: 9.2808\n",
            "Epoch 00008: val_loss did not improve from 3.72658\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.4969 - mae: 9.2808 - val_loss: 3.8899 - val_mae: 10.6726 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.2007 - mae: 7.7566\n",
            "Epoch 00009: val_loss improved from 3.72658 to 3.56525, saving model to /home/jupyter/IEEE/fold14.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 3.2007 - mae: 7.7566 - val_loss: 3.5652 - val_mae: 8.9294 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0729 - mae: 6.8754\n",
            "Epoch 00010: val_loss improved from 3.56525 to 3.26845, saving model to /home/jupyter/IEEE/fold14.h5\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 3.0729 - mae: 6.8754 - val_loss: 3.2684 - val_mae: 7.7735 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9799 - mae: 6.2966\n",
            "Epoch 00011: val_loss did not improve from 3.26845\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.9799 - mae: 6.2966 - val_loss: 3.5444 - val_mae: 7.1995 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9461 - mae: 5.7390\n",
            "Epoch 00012: val_loss did not improve from 3.26845\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.9461 - mae: 5.7390 - val_loss: 3.4057 - val_mae: 7.0733 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7931 - mae: 5.1042\n",
            "Epoch 00013: val_loss improved from 3.26845 to 3.17317, saving model to /home/jupyter/IEEE/fold14.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.7931 - mae: 5.1042 - val_loss: 3.1732 - val_mae: 6.2457 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.6815 - mae: 4.5621\n",
            "Epoch 00014: val_loss improved from 3.17317 to 3.11798, saving model to /home/jupyter/IEEE/fold14.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.6815 - mae: 4.5621 - val_loss: 3.1180 - val_mae: 5.7213 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.6442 - mae: 4.2459\n",
            "Epoch 00015: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.6442 - mae: 4.2459 - val_loss: 3.3712 - val_mae: 6.1845 - lr: 2.5000e-04\n",
            "Epoch 16/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5248 - mae: 3.8567\n",
            "Epoch 00016: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.5248 - mae: 3.8567 - val_loss: 3.4739 - val_mae: 5.2997 - lr: 2.5000e-04\n",
            "Epoch 17/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3787 - mae: 3.3148\n",
            "Epoch 00017: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.3787 - mae: 3.3148 - val_loss: 3.1309 - val_mae: 4.5694 - lr: 1.2500e-04\n",
            "Epoch 18/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3194 - mae: 3.2176\n",
            "Epoch 00018: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.3194 - mae: 3.2176 - val_loss: 3.4139 - val_mae: 4.9776 - lr: 1.2500e-04\n",
            "Epoch 19/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.2330 - mae: 2.8760\n",
            "Epoch 00019: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.2330 - mae: 2.8760 - val_loss: 3.2960 - val_mae: 4.6220 - lr: 6.2500e-05\n",
            "Epoch 20/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.1817 - mae: 2.6969\n",
            "Epoch 00020: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.1817 - mae: 2.6969 - val_loss: 3.3409 - val_mae: 4.7384 - lr: 6.2500e-05\n",
            "Epoch 21/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.1486 - mae: 2.6278\n",
            "Epoch 00021: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.1486 - mae: 2.6278 - val_loss: 3.3050 - val_mae: 4.2448 - lr: 3.1250e-05\n",
            "Epoch 22/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.1092 - mae: 2.5641\n",
            "Epoch 00022: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.1092 - mae: 2.5641 - val_loss: 3.3596 - val_mae: 4.3624 - lr: 3.1250e-05\n",
            "Epoch 23/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0973 - mae: 2.5443\n",
            "Epoch 00023: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0973 - mae: 2.5443 - val_loss: 3.5126 - val_mae: 4.5090 - lr: 1.5625e-05\n",
            "Epoch 24/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0694 - mae: 2.4896\n",
            "Epoch 00024: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0694 - mae: 2.4896 - val_loss: 3.2500 - val_mae: 4.0055 - lr: 1.5625e-05\n",
            "Epoch 25/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0582 - mae: 2.4687\n",
            "Epoch 00025: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0582 - mae: 2.4687 - val_loss: 3.3076 - val_mae: 4.2447 - lr: 7.8125e-06\n",
            "Epoch 26/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0420 - mae: 2.3876\n",
            "Epoch 00026: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0420 - mae: 2.3876 - val_loss: 3.3766 - val_mae: 4.5456 - lr: 7.8125e-06\n",
            "Epoch 27/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0459 - mae: 2.3933\n",
            "Epoch 00027: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.0459 - mae: 2.3933 - val_loss: 3.3155 - val_mae: 3.9967 - lr: 3.9063e-06\n",
            "Epoch 28/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0233 - mae: 2.3701\n",
            "Epoch 00028: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.0233 - mae: 2.3701 - val_loss: 3.3445 - val_mae: 4.0687 - lr: 3.9063e-06\n",
            "Epoch 29/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0403 - mae: 2.4005\n",
            "Epoch 00029: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.0403 - mae: 2.4005 - val_loss: 3.3555 - val_mae: 4.0940 - lr: 1.9531e-06\n",
            "Epoch 30/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0346 - mae: 2.3150\n",
            "Epoch 00030: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0346 - mae: 2.3150 - val_loss: 3.2501 - val_mae: 4.0661 - lr: 1.9531e-06\n",
            "Epoch 31/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0487 - mae: 2.3617\n",
            "Epoch 00031: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.0487 - mae: 2.3617 - val_loss: 3.4468 - val_mae: 4.0835 - lr: 9.7656e-07\n",
            "Epoch 32/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0076 - mae: 2.2986\n",
            "Epoch 00032: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0076 - mae: 2.2986 - val_loss: 3.3747 - val_mae: 4.1924 - lr: 9.7656e-07\n",
            "Epoch 33/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0289 - mae: 2.3354\n",
            "Epoch 00033: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0289 - mae: 2.3354 - val_loss: 3.3568 - val_mae: 4.3472 - lr: 4.8828e-07\n",
            "Epoch 34/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0133 - mae: 2.3029\n",
            "Epoch 00034: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0133 - mae: 2.3029 - val_loss: 3.5222 - val_mae: 4.3387 - lr: 4.8828e-07\n",
            "Epoch 35/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0110 - mae: 2.3939\n",
            "Epoch 00035: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0110 - mae: 2.3939 - val_loss: 3.4315 - val_mae: 4.3271 - lr: 2.4414e-07\n",
            "Epoch 36/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0192 - mae: 2.3768\n",
            "Epoch 00036: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0192 - mae: 2.3768 - val_loss: 3.3201 - val_mae: 4.0593 - lr: 2.4414e-07\n",
            "Epoch 37/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0082 - mae: 2.3554\n",
            "Epoch 00037: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0082 - mae: 2.3554 - val_loss: 3.5279 - val_mae: 4.3451 - lr: 1.2207e-07\n",
            "Epoch 38/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0316 - mae: 2.3388\n",
            "Epoch 00038: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0316 - mae: 2.3388 - val_loss: 3.5333 - val_mae: 4.5575 - lr: 1.2207e-07\n",
            "Epoch 39/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0419 - mae: 2.3753\n",
            "Epoch 00039: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0419 - mae: 2.3753 - val_loss: 3.2749 - val_mae: 4.2026 - lr: 6.1035e-08\n",
            "Epoch 40/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0482 - mae: 2.4094\n",
            "Epoch 00040: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0482 - mae: 2.4094 - val_loss: 3.4777 - val_mae: 4.1838 - lr: 6.1035e-08\n",
            "Epoch 41/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0169 - mae: 2.3214\n",
            "Epoch 00041: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0169 - mae: 2.3214 - val_loss: 3.3659 - val_mae: 4.1149 - lr: 3.0518e-08\n",
            "Epoch 42/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0385 - mae: 2.3844\n",
            "Epoch 00042: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0385 - mae: 2.3844 - val_loss: 3.2039 - val_mae: 4.0254 - lr: 3.0518e-08\n",
            "Epoch 43/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0323 - mae: 2.4004\n",
            "Epoch 00043: val_loss did not improve from 3.11798\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0323 - mae: 2.4004 - val_loss: 3.4901 - val_mae: 4.2532 - lr: 1.5259e-08\n",
            "Epoch 44/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0109 - mae: 2.3330Restoring model weights from the end of the best epoch: 14.\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 3.11798\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0109 - mae: 2.3330 - val_loss: 3.4212 - val_mae: 4.2447 - lr: 1.5259e-08\n",
            "Epoch 00044: early stopping\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 7.5469 - mae: 14.8643\n",
            "Fit model on training data fold  15\n",
            "Epoch 1/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 55.5590 - mae: 59.3638\n",
            "Epoch 00001: val_loss improved from inf to 4.66463, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 21s 143ms/step - loss: 55.5590 - mae: 59.3638 - val_loss: 4.6646 - val_mae: 16.0191 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 4.5312 - mae: 15.2818\n",
            "Epoch 00002: val_loss improved from 4.66463 to 4.47126, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 4.5327 - mae: 15.2976 - val_loss: 4.4713 - val_mae: 15.8912 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.9859 - mae: 13.0667\n",
            "Epoch 00003: val_loss improved from 4.47126 to 3.69110, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.9859 - mae: 13.0667 - val_loss: 3.6911 - val_mae: 11.4115 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 3.7687 - mae: 11.7760\n",
            "Epoch 00004: val_loss improved from 3.69110 to 3.42013, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 3.7673 - mae: 11.7692 - val_loss: 3.4201 - val_mae: 9.6268 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.6424 - mae: 11.0868\n",
            "Epoch 00005: val_loss did not improve from 3.42013\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.6424 - mae: 11.0868 - val_loss: 3.5250 - val_mae: 10.0245 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.5309 - mae: 9.8874\n",
            "Epoch 00006: val_loss did not improve from 3.42013\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.5309 - mae: 9.8874 - val_loss: 3.9847 - val_mae: 10.8773 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.2937 - mae: 8.4630\n",
            "Epoch 00007: val_loss did not improve from 3.42013\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 3.2937 - mae: 8.4630 - val_loss: 3.4301 - val_mae: 8.9373 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.1487 - mae: 7.3979\n",
            "Epoch 00008: val_loss did not improve from 3.42013\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 3.1487 - mae: 7.3979 - val_loss: 3.5722 - val_mae: 8.1613 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.1084 - mae: 6.9604\n",
            "Epoch 00009: val_loss improved from 3.42013 to 3.09396, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 3.1084 - mae: 6.9604 - val_loss: 3.0940 - val_mae: 7.2178 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9980 - mae: 6.3612\n",
            "Epoch 00010: val_loss improved from 3.09396 to 2.96784, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.9980 - mae: 6.3612 - val_loss: 2.9678 - val_mae: 6.3170 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.9086 - mae: 5.7967\n",
            "Epoch 00011: val_loss improved from 2.96784 to 2.93069, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.9081 - mae: 5.7988 - val_loss: 2.9307 - val_mae: 6.3717 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.8489 - mae: 5.4840\n",
            "Epoch 00012: val_loss improved from 2.93069 to 2.86012, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.8519 - mae: 5.5012 - val_loss: 2.8601 - val_mae: 5.8417 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8055 - mae: 5.3659\n",
            "Epoch 00013: val_loss did not improve from 2.86012\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.8055 - mae: 5.3659 - val_loss: 3.1567 - val_mae: 6.0685 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.7396 - mae: 4.8386\n",
            "Epoch 00014: val_loss did not improve from 2.86012\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.7408 - mae: 4.8419 - val_loss: 2.9965 - val_mae: 5.6015 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.6847 - mae: 4.5485\n",
            "Epoch 00015: val_loss did not improve from 2.86012\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.6847 - mae: 4.5485 - val_loss: 3.0306 - val_mae: 5.6403 - lr: 1.2500e-04\n",
            "Epoch 16/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.6009 - mae: 4.3140\n",
            "Epoch 00016: val_loss improved from 2.86012 to 2.84916, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.6009 - mae: 4.3140 - val_loss: 2.8492 - val_mae: 4.8025 - lr: 1.2500e-04\n",
            "Epoch 17/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.5386 - mae: 4.0308\n",
            "Epoch 00017: val_loss improved from 2.84916 to 2.68438, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 2.5386 - mae: 4.0269 - val_loss: 2.6844 - val_mae: 4.1006 - lr: 1.2500e-04\n",
            "Epoch 18/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5259 - mae: 3.8761\n",
            "Epoch 00018: val_loss did not improve from 2.68438\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.5259 - mae: 3.8761 - val_loss: 2.8070 - val_mae: 4.3155 - lr: 1.2500e-04\n",
            "Epoch 19/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.4913 - mae: 3.7738\n",
            "Epoch 00019: val_loss did not improve from 2.68438\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.4911 - mae: 3.7713 - val_loss: 2.7371 - val_mae: 4.5719 - lr: 1.2500e-04\n",
            "Epoch 20/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.4005 - mae: 3.4225\n",
            "Epoch 00020: val_loss improved from 2.68438 to 2.67428, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.4012 - mae: 3.4270 - val_loss: 2.6743 - val_mae: 3.8396 - lr: 6.2500e-05\n",
            "Epoch 21/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.3721 - mae: 3.2432\n",
            "Epoch 00021: val_loss improved from 2.67428 to 2.58329, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.3723 - mae: 3.2460 - val_loss: 2.5833 - val_mae: 3.6370 - lr: 6.2500e-05\n",
            "Epoch 22/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.3115 - mae: 3.0920\n",
            "Epoch 00022: val_loss did not improve from 2.58329\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.3133 - mae: 3.0916 - val_loss: 2.5913 - val_mae: 3.5258 - lr: 6.2500e-05\n",
            "Epoch 23/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.2793 - mae: 3.1134\n",
            "Epoch 00023: val_loss improved from 2.58329 to 2.52453, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.2795 - mae: 3.1150 - val_loss: 2.5245 - val_mae: 3.6378 - lr: 6.2500e-05\n",
            "Epoch 24/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.2807 - mae: 3.1089\n",
            "Epoch 00024: val_loss did not improve from 2.52453\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.2813 - mae: 3.1066 - val_loss: 2.6512 - val_mae: 3.6473 - lr: 6.2500e-05\n",
            "Epoch 25/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.2191 - mae: 2.8985\n",
            "Epoch 00025: val_loss did not improve from 2.52453\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.2219 - mae: 2.9036 - val_loss: 2.8107 - val_mae: 3.7317 - lr: 6.2500e-05\n",
            "Epoch 26/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.1801 - mae: 2.7655\n",
            "Epoch 00026: val_loss did not improve from 2.52453\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.1812 - mae: 2.7651 - val_loss: 2.6731 - val_mae: 3.6197 - lr: 3.1250e-05\n",
            "Epoch 27/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.1886 - mae: 2.7204\n",
            "Epoch 00027: val_loss did not improve from 2.52453\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.1886 - mae: 2.7204 - val_loss: 2.5975 - val_mae: 3.3326 - lr: 3.1250e-05\n",
            "Epoch 28/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.1526 - mae: 2.5883\n",
            "Epoch 00028: val_loss did not improve from 2.52453\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.1526 - mae: 2.5883 - val_loss: 2.6651 - val_mae: 3.4209 - lr: 1.5625e-05\n",
            "Epoch 29/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.1297 - mae: 2.6120\n",
            "Epoch 00029: val_loss did not improve from 2.52453\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.1309 - mae: 2.6139 - val_loss: 2.6501 - val_mae: 3.4009 - lr: 1.5625e-05\n",
            "Epoch 30/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0954 - mae: 2.5329\n",
            "Epoch 00030: val_loss did not improve from 2.52453\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.0971 - mae: 2.5319 - val_loss: 2.6043 - val_mae: 3.2591 - lr: 7.8125e-06\n",
            "Epoch 31/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0969 - mae: 2.5247\n",
            "Epoch 00031: val_loss did not improve from 2.52453\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0969 - mae: 2.5247 - val_loss: 2.6161 - val_mae: 3.3453 - lr: 7.8125e-06\n",
            "Epoch 32/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.1053 - mae: 2.5259\n",
            "Epoch 00032: val_loss did not improve from 2.52453\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.1052 - mae: 2.5272 - val_loss: 2.5979 - val_mae: 3.3278 - lr: 3.9063e-06\n",
            "Epoch 33/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0859 - mae: 2.4802\n",
            "Epoch 00033: val_loss did not improve from 2.52453\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0861 - mae: 2.4825 - val_loss: 2.5606 - val_mae: 3.3310 - lr: 3.9063e-06\n",
            "Epoch 34/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0925 - mae: 2.5679\n",
            "Epoch 00034: val_loss did not improve from 2.52453\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0925 - mae: 2.5679 - val_loss: 2.5810 - val_mae: 3.3288 - lr: 1.9531e-06\n",
            "Epoch 35/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0985 - mae: 2.5575\n",
            "Epoch 00035: val_loss did not improve from 2.52453\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0987 - mae: 2.5558 - val_loss: 2.5482 - val_mae: 3.1099 - lr: 1.9531e-06\n",
            "Epoch 36/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0707 - mae: 2.4717\n",
            "Epoch 00036: val_loss did not improve from 2.52453\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0707 - mae: 2.4717 - val_loss: 2.6182 - val_mae: 3.1411 - lr: 9.7656e-07\n",
            "Epoch 37/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.1008 - mae: 2.5535\n",
            "Epoch 00037: val_loss did not improve from 2.52453\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.1024 - mae: 2.5593 - val_loss: 2.5404 - val_mae: 3.1162 - lr: 9.7656e-07\n",
            "Epoch 38/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0784 - mae: 2.4956\n",
            "Epoch 00038: val_loss did not improve from 2.52453\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0784 - mae: 2.4956 - val_loss: 2.6681 - val_mae: 3.5964 - lr: 4.8828e-07\n",
            "Epoch 39/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0807 - mae: 2.5128\n",
            "Epoch 00039: val_loss did not improve from 2.52453\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0806 - mae: 2.5104 - val_loss: 2.5563 - val_mae: 3.2776 - lr: 4.8828e-07\n",
            "Epoch 40/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0815 - mae: 2.4523\n",
            "Epoch 00040: val_loss improved from 2.52453 to 2.49435, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.0815 - mae: 2.4523 - val_loss: 2.4943 - val_mae: 2.8705 - lr: 2.4414e-07\n",
            "Epoch 41/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0995 - mae: 2.5371\n",
            "Epoch 00041: val_loss improved from 2.49435 to 2.46932, saving model to /home/jupyter/IEEE/fold15.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.0997 - mae: 2.5355 - val_loss: 2.4693 - val_mae: 3.2457 - lr: 2.4414e-07\n",
            "Epoch 42/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0706 - mae: 2.4728\n",
            "Epoch 00042: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0706 - mae: 2.4728 - val_loss: 2.5859 - val_mae: 3.2724 - lr: 2.4414e-07\n",
            "Epoch 43/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0795 - mae: 2.5027\n",
            "Epoch 00043: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0804 - mae: 2.5065 - val_loss: 2.6217 - val_mae: 3.2297 - lr: 2.4414e-07\n",
            "Epoch 44/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0916 - mae: 2.5108\n",
            "Epoch 00044: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0928 - mae: 2.5139 - val_loss: 2.5727 - val_mae: 3.2550 - lr: 1.2207e-07\n",
            "Epoch 45/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0730 - mae: 2.4937\n",
            "Epoch 00045: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0728 - mae: 2.4948 - val_loss: 2.5489 - val_mae: 3.2036 - lr: 1.2207e-07\n",
            "Epoch 46/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0818 - mae: 2.5003\n",
            "Epoch 00046: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0818 - mae: 2.5003 - val_loss: 2.6033 - val_mae: 3.2333 - lr: 6.1035e-08\n",
            "Epoch 47/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0917 - mae: 2.5246\n",
            "Epoch 00047: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0917 - mae: 2.5246 - val_loss: 2.6260 - val_mae: 3.1805 - lr: 6.1035e-08\n",
            "Epoch 48/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0783 - mae: 2.4196\n",
            "Epoch 00048: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0789 - mae: 2.4197 - val_loss: 2.5422 - val_mae: 3.3306 - lr: 3.0518e-08\n",
            "Epoch 49/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0729 - mae: 2.5260\n",
            "Epoch 00049: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0729 - mae: 2.5260 - val_loss: 2.5721 - val_mae: 3.2400 - lr: 3.0518e-08\n",
            "Epoch 50/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0930 - mae: 2.5030\n",
            "Epoch 00050: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0930 - mae: 2.5030 - val_loss: 2.6021 - val_mae: 3.3123 - lr: 1.5259e-08\n",
            "Epoch 51/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0951 - mae: 2.4942\n",
            "Epoch 00051: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0953 - mae: 2.4949 - val_loss: 2.6471 - val_mae: 3.3674 - lr: 1.5259e-08\n",
            "Epoch 52/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.1170 - mae: 2.4888\n",
            "Epoch 00052: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.1170 - mae: 2.4888 - val_loss: 2.5846 - val_mae: 3.4043 - lr: 7.6294e-09\n",
            "Epoch 53/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0688 - mae: 2.4595\n",
            "Epoch 00053: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00053: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0688 - mae: 2.4595 - val_loss: 2.6208 - val_mae: 3.2596 - lr: 7.6294e-09\n",
            "Epoch 54/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0847 - mae: 2.5291\n",
            "Epoch 00054: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0847 - mae: 2.5291 - val_loss: 2.5711 - val_mae: 3.1129 - lr: 3.8147e-09\n",
            "Epoch 55/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0727 - mae: 2.4852\n",
            "Epoch 00055: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0741 - mae: 2.4893 - val_loss: 2.7337 - val_mae: 3.3600 - lr: 3.8147e-09\n",
            "Epoch 56/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.1004 - mae: 2.4937\n",
            "Epoch 00056: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.1004 - mae: 2.4937 - val_loss: 2.5784 - val_mae: 3.2548 - lr: 1.9073e-09\n",
            "Epoch 57/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0658 - mae: 2.3923\n",
            "Epoch 00057: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0684 - mae: 2.3976 - val_loss: 2.6236 - val_mae: 3.4183 - lr: 1.9073e-09\n",
            "Epoch 58/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0733 - mae: 2.4889\n",
            "Epoch 00058: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0787 - mae: 2.4908 - val_loss: 2.5542 - val_mae: 3.1876 - lr: 9.5367e-10\n",
            "Epoch 59/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0759 - mae: 2.4865\n",
            "Epoch 00059: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00059: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0759 - mae: 2.4865 - val_loss: 2.5454 - val_mae: 3.1073 - lr: 9.5367e-10\n",
            "Epoch 60/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0664 - mae: 2.4550\n",
            "Epoch 00060: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0670 - mae: 2.4551 - val_loss: 2.6161 - val_mae: 3.2490 - lr: 4.7684e-10\n",
            "Epoch 61/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0863 - mae: 2.4803\n",
            "Epoch 00061: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0863 - mae: 2.4803 - val_loss: 2.5978 - val_mae: 3.5789 - lr: 4.7684e-10\n",
            "Epoch 62/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0852 - mae: 2.5853\n",
            "Epoch 00062: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0870 - mae: 2.5842 - val_loss: 2.5447 - val_mae: 3.1157 - lr: 2.3842e-10\n",
            "Epoch 63/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0815 - mae: 2.4823\n",
            "Epoch 00063: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.0814 - mae: 2.4812 - val_loss: 2.5511 - val_mae: 3.1989 - lr: 2.3842e-10\n",
            "Epoch 64/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0819 - mae: 2.5129\n",
            "Epoch 00064: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0815 - mae: 2.5092 - val_loss: 2.6048 - val_mae: 3.3657 - lr: 1.1921e-10\n",
            "Epoch 65/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0703 - mae: 2.4633\n",
            "Epoch 00065: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00065: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0703 - mae: 2.4612 - val_loss: 2.4718 - val_mae: 3.0815 - lr: 1.1921e-10\n",
            "Epoch 66/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.1106 - mae: 2.6309\n",
            "Epoch 00066: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.1106 - mae: 2.6309 - val_loss: 2.5766 - val_mae: 3.2459 - lr: 5.9605e-11\n",
            "Epoch 67/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0842 - mae: 2.4743\n",
            "Epoch 00067: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00067: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0842 - mae: 2.4743 - val_loss: 2.5518 - val_mae: 3.1667 - lr: 5.9605e-11\n",
            "Epoch 68/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0835 - mae: 2.5105\n",
            "Epoch 00068: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0835 - mae: 2.5105 - val_loss: 2.5354 - val_mae: 3.2931 - lr: 2.9802e-11\n",
            "Epoch 69/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0931 - mae: 2.4988\n",
            "Epoch 00069: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0931 - mae: 2.4995 - val_loss: 2.5589 - val_mae: 2.9377 - lr: 2.9802e-11\n",
            "Epoch 70/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0782 - mae: 2.4292\n",
            "Epoch 00070: val_loss did not improve from 2.46932\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.0777 - mae: 2.4287 - val_loss: 2.5854 - val_mae: 3.2142 - lr: 1.4901e-11\n",
            "Epoch 71/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0755 - mae: 2.4666Restoring model weights from the end of the best epoch: 41.\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 2.46932\n",
            "\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0775 - mae: 2.4651 - val_loss: 2.6299 - val_mae: 3.2998 - lr: 1.4901e-11\n",
            "Epoch 00071: early stopping\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 2.9222 - mae: 4.1246\n",
            "Fit model on training data fold  16\n",
            "Epoch 1/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 71.1849 - mae: 63.0010\n",
            "Epoch 00001: val_loss improved from inf to 5.11477, saving model to /home/jupyter/IEEE/fold16.h5\n",
            "84/84 [==============================] - 20s 127ms/step - loss: 71.1849 - mae: 63.0010 - val_loss: 5.1148 - val_mae: 13.6746 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.4151 - mae: 12.6812\n",
            "Epoch 00002: val_loss did not improve from 5.11477\n",
            "84/84 [==============================] - 8s 98ms/step - loss: 4.4151 - mae: 12.6812 - val_loss: 5.3033 - val_mae: 17.3580 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.9471 - mae: 11.0092\n",
            "Epoch 00003: val_loss improved from 5.11477 to 3.63911, saving model to /home/jupyter/IEEE/fold16.h5\n",
            "84/84 [==============================] - 8s 97ms/step - loss: 3.9471 - mae: 11.0092 - val_loss: 3.6391 - val_mae: 9.9314 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.6115 - mae: 9.9011\n",
            "Epoch 00004: val_loss improved from 3.63911 to 3.33030, saving model to /home/jupyter/IEEE/fold16.h5\n",
            "84/84 [==============================] - 8s 94ms/step - loss: 3.6115 - mae: 9.9011 - val_loss: 3.3303 - val_mae: 8.6300 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 3.4812 - mae: 8.9582\n",
            "Epoch 00005: val_loss improved from 3.33030 to 3.31950, saving model to /home/jupyter/IEEE/fold16.h5\n",
            "84/84 [==============================] - 8s 92ms/step - loss: 3.4813 - mae: 8.9568 - val_loss: 3.3195 - val_mae: 8.1251 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 3.4568 - mae: 8.5731\n",
            "Epoch 00006: val_loss did not improve from 3.31950\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 3.4563 - mae: 8.5785 - val_loss: 4.3244 - val_mae: 12.3334 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 3.4230 - mae: 8.3188\n",
            "Epoch 00007: val_loss did not improve from 3.31950\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 3.4251 - mae: 8.3209 - val_loss: 3.5190 - val_mae: 9.0458 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.0675 - mae: 6.6256\n",
            "Epoch 00008: val_loss improved from 3.31950 to 3.02106, saving model to /home/jupyter/IEEE/fold16.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 3.0675 - mae: 6.6256 - val_loss: 3.0211 - val_mae: 6.6042 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9999 - mae: 5.8105\n",
            "Epoch 00009: val_loss did not improve from 3.02106\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.9999 - mae: 5.8105 - val_loss: 3.4196 - val_mae: 7.2574 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.9380 - mae: 5.4248\n",
            "Epoch 00010: val_loss did not improve from 3.02106\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.9367 - mae: 5.4160 - val_loss: 3.2212 - val_mae: 6.0076 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.6788 - mae: 4.4350\n",
            "Epoch 00011: val_loss did not improve from 3.02106\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.6788 - mae: 4.4350 - val_loss: 3.4674 - val_mae: 5.8271 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.5990 - mae: 4.1624\n",
            "Epoch 00012: val_loss improved from 3.02106 to 2.92408, saving model to /home/jupyter/IEEE/fold16.h5\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.5990 - mae: 4.1624 - val_loss: 2.9241 - val_mae: 4.5135 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.5574 - mae: 3.8814\n",
            "Epoch 00013: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.5574 - mae: 3.8796 - val_loss: 3.2832 - val_mae: 4.7615 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.5220 - mae: 3.5894\n",
            "Epoch 00014: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.5231 - mae: 3.5914 - val_loss: 3.6061 - val_mae: 5.3905 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.3887 - mae: 3.3356\n",
            "Epoch 00015: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.3891 - mae: 3.3342 - val_loss: 4.0094 - val_mae: 5.3795 - lr: 1.2500e-04\n",
            "Epoch 16/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.3048 - mae: 2.9422\n",
            "Epoch 00016: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.3048 - mae: 2.9422 - val_loss: 3.5603 - val_mae: 4.5233 - lr: 1.2500e-04\n",
            "Epoch 17/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.2100 - mae: 2.7980\n",
            "Epoch 00017: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.2112 - mae: 2.7966 - val_loss: 3.6509 - val_mae: 4.3060 - lr: 6.2500e-05\n",
            "Epoch 18/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.1693 - mae: 2.6854\n",
            "Epoch 00018: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.1693 - mae: 2.6854 - val_loss: 3.3547 - val_mae: 3.7040 - lr: 6.2500e-05\n",
            "Epoch 19/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.1135 - mae: 2.5727\n",
            "Epoch 00019: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.1135 - mae: 2.5727 - val_loss: 3.4353 - val_mae: 3.7485 - lr: 3.1250e-05\n",
            "Epoch 20/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.1032 - mae: 2.5491\n",
            "Epoch 00020: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.1045 - mae: 2.5505 - val_loss: 3.4052 - val_mae: 3.6308 - lr: 3.1250e-05\n",
            "Epoch 21/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0765 - mae: 2.3755\n",
            "Epoch 00021: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0765 - mae: 2.3755 - val_loss: 3.4009 - val_mae: 3.5229 - lr: 1.5625e-05\n",
            "Epoch 22/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0828 - mae: 2.4477\n",
            "Epoch 00022: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.0841 - mae: 2.4504 - val_loss: 3.5234 - val_mae: 3.7217 - lr: 1.5625e-05\n",
            "Epoch 23/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0302 - mae: 2.4222\n",
            "Epoch 00023: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0299 - mae: 2.4204 - val_loss: 3.5419 - val_mae: 3.7945 - lr: 7.8125e-06\n",
            "Epoch 24/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0348 - mae: 2.3475\n",
            "Epoch 00024: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0348 - mae: 2.3475 - val_loss: 3.7712 - val_mae: 3.9143 - lr: 7.8125e-06\n",
            "Epoch 25/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0302 - mae: 2.3177\n",
            "Epoch 00025: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0302 - mae: 2.3177 - val_loss: 3.4456 - val_mae: 3.5984 - lr: 3.9063e-06\n",
            "Epoch 26/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 1.9963 - mae: 2.3534\n",
            "Epoch 00026: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.9963 - mae: 2.3534 - val_loss: 3.7750 - val_mae: 3.8671 - lr: 3.9063e-06\n",
            "Epoch 27/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0357 - mae: 2.3261\n",
            "Epoch 00027: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0406 - mae: 2.3326 - val_loss: 3.5860 - val_mae: 3.4874 - lr: 1.9531e-06\n",
            "Epoch 28/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0073 - mae: 2.3099\n",
            "Epoch 00028: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0069 - mae: 2.3091 - val_loss: 3.5592 - val_mae: 3.3977 - lr: 1.9531e-06\n",
            "Epoch 29/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0157 - mae: 2.3629\n",
            "Epoch 00029: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0180 - mae: 2.3677 - val_loss: 3.6865 - val_mae: 3.7885 - lr: 9.7656e-07\n",
            "Epoch 30/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0190 - mae: 2.3539\n",
            "Epoch 00030: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0190 - mae: 2.3539 - val_loss: 3.6346 - val_mae: 3.9721 - lr: 9.7656e-07\n",
            "Epoch 31/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0311 - mae: 2.3597\n",
            "Epoch 00031: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0317 - mae: 2.3585 - val_loss: 3.7222 - val_mae: 3.6914 - lr: 4.8828e-07\n",
            "Epoch 32/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 1.9976 - mae: 2.3203\n",
            "Epoch 00032: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 1.9976 - mae: 2.3203 - val_loss: 3.5838 - val_mae: 3.5794 - lr: 4.8828e-07\n",
            "Epoch 33/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0267 - mae: 2.3304\n",
            "Epoch 00033: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0267 - mae: 2.3313 - val_loss: 3.7009 - val_mae: 3.7786 - lr: 2.4414e-07\n",
            "Epoch 34/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0316 - mae: 2.3953\n",
            "Epoch 00034: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.0316 - mae: 2.3953 - val_loss: 3.7859 - val_mae: 3.8666 - lr: 2.4414e-07\n",
            "Epoch 35/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.0241 - mae: 2.3332\n",
            "Epoch 00035: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.0241 - mae: 2.3332 - val_loss: 3.5462 - val_mae: 3.6828 - lr: 1.2207e-07\n",
            "Epoch 36/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0254 - mae: 2.3575\n",
            "Epoch 00036: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0251 - mae: 2.3586 - val_loss: 3.6087 - val_mae: 3.4063 - lr: 1.2207e-07\n",
            "Epoch 37/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0212 - mae: 2.3593\n",
            "Epoch 00037: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.0212 - mae: 2.3590 - val_loss: 3.8007 - val_mae: 3.9217 - lr: 6.1035e-08\n",
            "Epoch 38/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0045 - mae: 2.2813\n",
            "Epoch 00038: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.0047 - mae: 2.2781 - val_loss: 3.4999 - val_mae: 3.4261 - lr: 6.1035e-08\n",
            "Epoch 39/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0283 - mae: 2.3249\n",
            "Epoch 00039: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0316 - mae: 2.3285 - val_loss: 3.6314 - val_mae: 3.7700 - lr: 3.0518e-08\n",
            "Epoch 40/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0358 - mae: 2.2900\n",
            "Epoch 00040: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.0398 - mae: 2.2923 - val_loss: 3.6465 - val_mae: 3.6989 - lr: 3.0518e-08\n",
            "Epoch 41/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0191 - mae: 2.3439\n",
            "Epoch 00041: val_loss did not improve from 2.92408\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.0220 - mae: 2.3516 - val_loss: 3.4474 - val_mae: 3.3231 - lr: 1.5259e-08\n",
            "Epoch 42/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0219 - mae: 2.3160Restoring model weights from the end of the best epoch: 12.\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 2.92408\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.0243 - mae: 2.3192 - val_loss: 3.5378 - val_mae: 3.5904 - lr: 1.5259e-08\n",
            "Epoch 00042: early stopping\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 3.0156 - mae: 5.0596\n",
            "Fit model on training data fold  17\n",
            "Epoch 1/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 75.8839 - mae: 95.8041\n",
            "Epoch 00001: val_loss improved from inf to 4.70158, saving model to /home/jupyter/IEEE/fold17.h5\n",
            "84/84 [==============================] - 21s 133ms/step - loss: 75.8839 - mae: 95.8041 - val_loss: 4.7016 - val_mae: 18.7006 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.4865 - mae: 17.0281\n",
            "Epoch 00002: val_loss improved from 4.70158 to 3.94458, saving model to /home/jupyter/IEEE/fold17.h5\n",
            "84/84 [==============================] - 8s 100ms/step - loss: 4.4865 - mae: 17.0281 - val_loss: 3.9446 - val_mae: 14.1248 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 4.0018 - mae: 14.1024\n",
            "Epoch 00003: val_loss improved from 3.94458 to 3.73589, saving model to /home/jupyter/IEEE/fold17.h5\n",
            "84/84 [==============================] - 8s 98ms/step - loss: 4.0018 - mae: 14.1024 - val_loss: 3.7359 - val_mae: 12.0219 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.7795 - mae: 12.2288\n",
            "Epoch 00004: val_loss improved from 3.73589 to 3.53749, saving model to /home/jupyter/IEEE/fold17.h5\n",
            "84/84 [==============================] - 8s 98ms/step - loss: 3.7795 - mae: 12.2288 - val_loss: 3.5375 - val_mae: 9.9010 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 3.6615 - mae: 10.9755\n",
            "Epoch 00005: val_loss improved from 3.53749 to 3.47421, saving model to /home/jupyter/IEEE/fold17.h5\n",
            "84/84 [==============================] - 8s 97ms/step - loss: 3.6606 - mae: 10.9729 - val_loss: 3.4742 - val_mae: 9.7096 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.5555 - mae: 9.6766\n",
            "Epoch 00006: val_loss did not improve from 3.47421\n",
            "84/84 [==============================] - 8s 95ms/step - loss: 3.5555 - mae: 9.6766 - val_loss: 4.3386 - val_mae: 12.1195 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.4180 - mae: 8.4514\n",
            "Epoch 00007: val_loss improved from 3.47421 to 3.36952, saving model to /home/jupyter/IEEE/fold17.h5\n",
            "84/84 [==============================] - 8s 97ms/step - loss: 3.4180 - mae: 8.4514 - val_loss: 3.3695 - val_mae: 8.0662 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.3901 - mae: 7.7978\n",
            "Epoch 00008: val_loss did not improve from 3.36952\n",
            "84/84 [==============================] - 8s 93ms/step - loss: 3.3901 - mae: 7.7978 - val_loss: 3.8307 - val_mae: 9.9468 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 3.3671 - mae: 7.9658\n",
            "Epoch 00009: val_loss did not improve from 3.36952\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "84/84 [==============================] - 8s 93ms/step - loss: 3.3671 - mae: 7.9658 - val_loss: 3.4990 - val_mae: 7.6789 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.9952 - mae: 5.8007\n",
            "Epoch 00010: val_loss improved from 3.36952 to 3.13701, saving model to /home/jupyter/IEEE/fold17.h5\n",
            "84/84 [==============================] - 8s 95ms/step - loss: 2.9952 - mae: 5.8007 - val_loss: 3.1370 - val_mae: 5.5610 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.8375 - mae: 4.5997\n",
            "Epoch 00011: val_loss improved from 3.13701 to 2.98338, saving model to /home/jupyter/IEEE/fold17.h5\n",
            "84/84 [==============================] - 8s 93ms/step - loss: 2.8375 - mae: 4.5997 - val_loss: 2.9834 - val_mae: 4.8812 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.7234 - mae: 4.2745\n",
            "Epoch 00012: val_loss improved from 2.98338 to 2.78110, saving model to /home/jupyter/IEEE/fold17.h5\n",
            "84/84 [==============================] - 8s 93ms/step - loss: 2.7234 - mae: 4.2745 - val_loss: 2.7811 - val_mae: 4.1137 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 2.6712 - mae: 4.0013\n",
            "Epoch 00013: val_loss did not improve from 2.78110\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.6712 - mae: 4.0013 - val_loss: 3.0215 - val_mae: 4.3633 - lr: 5.0000e-04\n",
            "Epoch 14/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.5520 - mae: 3.5807\n",
            "Epoch 00014: val_loss did not improve from 2.78110\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.5522 - mae: 3.5809 - val_loss: 3.2621 - val_mae: 4.5789 - lr: 5.0000e-04\n",
            "Epoch 15/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.4023 - mae: 3.0905\n",
            "Epoch 00015: val_loss did not improve from 2.78110\n",
            "84/84 [==============================] - 8s 90ms/step - loss: 2.4073 - mae: 3.1007 - val_loss: 3.3478 - val_mae: 4.1806 - lr: 2.5000e-04\n",
            "Epoch 16/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.4020 - mae: 3.1115\n",
            "Epoch 00016: val_loss improved from 2.78110 to 2.75531, saving model to /home/jupyter/IEEE/fold17.h5\n",
            "84/84 [==============================] - 8s 91ms/step - loss: 2.4014 - mae: 3.1113 - val_loss: 2.7553 - val_mae: 3.3347 - lr: 2.5000e-04\n",
            "Epoch 17/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.2627 - mae: 2.6371\n",
            "Epoch 00017: val_loss did not improve from 2.75531\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.2641 - mae: 2.6385 - val_loss: 3.5833 - val_mae: 4.4614 - lr: 2.5000e-04\n",
            "Epoch 18/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.2858 - mae: 2.7385\n",
            "Epoch 00018: val_loss improved from 2.75531 to 2.73807, saving model to /home/jupyter/IEEE/fold17.h5\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 2.2870 - mae: 2.7398 - val_loss: 2.7381 - val_mae: 3.1476 - lr: 2.5000e-04\n",
            "Epoch 19/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.1537 - mae: 2.3774\n",
            "Epoch 00019: val_loss did not improve from 2.73807\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.1533 - mae: 2.3760 - val_loss: 3.0051 - val_mae: 3.2345 - lr: 2.5000e-04\n",
            "Epoch 20/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.1217 - mae: 2.3139\n",
            "Epoch 00020: val_loss did not improve from 2.73807\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 2.1231 - mae: 2.3176 - val_loss: 3.7002 - val_mae: 3.8729 - lr: 2.5000e-04\n",
            "Epoch 21/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.9605 - mae: 2.0917\n",
            "Epoch 00021: val_loss did not improve from 2.73807\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.9681 - mae: 2.0997 - val_loss: 3.4533 - val_mae: 3.3538 - lr: 1.2500e-04\n",
            "Epoch 22/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 2.0028 - mae: 2.0532\n",
            "Epoch 00022: val_loss did not improve from 2.73807\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 2.0097 - mae: 2.0603 - val_loss: 2.9925 - val_mae: 2.9616 - lr: 1.2500e-04\n",
            "Epoch 23/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.8617 - mae: 1.9047\n",
            "Epoch 00023: val_loss improved from 2.73807 to 2.63251, saving model to /home/jupyter/IEEE/fold17.h5\n",
            "84/84 [==============================] - 7s 89ms/step - loss: 1.8613 - mae: 1.9036 - val_loss: 2.6325 - val_mae: 2.5678 - lr: 6.2500e-05\n",
            "Epoch 24/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.7952 - mae: 1.7282\n",
            "Epoch 00024: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.7969 - mae: 1.7282 - val_loss: 2.9952 - val_mae: 2.6608 - lr: 6.2500e-05\n",
            "Epoch 25/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.8034 - mae: 1.7314\n",
            "Epoch 00025: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "84/84 [==============================] - 7s 86ms/step - loss: 1.8043 - mae: 1.7327 - val_loss: 2.8391 - val_mae: 2.5481 - lr: 6.2500e-05\n",
            "Epoch 26/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.7513 - mae: 1.6579\n",
            "Epoch 00026: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 1.7525 - mae: 1.6590 - val_loss: 3.0822 - val_mae: 2.6353 - lr: 3.1250e-05\n",
            "Epoch 27/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.7033 - mae: 1.6031\n",
            "Epoch 00027: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "84/84 [==============================] - 7s 86ms/step - loss: 1.7067 - mae: 1.6054 - val_loss: 3.0974 - val_mae: 2.6546 - lr: 3.1250e-05\n",
            "Epoch 28/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.7249 - mae: 1.5858\n",
            "Epoch 00028: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.7269 - mae: 1.5888 - val_loss: 2.9856 - val_mae: 2.3628 - lr: 1.5625e-05\n",
            "Epoch 29/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6751 - mae: 1.5037\n",
            "Epoch 00029: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 1.6771 - mae: 1.5046 - val_loss: 3.0907 - val_mae: 2.7002 - lr: 1.5625e-05\n",
            "Epoch 30/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6697 - mae: 1.5512\n",
            "Epoch 00030: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 86ms/step - loss: 1.6715 - mae: 1.5505 - val_loss: 3.1950 - val_mae: 2.6481 - lr: 7.8125e-06\n",
            "Epoch 31/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 1.6891 - mae: 1.5386\n",
            "Epoch 00031: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.6891 - mae: 1.5386 - val_loss: 3.0818 - val_mae: 2.5197 - lr: 7.8125e-06\n",
            "Epoch 32/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 1.6686 - mae: 1.5268\n",
            "Epoch 00032: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 1.6686 - mae: 1.5268 - val_loss: 2.8924 - val_mae: 2.4428 - lr: 3.9063e-06\n",
            "Epoch 33/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 1.6885 - mae: 1.5275\n",
            "Epoch 00033: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 1.6885 - mae: 1.5275 - val_loss: 3.3913 - val_mae: 2.7506 - lr: 3.9063e-06\n",
            "Epoch 34/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6501 - mae: 1.4825\n",
            "Epoch 00034: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.6513 - mae: 1.4835 - val_loss: 3.4585 - val_mae: 2.8487 - lr: 1.9531e-06\n",
            "Epoch 35/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6472 - mae: 1.5055\n",
            "Epoch 00035: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.6492 - mae: 1.5053 - val_loss: 2.8707 - val_mae: 2.3289 - lr: 1.9531e-06\n",
            "Epoch 36/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6695 - mae: 1.4964\n",
            "Epoch 00036: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 86ms/step - loss: 1.6692 - mae: 1.4961 - val_loss: 2.9263 - val_mae: 2.4632 - lr: 9.7656e-07\n",
            "Epoch 37/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6474 - mae: 1.5070\n",
            "Epoch 00037: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.6589 - mae: 1.5149 - val_loss: 3.1608 - val_mae: 2.5565 - lr: 9.7656e-07\n",
            "Epoch 38/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6568 - mae: 1.5070\n",
            "Epoch 00038: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 86ms/step - loss: 1.6577 - mae: 1.5078 - val_loss: 2.7783 - val_mae: 2.3663 - lr: 4.8828e-07\n",
            "Epoch 39/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 1.6678 - mae: 1.5552\n",
            "Epoch 00039: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 1.6678 - mae: 1.5552 - val_loss: 2.9073 - val_mae: 2.4037 - lr: 4.8828e-07\n",
            "Epoch 40/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6711 - mae: 1.5691\n",
            "Epoch 00040: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 1.6778 - mae: 1.5752 - val_loss: 2.8293 - val_mae: 2.3787 - lr: 2.4414e-07\n",
            "Epoch 41/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6509 - mae: 1.4746\n",
            "Epoch 00041: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.6507 - mae: 1.4738 - val_loss: 3.0345 - val_mae: 2.4968 - lr: 2.4414e-07\n",
            "Epoch 42/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.7008 - mae: 1.5522\n",
            "Epoch 00042: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 1.7007 - mae: 1.5530 - val_loss: 2.9105 - val_mae: 2.4535 - lr: 1.2207e-07\n",
            "Epoch 43/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 1.6632 - mae: 1.5452\n",
            "Epoch 00043: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 1.6632 - mae: 1.5452 - val_loss: 3.0789 - val_mae: 2.6793 - lr: 1.2207e-07\n",
            "Epoch 44/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6404 - mae: 1.5239\n",
            "Epoch 00044: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.6398 - mae: 1.5236 - val_loss: 3.0139 - val_mae: 2.4431 - lr: 6.1035e-08\n",
            "Epoch 45/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6649 - mae: 1.5263\n",
            "Epoch 00045: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 1.6642 - mae: 1.5263 - val_loss: 2.9158 - val_mae: 2.4594 - lr: 6.1035e-08\n",
            "Epoch 46/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6663 - mae: 1.4884\n",
            "Epoch 00046: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.6656 - mae: 1.4868 - val_loss: 3.2322 - val_mae: 2.4918 - lr: 3.0518e-08\n",
            "Epoch 47/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6655 - mae: 1.4794\n",
            "Epoch 00047: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.6674 - mae: 1.4823 - val_loss: 3.4675 - val_mae: 2.8618 - lr: 3.0518e-08\n",
            "Epoch 48/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 1.6704 - mae: 1.5202\n",
            "Epoch 00048: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.6704 - mae: 1.5202 - val_loss: 3.1495 - val_mae: 2.5876 - lr: 1.5259e-08\n",
            "Epoch 49/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6569 - mae: 1.5206\n",
            "Epoch 00049: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 1.6571 - mae: 1.5223 - val_loss: 3.0983 - val_mae: 2.5109 - lr: 1.5259e-08\n",
            "Epoch 50/200\n",
            "84/84 [==============================] - ETA: 0s - loss: 1.6496 - mae: 1.4921\n",
            "Epoch 00050: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.6496 - mae: 1.4921 - val_loss: 3.0870 - val_mae: 2.5906 - lr: 7.6294e-09\n",
            "Epoch 51/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6725 - mae: 1.5182\n",
            "Epoch 00051: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "84/84 [==============================] - 7s 87ms/step - loss: 1.6731 - mae: 1.5171 - val_loss: 3.4410 - val_mae: 2.6632 - lr: 7.6294e-09\n",
            "Epoch 52/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6799 - mae: 1.4872\n",
            "Epoch 00052: val_loss did not improve from 2.63251\n",
            "84/84 [==============================] - 7s 86ms/step - loss: 1.6802 - mae: 1.4885 - val_loss: 3.0472 - val_mae: 2.3626 - lr: 3.8147e-09\n",
            "Epoch 53/200\n",
            "83/84 [============================>.] - ETA: 0s - loss: 1.6693 - mae: 1.5390Restoring model weights from the end of the best epoch: 23.\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 2.63251\n",
            "\n",
            "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
            "84/84 [==============================] - 7s 88ms/step - loss: 1.6692 - mae: 1.5384 - val_loss: 3.3186 - val_mae: 2.6901 - lr: 3.8147e-09\n",
            "Epoch 00053: early stopping\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 1.8269 - mae: 1.6585\n",
            "Fit model on training data fold  18\n",
            "Epoch 1/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 35.7999 - mae: 70.7807\n",
            "Epoch 00001: val_loss improved from inf to 5.03832, saving model to /home/jupyter/IEEE/fold18.h5\n",
            "80/80 [==============================] - 20s 133ms/step - loss: 35.7999 - mae: 70.7807 - val_loss: 5.0383 - val_mae: 26.8630 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 4.4707 - mae: 20.4426\n",
            "Epoch 00002: val_loss did not improve from 5.03832\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 4.4719 - mae: 20.4388 - val_loss: 5.2551 - val_mae: 25.5997 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 4.1375 - mae: 16.7739\n",
            "Epoch 00003: val_loss did not improve from 5.03832\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 4.1405 - mae: 16.8042 - val_loss: 5.2987 - val_mae: 27.5249 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 4.0554 - mae: 15.9419\n",
            "Epoch 00004: val_loss improved from 5.03832 to 5.01774, saving model to /home/jupyter/IEEE/fold18.h5\n",
            "80/80 [==============================] - 7s 94ms/step - loss: 4.0580 - mae: 15.9729 - val_loss: 5.0177 - val_mae: 23.5567 - lr: 5.0000e-04\n",
            "Epoch 5/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.8524 - mae: 14.1015\n",
            "Epoch 00005: val_loss improved from 5.01774 to 5.01169, saving model to /home/jupyter/IEEE/fold18.h5\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 3.8523 - mae: 14.1091 - val_loss: 5.0117 - val_mae: 23.2736 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.7456 - mae: 13.2800\n",
            "Epoch 00006: val_loss improved from 5.01169 to 4.91589, saving model to /home/jupyter/IEEE/fold18.h5\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 3.7454 - mae: 13.2632 - val_loss: 4.9159 - val_mae: 21.6904 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.6610 - mae: 12.3751\n",
            "Epoch 00007: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.6605 - mae: 12.3646 - val_loss: 5.1782 - val_mae: 22.1849 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5654 - mae: 11.0498\n",
            "Epoch 00008: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.5659 - mae: 11.0414 - val_loss: 5.1246 - val_mae: 20.7617 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.4561 - mae: 10.0483\n",
            "Epoch 00009: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 91ms/step - loss: 3.4558 - mae: 10.0541 - val_loss: 5.1334 - val_mae: 19.4028 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.3193 - mae: 9.0453\n",
            "Epoch 00010: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.3198 - mae: 9.0397 - val_loss: 5.1736 - val_mae: 19.6309 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.2724 - mae: 8.8501\n",
            "Epoch 00011: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.2719 - mae: 8.8448 - val_loss: 5.1290 - val_mae: 18.7342 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.2228 - mae: 8.4477\n",
            "Epoch 00012: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.2228 - mae: 8.4477 - val_loss: 5.0800 - val_mae: 18.3337 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.1848 - mae: 8.2454\n",
            "Epoch 00013: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.1848 - mae: 8.2497 - val_loss: 5.0801 - val_mae: 17.0138 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.1584 - mae: 8.1306\n",
            "Epoch 00014: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.1584 - mae: 8.1280 - val_loss: 5.1503 - val_mae: 17.4613 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.1474 - mae: 7.8213\n",
            "Epoch 00015: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.1477 - mae: 7.8226 - val_loss: 5.1644 - val_mae: 18.2584 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.0981 - mae: 7.6138\n",
            "Epoch 00016: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.0981 - mae: 7.6138 - val_loss: 5.2008 - val_mae: 17.3524 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0945 - mae: 7.5154\n",
            "Epoch 00017: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.0946 - mae: 7.5076 - val_loss: 5.2724 - val_mae: 17.6454 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0980 - mae: 7.6589\n",
            "Epoch 00018: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.0991 - mae: 7.6596 - val_loss: 5.0993 - val_mae: 16.1972 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0869 - mae: 7.4633\n",
            "Epoch 00019: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.0871 - mae: 7.4718 - val_loss: 5.1872 - val_mae: 17.2468 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.0804 - mae: 7.3976\n",
            "Epoch 00020: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 3.0804 - mae: 7.3976 - val_loss: 5.1960 - val_mae: 17.1366 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0721 - mae: 7.3057\n",
            "Epoch 00021: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.0731 - mae: 7.3160 - val_loss: 5.2254 - val_mae: 17.3886 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0719 - mae: 7.3268\n",
            "Epoch 00022: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.0718 - mae: 7.3257 - val_loss: 5.2479 - val_mae: 17.1193 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.0748 - mae: 7.4375\n",
            "Epoch 00023: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.0748 - mae: 7.4375 - val_loss: 5.1906 - val_mae: 17.3312 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0693 - mae: 7.2936\n",
            "Epoch 00024: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.0695 - mae: 7.2993 - val_loss: 5.1799 - val_mae: 17.2992 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.0719 - mae: 7.3185\n",
            "Epoch 00025: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.0719 - mae: 7.3185 - val_loss: 5.2442 - val_mae: 17.4599 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0731 - mae: 7.2419\n",
            "Epoch 00026: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.0750 - mae: 7.2468 - val_loss: 5.2279 - val_mae: 16.9739 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0710 - mae: 7.0834\n",
            "Epoch 00027: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.0735 - mae: 7.0950 - val_loss: 5.1182 - val_mae: 16.1508 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0757 - mae: 7.2126\n",
            "Epoch 00028: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.0755 - mae: 7.2042 - val_loss: 5.1654 - val_mae: 16.4256 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0719 - mae: 7.4722\n",
            "Epoch 00029: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 91ms/step - loss: 3.0718 - mae: 7.4675 - val_loss: 5.1425 - val_mae: 16.9277 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.0662 - mae: 7.0674\n",
            "Epoch 00030: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 3.0662 - mae: 7.0674 - val_loss: 5.2588 - val_mae: 17.0557 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.0656 - mae: 7.0039\n",
            "Epoch 00031: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.0656 - mae: 7.0039 - val_loss: 5.2478 - val_mae: 16.9130 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0700 - mae: 7.3563\n",
            "Epoch 00032: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "80/80 [==============================] - 7s 91ms/step - loss: 3.0699 - mae: 7.3626 - val_loss: 5.2166 - val_mae: 17.0066 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.0667 - mae: 7.4310\n",
            "Epoch 00033: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 3.0667 - mae: 7.4310 - val_loss: 5.1591 - val_mae: 16.6421 - lr: 6.1035e-08\n",
            "Epoch 34/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0576 - mae: 7.3257\n",
            "Epoch 00034: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "80/80 [==============================] - 7s 91ms/step - loss: 3.0597 - mae: 7.3204 - val_loss: 5.1912 - val_mae: 17.6477 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0641 - mae: 7.3852\n",
            "Epoch 00035: val_loss did not improve from 4.91589\n",
            "80/80 [==============================] - 7s 91ms/step - loss: 3.0652 - mae: 7.3866 - val_loss: 5.1164 - val_mae: 16.5089 - lr: 3.0518e-08\n",
            "Epoch 36/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0660 - mae: 7.3576Restoring model weights from the end of the best epoch: 6.\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 4.91589\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.0660 - mae: 7.3586 - val_loss: 5.1706 - val_mae: 17.3479 - lr: 3.0518e-08\n",
            "Epoch 00036: early stopping\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 3.6217 - mae: 11.5826\n",
            "Fit model on training data fold  19\n",
            "Epoch 1/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 37.7650 - mae: 73.4483\n",
            "Epoch 00001: val_loss improved from inf to 5.14386, saving model to /home/jupyter/IEEE/fold19.h5\n",
            "80/80 [==============================] - 19s 128ms/step - loss: 37.7650 - mae: 73.4483 - val_loss: 5.1439 - val_mae: 27.9111 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.3775 - mae: 28.1071\n",
            "Epoch 00002: val_loss improved from 5.14386 to 5.03547, saving model to /home/jupyter/IEEE/fold19.h5\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 5.3775 - mae: 28.1071 - val_loss: 5.0355 - val_mae: 27.9396 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 4.9907 - mae: 27.1415\n",
            "Epoch 00003: val_loss improved from 5.03547 to 4.88872, saving model to /home/jupyter/IEEE/fold19.h5\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 4.9907 - mae: 27.1415 - val_loss: 4.8887 - val_mae: 26.3488 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 4.7008 - mae: 24.6556\n",
            "Epoch 00004: val_loss improved from 4.88872 to 4.77464, saving model to /home/jupyter/IEEE/fold19.h5\n",
            "80/80 [==============================] - 7s 91ms/step - loss: 4.7010 - mae: 24.7033 - val_loss: 4.7746 - val_mae: 23.8260 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 4.3925 - mae: 20.9110\n",
            "Epoch 00005: val_loss did not improve from 4.77464\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 4.3926 - mae: 20.9172 - val_loss: 4.8112 - val_mae: 26.5788 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 4.2332 - mae: 19.0459\n",
            "Epoch 00006: val_loss did not improve from 4.77464\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 4.2328 - mae: 19.0564 - val_loss: 4.9535 - val_mae: 27.0856 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 4.0449 - mae: 17.2435\n",
            "Epoch 00007: val_loss did not improve from 4.77464\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 4.0449 - mae: 17.2435 - val_loss: 4.8240 - val_mae: 24.8378 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.9393 - mae: 16.0540\n",
            "Epoch 00008: val_loss did not improve from 4.77464\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.9393 - mae: 16.0540 - val_loss: 4.8514 - val_mae: 24.8929 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.9043 - mae: 14.6889\n",
            "Epoch 00009: val_loss improved from 4.77464 to 4.70412, saving model to /home/jupyter/IEEE/fold19.h5\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 3.9060 - mae: 14.7037 - val_loss: 4.7041 - val_mae: 21.5988 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.8335 - mae: 13.9772\n",
            "Epoch 00010: val_loss improved from 4.70412 to 4.61471, saving model to /home/jupyter/IEEE/fold19.h5\n",
            "80/80 [==============================] - 7s 90ms/step - loss: 3.8337 - mae: 13.9860 - val_loss: 4.6147 - val_mae: 20.8746 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.8006 - mae: 13.9749\n",
            "Epoch 00011: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.8007 - mae: 13.9691 - val_loss: 4.7291 - val_mae: 20.6636 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.7437 - mae: 13.2513\n",
            "Epoch 00012: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.7437 - mae: 13.2619 - val_loss: 4.6795 - val_mae: 19.9361 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.6776 - mae: 12.6453\n",
            "Epoch 00013: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.6776 - mae: 12.6453 - val_loss: 4.6575 - val_mae: 19.5826 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.6786 - mae: 12.5004\n",
            "Epoch 00014: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.6823 - mae: 12.5126 - val_loss: 4.7815 - val_mae: 20.0929 - lr: 1.2500e-04\n",
            "Epoch 15/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.6360 - mae: 11.9118\n",
            "Epoch 00015: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.6372 - mae: 11.9376 - val_loss: 4.8001 - val_mae: 21.4057 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.6411 - mae: 12.2763\n",
            "Epoch 00016: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.6410 - mae: 12.2762 - val_loss: 4.7418 - val_mae: 20.9272 - lr: 6.2500e-05\n",
            "Epoch 17/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.5932 - mae: 11.7242\n",
            "Epoch 00017: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.5932 - mae: 11.7242 - val_loss: 4.7519 - val_mae: 19.9528 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5826 - mae: 11.6368\n",
            "Epoch 00018: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.5830 - mae: 11.6323 - val_loss: 4.7124 - val_mae: 19.4869 - lr: 3.1250e-05\n",
            "Epoch 19/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5860 - mae: 11.3772\n",
            "Epoch 00019: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.5863 - mae: 11.3748 - val_loss: 4.6880 - val_mae: 19.4862 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5593 - mae: 11.3474\n",
            "Epoch 00020: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.5598 - mae: 11.3462 - val_loss: 4.7222 - val_mae: 19.4856 - lr: 1.5625e-05\n",
            "Epoch 21/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5575 - mae: 11.1483\n",
            "Epoch 00021: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.5588 - mae: 11.1517 - val_loss: 4.7440 - val_mae: 18.3029 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5526 - mae: 11.2084\n",
            "Epoch 00022: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.5524 - mae: 11.2084 - val_loss: 4.7815 - val_mae: 19.4498 - lr: 7.8125e-06\n",
            "Epoch 23/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.5479 - mae: 11.0885\n",
            "Epoch 00023: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.5479 - mae: 11.0885 - val_loss: 4.7613 - val_mae: 18.9691 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.5338 - mae: 11.3382\n",
            "Epoch 00024: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.5338 - mae: 11.3382 - val_loss: 4.7457 - val_mae: 18.3979 - lr: 3.9063e-06\n",
            "Epoch 25/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5536 - mae: 11.0347\n",
            "Epoch 00025: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.5530 - mae: 11.0259 - val_loss: 4.7110 - val_mae: 19.8028 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5360 - mae: 11.0415\n",
            "Epoch 00026: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.5367 - mae: 11.0610 - val_loss: 4.7376 - val_mae: 19.3396 - lr: 1.9531e-06\n",
            "Epoch 27/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5354 - mae: 11.6022\n",
            "Epoch 00027: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.5369 - mae: 11.6084 - val_loss: 4.7019 - val_mae: 19.3076 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.5400 - mae: 10.6430\n",
            "Epoch 00028: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.5400 - mae: 10.6430 - val_loss: 4.7220 - val_mae: 18.6669 - lr: 9.7656e-07\n",
            "Epoch 29/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5401 - mae: 11.4899\n",
            "Epoch 00029: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.5425 - mae: 11.5045 - val_loss: 4.8214 - val_mae: 19.8727 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5264 - mae: 11.2563\n",
            "Epoch 00030: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.5275 - mae: 11.2709 - val_loss: 4.7603 - val_mae: 19.1420 - lr: 4.8828e-07\n",
            "Epoch 31/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5565 - mae: 11.0904\n",
            "Epoch 00031: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.5585 - mae: 11.0892 - val_loss: 4.7876 - val_mae: 19.3158 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.5390 - mae: 10.8801\n",
            "Epoch 00032: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.5390 - mae: 10.8801 - val_loss: 4.7835 - val_mae: 18.7636 - lr: 2.4414e-07\n",
            "Epoch 33/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5419 - mae: 11.0270\n",
            "Epoch 00033: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.5437 - mae: 11.0421 - val_loss: 4.7929 - val_mae: 19.5350 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5599 - mae: 11.3026\n",
            "Epoch 00034: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.5614 - mae: 11.3100 - val_loss: 4.7767 - val_mae: 18.5615 - lr: 1.2207e-07\n",
            "Epoch 35/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5390 - mae: 10.9860\n",
            "Epoch 00035: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.5396 - mae: 10.9945 - val_loss: 4.7760 - val_mae: 18.8175 - lr: 6.1035e-08\n",
            "Epoch 36/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.5274 - mae: 11.0510\n",
            "Epoch 00036: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.5274 - mae: 11.0510 - val_loss: 4.8286 - val_mae: 19.7078 - lr: 6.1035e-08\n",
            "Epoch 37/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.5300 - mae: 10.9410\n",
            "Epoch 00037: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.5300 - mae: 10.9410 - val_loss: 4.7309 - val_mae: 18.5012 - lr: 3.0518e-08\n",
            "Epoch 38/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5338 - mae: 11.2321\n",
            "Epoch 00038: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.5336 - mae: 11.2151 - val_loss: 4.7942 - val_mae: 19.3502 - lr: 3.0518e-08\n",
            "Epoch 39/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5287 - mae: 11.3052\n",
            "Epoch 00039: val_loss did not improve from 4.61471\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 3.5287 - mae: 11.2958 - val_loss: 4.7759 - val_mae: 19.0911 - lr: 1.5259e-08\n",
            "Epoch 40/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.5606 - mae: 11.3417Restoring model weights from the end of the best epoch: 10.\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 4.61471\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "80/80 [==============================] - 7s 90ms/step - loss: 3.5606 - mae: 11.3417 - val_loss: 4.7522 - val_mae: 19.2474 - lr: 1.5259e-08\n",
            "Epoch 00040: early stopping\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 4.2660 - mae: 17.7715\n",
            "Fit model on training data fold  20\n",
            "Epoch 1/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 79.9023 - mae: 69.0141\n",
            "Epoch 00001: val_loss improved from inf to 6.36146, saving model to /home/jupyter/IEEE/fold20.h5\n",
            "80/80 [==============================] - 20s 130ms/step - loss: 79.9023 - mae: 69.0141 - val_loss: 6.3615 - val_mae: 19.4672 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 4.7170 - mae: 13.8211\n",
            "Epoch 00002: val_loss improved from 6.36146 to 5.94473, saving model to /home/jupyter/IEEE/fold20.h5\n",
            "80/80 [==============================] - 8s 100ms/step - loss: 4.7153 - mae: 13.8208 - val_loss: 5.9447 - val_mae: 20.1047 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 4.0008 - mae: 11.3852\n",
            "Epoch 00003: val_loss improved from 5.94473 to 5.30757, saving model to /home/jupyter/IEEE/fold20.h5\n",
            "80/80 [==============================] - 8s 96ms/step - loss: 4.0008 - mae: 11.3852 - val_loss: 5.3076 - val_mae: 19.0976 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.7418 - mae: 10.5481\n",
            "Epoch 00004: val_loss improved from 5.30757 to 4.34451, saving model to /home/jupyter/IEEE/fold20.h5\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 3.7417 - mae: 10.5549 - val_loss: 4.3445 - val_mae: 12.9791 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.6972 - mae: 10.4001\n",
            "Epoch 00005: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 3.6972 - mae: 10.4001 - val_loss: 5.9741 - val_mae: 24.7722 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.6246 - mae: 10.0565\n",
            "Epoch 00006: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 3.6246 - mae: 10.0565 - val_loss: 5.0012 - val_mae: 17.5515 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.3151 - mae: 8.3713\n",
            "Epoch 00007: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 7s 91ms/step - loss: 3.3151 - mae: 8.3713 - val_loss: 4.4462 - val_mae: 13.6774 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.2279 - mae: 7.7749\n",
            "Epoch 00008: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "80/80 [==============================] - 7s 94ms/step - loss: 3.2289 - mae: 7.7950 - val_loss: 4.6336 - val_mae: 13.8243 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.1240 - mae: 7.2627\n",
            "Epoch 00009: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.1240 - mae: 7.2627 - val_loss: 4.5267 - val_mae: 13.0961 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.0561 - mae: 6.7834\n",
            "Epoch 00010: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.0561 - mae: 6.7834 - val_loss: 4.5780 - val_mae: 13.4515 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 2.9684 - mae: 6.1188\n",
            "Epoch 00011: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 2.9684 - mae: 6.1188 - val_loss: 4.4811 - val_mae: 11.9567 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9171 - mae: 5.9371\n",
            "Epoch 00012: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 2.9170 - mae: 5.9324 - val_loss: 4.6459 - val_mae: 12.7883 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 2.8779 - mae: 5.6694\n",
            "Epoch 00013: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 2.8779 - mae: 5.6694 - val_loss: 4.5876 - val_mae: 11.7108 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.8378 - mae: 5.6873\n",
            "Epoch 00014: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 2.8379 - mae: 5.6832 - val_loss: 4.6342 - val_mae: 12.0874 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.8179 - mae: 5.4494\n",
            "Epoch 00015: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 2.8178 - mae: 5.4441 - val_loss: 4.5645 - val_mae: 11.5822 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 2.7981 - mae: 5.4051\n",
            "Epoch 00016: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 2.7981 - mae: 5.4051 - val_loss: 4.6267 - val_mae: 11.6712 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.7839 - mae: 5.2979\n",
            "Epoch 00017: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 2.7837 - mae: 5.2985 - val_loss: 4.6505 - val_mae: 11.9294 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.7916 - mae: 5.4237\n",
            "Epoch 00018: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 2.7927 - mae: 5.4261 - val_loss: 4.5107 - val_mae: 11.1228 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.7662 - mae: 5.3902\n",
            "Epoch 00019: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 2.7676 - mae: 5.3965 - val_loss: 4.7367 - val_mae: 11.8786 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 2.7620 - mae: 5.2586\n",
            "Epoch 00020: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 2.7620 - mae: 5.2586 - val_loss: 4.8142 - val_mae: 12.2145 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.7607 - mae: 5.1254\n",
            "Epoch 00021: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 2.7614 - mae: 5.1212 - val_loss: 4.8248 - val_mae: 12.0355 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.7663 - mae: 5.2418\n",
            "Epoch 00022: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 2.7665 - mae: 5.2445 - val_loss: 4.4758 - val_mae: 10.8614 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.7635 - mae: 5.2561\n",
            "Epoch 00023: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 2.7631 - mae: 5.2505 - val_loss: 4.7234 - val_mae: 11.7093 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 2.7543 - mae: 5.2943\n",
            "Epoch 00024: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "80/80 [==============================] - 7s 94ms/step - loss: 2.7543 - mae: 5.2943 - val_loss: 4.5117 - val_mae: 10.9557 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 2.7668 - mae: 5.2587\n",
            "Epoch 00025: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 7s 90ms/step - loss: 2.7668 - mae: 5.2587 - val_loss: 4.5764 - val_mae: 11.1355 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.7516 - mae: 5.2874\n",
            "Epoch 00026: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "80/80 [==============================] - 7s 90ms/step - loss: 2.7526 - mae: 5.2943 - val_loss: 4.6269 - val_mae: 11.1819 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.7600 - mae: 5.2250\n",
            "Epoch 00027: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 2.7624 - mae: 5.2328 - val_loss: 4.7028 - val_mae: 11.9825 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 2.7542 - mae: 5.1210\n",
            "Epoch 00028: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 2.7542 - mae: 5.1210 - val_loss: 4.4811 - val_mae: 10.7557 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.7629 - mae: 5.3256\n",
            "Epoch 00029: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 2.7626 - mae: 5.3237 - val_loss: 4.6505 - val_mae: 11.7040 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.7558 - mae: 5.3200\n",
            "Epoch 00030: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 2.7560 - mae: 5.3148 - val_loss: 4.5508 - val_mae: 11.2091 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.7546 - mae: 5.0535\n",
            "Epoch 00031: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 2.7559 - mae: 5.0539 - val_loss: 4.7552 - val_mae: 11.9502 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.7525 - mae: 5.2975\n",
            "Epoch 00032: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 2.7529 - mae: 5.2953 - val_loss: 4.6772 - val_mae: 11.4833 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 2.7657 - mae: 5.2150\n",
            "Epoch 00033: val_loss did not improve from 4.34451\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 2.7657 - mae: 5.2150 - val_loss: 4.6945 - val_mae: 11.4350 - lr: 6.1035e-08\n",
            "Epoch 34/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.7640 - mae: 5.1613Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 4.34451\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 2.7654 - mae: 5.1687 - val_loss: 4.6674 - val_mae: 11.5173 - lr: 6.1035e-08\n",
            "Epoch 00034: early stopping\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 6.1197 - mae: 21.1057\n",
            "Fit model on training data fold  21\n",
            "Epoch 1/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 68.1226 - mae: 68.7412\n",
            "Epoch 00001: val_loss improved from inf to 5.26040, saving model to /home/jupyter/IEEE/fold21.h5\n",
            "80/80 [==============================] - 19s 123ms/step - loss: 68.1226 - mae: 68.7412 - val_loss: 5.2604 - val_mae: 18.7127 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 4.7787 - mae: 17.8840\n",
            "Epoch 00002: val_loss improved from 5.26040 to 4.21413, saving model to /home/jupyter/IEEE/fold21.h5\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 4.7763 - mae: 17.8693 - val_loss: 4.2141 - val_mae: 14.7409 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 4.1515 - mae: 14.7751\n",
            "Epoch 00003: val_loss did not improve from 4.21413\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 4.1505 - mae: 14.8029 - val_loss: 4.6972 - val_mae: 16.6199 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.8764 - mae: 13.1985\n",
            "Epoch 00004: val_loss did not improve from 4.21413\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.8752 - mae: 13.1920 - val_loss: 4.4357 - val_mae: 15.4345 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.6537 - mae: 11.4087\n",
            "Epoch 00005: val_loss did not improve from 4.21413\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.6536 - mae: 11.4107 - val_loss: 4.3762 - val_mae: 15.2283 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.5422 - mae: 10.2757\n",
            "Epoch 00006: val_loss did not improve from 4.21413\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.5423 - mae: 10.2597 - val_loss: 5.0028 - val_mae: 17.1307 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.4421 - mae: 9.6049\n",
            "Epoch 00007: val_loss did not improve from 4.21413\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.4427 - mae: 9.6101 - val_loss: 4.4458 - val_mae: 14.4760 - lr: 2.5000e-04\n",
            "Epoch 8/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.5561 - mae: 10.1758\n",
            "Epoch 00008: val_loss improved from 4.21413 to 4.13375, saving model to /home/jupyter/IEEE/fold21.h5\n",
            "80/80 [==============================] - 7s 91ms/step - loss: 3.5561 - mae: 10.1758 - val_loss: 4.1338 - val_mae: 13.1326 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.3930 - mae: 9.1367\n",
            "Epoch 00009: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.3930 - mae: 9.1367 - val_loss: 4.2842 - val_mae: 13.7780 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.3231 - mae: 8.4143\n",
            "Epoch 00010: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.3228 - mae: 8.4047 - val_loss: 4.3907 - val_mae: 13.2730 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.2086 - mae: 8.0859\n",
            "Epoch 00011: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.2142 - mae: 8.1250 - val_loss: 4.5616 - val_mae: 13.3593 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.1680 - mae: 7.7926\n",
            "Epoch 00012: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.1687 - mae: 7.7849 - val_loss: 4.3769 - val_mae: 13.0234 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.1172 - mae: 7.4243\n",
            "Epoch 00013: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.1177 - mae: 7.4327 - val_loss: 4.3876 - val_mae: 12.4632 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0733 - mae: 7.1925\n",
            "Epoch 00014: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 3.0729 - mae: 7.1867 - val_loss: 4.3702 - val_mae: 12.2865 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 3.0346 - mae: 6.8090\n",
            "Epoch 00015: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.0346 - mae: 6.8090 - val_loss: 4.4595 - val_mae: 12.4864 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0204 - mae: 6.7659\n",
            "Epoch 00016: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.0216 - mae: 6.7678 - val_loss: 4.4959 - val_mae: 12.3029 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0210 - mae: 6.7309\n",
            "Epoch 00017: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 3.0218 - mae: 6.7395 - val_loss: 4.5695 - val_mae: 12.5812 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0057 - mae: 6.9206\n",
            "Epoch 00018: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.0072 - mae: 6.9215 - val_loss: 4.5704 - val_mae: 12.8376 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9965 - mae: 6.6555\n",
            "Epoch 00019: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 2.9986 - mae: 6.6730 - val_loss: 4.4685 - val_mae: 11.7675 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 3.0005 - mae: 6.6854\n",
            "Epoch 00020: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 3.0001 - mae: 6.6817 - val_loss: 4.5705 - val_mae: 12.4998 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 2.9989 - mae: 6.5991\n",
            "Epoch 00021: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 2.9989 - mae: 6.5991 - val_loss: 4.4762 - val_mae: 11.9502 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9886 - mae: 6.5196\n",
            "Epoch 00022: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 2.9886 - mae: 6.5199 - val_loss: 4.5208 - val_mae: 12.2322 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9928 - mae: 6.6048\n",
            "Epoch 00023: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 2.9929 - mae: 6.6175 - val_loss: 4.5637 - val_mae: 11.4965 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9711 - mae: 6.5384\n",
            "Epoch 00024: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 2.9720 - mae: 6.5429 - val_loss: 4.5326 - val_mae: 11.8932 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9827 - mae: 6.6856\n",
            "Epoch 00025: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 2.9837 - mae: 6.6871 - val_loss: 4.5531 - val_mae: 12.2641 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9812 - mae: 6.5830\n",
            "Epoch 00026: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 2.9817 - mae: 6.5936 - val_loss: 4.5009 - val_mae: 11.8252 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9888 - mae: 6.8267\n",
            "Epoch 00027: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 2.9903 - mae: 6.8283 - val_loss: 4.6137 - val_mae: 11.9999 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 2.9804 - mae: 6.7261\n",
            "Epoch 00028: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 2.9804 - mae: 6.7261 - val_loss: 4.5885 - val_mae: 12.3708 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9777 - mae: 6.5706\n",
            "Epoch 00029: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 2.9820 - mae: 6.5799 - val_loss: 4.5277 - val_mae: 12.0121 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9837 - mae: 6.4928\n",
            "Epoch 00030: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 2.9845 - mae: 6.4851 - val_loss: 4.6402 - val_mae: 12.2824 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9797 - mae: 6.5195\n",
            "Epoch 00031: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 2.9797 - mae: 6.5163 - val_loss: 4.5456 - val_mae: 12.7644 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9880 - mae: 6.4993\n",
            "Epoch 00032: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 2.9879 - mae: 6.5021 - val_loss: 4.5073 - val_mae: 12.2255 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9825 - mae: 6.5552\n",
            "Epoch 00033: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 2.9825 - mae: 6.5578 - val_loss: 4.5024 - val_mae: 11.6924 - lr: 6.1035e-08\n",
            "Epoch 34/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 2.9860 - mae: 6.8178\n",
            "Epoch 00034: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "80/80 [==============================] - 7s 88ms/step - loss: 2.9860 - mae: 6.8178 - val_loss: 4.4531 - val_mae: 12.4332 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9756 - mae: 6.5662\n",
            "Epoch 00035: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 2.9757 - mae: 6.5653 - val_loss: 4.5611 - val_mae: 12.0578 - lr: 3.0518e-08\n",
            "Epoch 36/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 2.9691 - mae: 6.4540\n",
            "Epoch 00036: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 2.9691 - mae: 6.4540 - val_loss: 4.5906 - val_mae: 12.7826 - lr: 3.0518e-08\n",
            "Epoch 37/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9791 - mae: 6.4920\n",
            "Epoch 00037: val_loss did not improve from 4.13375\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 2.9791 - mae: 6.4992 - val_loss: 4.4952 - val_mae: 12.2164 - lr: 1.5259e-08\n",
            "Epoch 38/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 2.9840 - mae: 6.6664Restoring model weights from the end of the best epoch: 8.\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 4.13375\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 2.9846 - mae: 6.6635 - val_loss: 4.5073 - val_mae: 12.3662 - lr: 1.5259e-08\n",
            "Epoch 00038: early stopping\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 6.1148 - mae: 26.0621\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2160x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval Mean 15.643461589614326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BAMI_Data_PPG, BAMI_Data_ACC, BAMI_Data_truth = preprocess_dataset(BAMI_Data)\n",
        "train_model_loso(BAMI_Data, BAMI_Data_PPG, BAMI_Data_ACC, BAMI_Data_truth, \"BAMI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e9ubAniqoABN",
        "outputId": "13180921-b229-4d52-dd1c-7d1d457cfef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fit model on training data fold  0\n",
            "Epoch 1/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 26.5969 - mae: 27.3459\n",
            "Epoch 00001: val_loss improved from inf to 3.86390, saving model to /home/jupyter/BAMI/fold0.h5\n",
            "238/238 [==============================] - 34s 104ms/step - loss: 26.5969 - mae: 27.3459 - val_loss: 3.8639 - val_mae: 10.0977 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 3.2564 - mae: 7.3044\n",
            "Epoch 00002: val_loss improved from 3.86390 to 3.17298, saving model to /home/jupyter/BAMI/fold0.h5\n",
            "238/238 [==============================] - 20s 86ms/step - loss: 3.2564 - mae: 7.3044 - val_loss: 3.1730 - val_mae: 6.4887 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 3.0742 - mae: 5.9938\n",
            "Epoch 00003: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 84ms/step - loss: 3.0742 - mae: 5.9938 - val_loss: 3.7341 - val_mae: 8.0158 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 3.0075 - mae: 5.5319\n",
            "Epoch 00004: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 3.0075 - mae: 5.5319 - val_loss: 3.7287 - val_mae: 7.2539 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.6513 - mae: 4.0691\n",
            "Epoch 00005: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.6513 - mae: 4.0691 - val_loss: 3.6886 - val_mae: 5.8631 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.6161 - mae: 3.6814\n",
            "Epoch 00006: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.6161 - mae: 3.6814 - val_loss: 3.8876 - val_mae: 5.8570 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.4263 - mae: 3.1397\n",
            "Epoch 00007: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.4263 - mae: 3.1397 - val_loss: 3.1808 - val_mae: 4.1197 - lr: 2.5000e-04\n",
            "Epoch 8/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.3456 - mae: 2.8466\n",
            "Epoch 00008: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.3456 - mae: 2.8466 - val_loss: 3.3221 - val_mae: 4.2630 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.2579 - mae: 2.5660\n",
            "Epoch 00009: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 84ms/step - loss: 2.2579 - mae: 2.5660 - val_loss: 3.5364 - val_mae: 4.2684 - lr: 1.2500e-04\n",
            "Epoch 10/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.2301 - mae: 2.5184\n",
            "Epoch 00010: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.2301 - mae: 2.5184 - val_loss: 3.4569 - val_mae: 4.0732 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1795 - mae: 2.4181\n",
            "Epoch 00011: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1795 - mae: 2.4181 - val_loss: 3.5801 - val_mae: 4.2094 - lr: 6.2500e-05\n",
            "Epoch 12/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1772 - mae: 2.3771\n",
            "Epoch 00012: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1772 - mae: 2.3771 - val_loss: 3.7029 - val_mae: 4.3176 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1604 - mae: 2.3119\n",
            "Epoch 00013: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 84ms/step - loss: 2.1604 - mae: 2.3119 - val_loss: 3.5666 - val_mae: 4.0693 - lr: 3.1250e-05\n",
            "Epoch 14/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1366 - mae: 2.2823\n",
            "Epoch 00014: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "238/238 [==============================] - 20s 84ms/step - loss: 2.1366 - mae: 2.2823 - val_loss: 4.0367 - val_mae: 4.6312 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1248 - mae: 2.2778\n",
            "Epoch 00015: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1248 - mae: 2.2778 - val_loss: 3.9667 - val_mae: 4.4961 - lr: 1.5625e-05\n",
            "Epoch 16/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1253 - mae: 2.2334\n",
            "Epoch 00016: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1253 - mae: 2.2334 - val_loss: 3.5947 - val_mae: 3.8731 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1163 - mae: 2.2566\n",
            "Epoch 00017: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1163 - mae: 2.2566 - val_loss: 3.6416 - val_mae: 4.0221 - lr: 7.8125e-06\n",
            "Epoch 18/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1119 - mae: 2.1769\n",
            "Epoch 00018: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1119 - mae: 2.1769 - val_loss: 3.8589 - val_mae: 4.2496 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1068 - mae: 2.1781\n",
            "Epoch 00019: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1068 - mae: 2.1781 - val_loss: 3.7261 - val_mae: 4.1092 - lr: 3.9063e-06\n",
            "Epoch 20/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1110 - mae: 2.1869\n",
            "Epoch 00020: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1110 - mae: 2.1869 - val_loss: 3.5720 - val_mae: 3.8640 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1053 - mae: 2.1858\n",
            "Epoch 00021: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 84ms/step - loss: 2.1053 - mae: 2.1858 - val_loss: 4.1678 - val_mae: 4.7451 - lr: 1.9531e-06\n",
            "Epoch 22/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1050 - mae: 2.2003\n",
            "Epoch 00022: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1050 - mae: 2.2003 - val_loss: 3.6731 - val_mae: 4.0294 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1108 - mae: 2.2128\n",
            "Epoch 00023: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1108 - mae: 2.2128 - val_loss: 3.6505 - val_mae: 4.0576 - lr: 9.7656e-07\n",
            "Epoch 24/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1104 - mae: 2.2285\n",
            "Epoch 00024: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1104 - mae: 2.2285 - val_loss: 3.8925 - val_mae: 4.4215 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1069 - mae: 2.2101\n",
            "Epoch 00025: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1069 - mae: 2.2101 - val_loss: 3.7225 - val_mae: 4.1453 - lr: 4.8828e-07\n",
            "Epoch 26/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1063 - mae: 2.2077\n",
            "Epoch 00026: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1063 - mae: 2.2077 - val_loss: 3.5865 - val_mae: 3.9257 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1013 - mae: 2.2066\n",
            "Epoch 00027: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 84ms/step - loss: 2.1013 - mae: 2.2066 - val_loss: 3.5645 - val_mae: 3.9901 - lr: 2.4414e-07\n",
            "Epoch 28/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1076 - mae: 2.1942\n",
            "Epoch 00028: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "238/238 [==============================] - 20s 84ms/step - loss: 2.1076 - mae: 2.1942 - val_loss: 3.9538 - val_mae: 4.4531 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1045 - mae: 2.2210\n",
            "Epoch 00029: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1045 - mae: 2.2210 - val_loss: 3.8229 - val_mae: 4.3273 - lr: 1.2207e-07\n",
            "Epoch 30/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1120 - mae: 2.2105\n",
            "Epoch 00030: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1120 - mae: 2.2105 - val_loss: 3.8845 - val_mae: 4.2474 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1098 - mae: 2.2207\n",
            "Epoch 00031: val_loss did not improve from 3.17298\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1098 - mae: 2.2207 - val_loss: 3.7664 - val_mae: 4.1474 - lr: 6.1035e-08\n",
            "Epoch 32/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1038 - mae: 2.1841Restoring model weights from the end of the best epoch: 2.\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 3.17298\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1038 - mae: 2.1841 - val_loss: 3.6491 - val_mae: 4.1084 - lr: 6.1035e-08\n",
            "Epoch 00032: early stopping\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 4.0597 - mae: 8.9400\n",
            "Fit model on training data fold  1\n",
            "Epoch 1/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 14.7730 - mae: 27.9782\n",
            "Epoch 00001: val_loss improved from inf to 5.78869, saving model to /home/jupyter/BAMI/fold1.h5\n",
            "238/238 [==============================] - 34s 104ms/step - loss: 14.7730 - mae: 27.9782 - val_loss: 5.7887 - val_mae: 27.4754 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 3.8879 - mae: 13.5401\n",
            "Epoch 00002: val_loss improved from 5.78869 to 4.47197, saving model to /home/jupyter/BAMI/fold1.h5\n",
            "238/238 [==============================] - 20s 86ms/step - loss: 3.8879 - mae: 13.5401 - val_loss: 4.4720 - val_mae: 16.2516 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 3.6040 - mae: 10.6791\n",
            "Epoch 00003: val_loss did not improve from 4.47197\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 3.6040 - mae: 10.6791 - val_loss: 4.4950 - val_mae: 14.6629 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 3.3392 - mae: 8.3462\n",
            "Epoch 00004: val_loss did not improve from 4.47197\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 3.3392 - mae: 8.3462 - val_loss: 4.8521 - val_mae: 13.2220 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.9697 - mae: 5.8853\n",
            "Epoch 00005: val_loss did not improve from 4.47197\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.9697 - mae: 5.8853 - val_loss: 4.5752 - val_mae: 10.5285 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.8756 - mae: 4.9388\n",
            "Epoch 00006: val_loss improved from 4.47197 to 4.06752, saving model to /home/jupyter/BAMI/fold1.h5\n",
            "238/238 [==============================] - 21s 86ms/step - loss: 2.8756 - mae: 4.9388 - val_loss: 4.0675 - val_mae: 7.6470 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.7624 - mae: 4.4382\n",
            "Epoch 00007: val_loss improved from 4.06752 to 3.89148, saving model to /home/jupyter/BAMI/fold1.h5\n",
            "238/238 [==============================] - 21s 86ms/step - loss: 2.7624 - mae: 4.4382 - val_loss: 3.8915 - val_mae: 6.4606 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.6912 - mae: 4.0013\n",
            "Epoch 00008: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.6912 - mae: 4.0013 - val_loss: 7.8046 - val_mae: 15.6353 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.5762 - mae: 3.6361\n",
            "Epoch 00009: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.5762 - mae: 3.6361 - val_loss: 4.2667 - val_mae: 6.5953 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.3916 - mae: 3.0210\n",
            "Epoch 00010: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.3916 - mae: 3.0210 - val_loss: 4.9168 - val_mae: 6.7063 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.3501 - mae: 2.7906\n",
            "Epoch 00011: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.3501 - mae: 2.7906 - val_loss: 4.3424 - val_mae: 5.5917 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.2557 - mae: 2.5804\n",
            "Epoch 00012: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.2557 - mae: 2.5804 - val_loss: 4.2053 - val_mae: 5.2539 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.2251 - mae: 2.5280\n",
            "Epoch 00013: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.2251 - mae: 2.5280 - val_loss: 5.2110 - val_mae: 6.3851 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1850 - mae: 2.3794\n",
            "Epoch 00014: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1850 - mae: 2.3794 - val_loss: 5.0486 - val_mae: 6.1840 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1740 - mae: 2.4072\n",
            "Epoch 00015: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1740 - mae: 2.4072 - val_loss: 5.7017 - val_mae: 6.7197 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1520 - mae: 2.3585\n",
            "Epoch 00016: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1520 - mae: 2.3585 - val_loss: 5.1842 - val_mae: 5.9988 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1336 - mae: 2.2862\n",
            "Epoch 00017: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1336 - mae: 2.2862 - val_loss: 4.7903 - val_mae: 5.3784 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1208 - mae: 2.2391\n",
            "Epoch 00018: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1208 - mae: 2.2391 - val_loss: 3.9863 - val_mae: 4.4706 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1211 - mae: 2.2580\n",
            "Epoch 00019: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1211 - mae: 2.2580 - val_loss: 4.7424 - val_mae: 5.3598 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1039 - mae: 2.2501\n",
            "Epoch 00020: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1039 - mae: 2.2501 - val_loss: 5.0796 - val_mae: 5.6618 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1017 - mae: 2.2048\n",
            "Epoch 00021: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1017 - mae: 2.2048 - val_loss: 5.1894 - val_mae: 5.8966 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0999 - mae: 2.2013\n",
            "Epoch 00022: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.0999 - mae: 2.2013 - val_loss: 5.1312 - val_mae: 5.8449 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0987 - mae: 2.1953\n",
            "Epoch 00023: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.0987 - mae: 2.1953 - val_loss: 5.2854 - val_mae: 5.9067 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0909 - mae: 2.2172\n",
            "Epoch 00024: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 86ms/step - loss: 2.0909 - mae: 2.2172 - val_loss: 5.4922 - val_mae: 6.1793 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0982 - mae: 2.1948\n",
            "Epoch 00025: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.0982 - mae: 2.1948 - val_loss: 4.7125 - val_mae: 5.1931 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0893 - mae: 2.1868\n",
            "Epoch 00026: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.0893 - mae: 2.1868 - val_loss: 4.4760 - val_mae: 4.9840 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0937 - mae: 2.1770\n",
            "Epoch 00027: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.0937 - mae: 2.1770 - val_loss: 4.7140 - val_mae: 5.1916 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0910 - mae: 2.2227\n",
            "Epoch 00028: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.0910 - mae: 2.2227 - val_loss: 5.2125 - val_mae: 5.7980 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0968 - mae: 2.1848\n",
            "Epoch 00029: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.0968 - mae: 2.1848 - val_loss: 4.5592 - val_mae: 4.9779 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0953 - mae: 2.1687\n",
            "Epoch 00030: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.0953 - mae: 2.1687 - val_loss: 6.4701 - val_mae: 7.2756 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1015 - mae: 2.1968\n",
            "Epoch 00031: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1015 - mae: 2.1968 - val_loss: 4.5677 - val_mae: 4.9627 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0862 - mae: 2.1910\n",
            "Epoch 00032: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.0862 - mae: 2.1910 - val_loss: 4.6667 - val_mae: 5.1491 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0963 - mae: 2.2024\n",
            "Epoch 00033: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.0963 - mae: 2.2024 - val_loss: 4.7435 - val_mae: 5.2412 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0932 - mae: 2.1716\n",
            "Epoch 00034: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.0932 - mae: 2.1716 - val_loss: 5.1712 - val_mae: 5.7001 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0947 - mae: 2.1700\n",
            "Epoch 00035: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.0947 - mae: 2.1700 - val_loss: 4.7987 - val_mae: 5.1910 - lr: 6.1035e-08\n",
            "Epoch 36/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0859 - mae: 2.1753\n",
            "Epoch 00036: val_loss did not improve from 3.89148\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.0859 - mae: 2.1753 - val_loss: 5.1177 - val_mae: 5.7560 - lr: 3.0518e-08\n",
            "Epoch 37/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.0941 - mae: 2.1629Restoring model weights from the end of the best epoch: 7.\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 3.89148\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "238/238 [==============================] - 20s 86ms/step - loss: 2.0941 - mae: 2.1629 - val_loss: 5.3745 - val_mae: 5.9564 - lr: 3.0518e-08\n",
            "Epoch 00037: early stopping\n",
            "14/14 [==============================] - 0s 23ms/step - loss: 3.6828 - mae: 6.1066\n",
            "Fit model on training data fold  2\n",
            "Epoch 1/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 26.0929 - mae: 33.9709\n",
            "Epoch 00001: val_loss improved from inf to 4.05457, saving model to /home/jupyter/BAMI/fold2.h5\n",
            "238/238 [==============================] - 35s 107ms/step - loss: 26.0929 - mae: 33.9709 - val_loss: 4.0546 - val_mae: 13.9895 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 3.5344 - mae: 9.8809\n",
            "Epoch 00002: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 86ms/step - loss: 3.5344 - mae: 9.8809 - val_loss: 5.1599 - val_mae: 16.2282 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 3.2630 - mae: 7.2396\n",
            "Epoch 00003: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 3.2630 - mae: 7.2396 - val_loss: 4.4830 - val_mae: 10.3835 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.8712 - mae: 4.9218\n",
            "Epoch 00004: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.8712 - mae: 4.9218 - val_loss: 4.6914 - val_mae: 8.9671 - lr: 5.0000e-04\n",
            "Epoch 5/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.7971 - mae: 4.4231\n",
            "Epoch 00005: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.7971 - mae: 4.4231 - val_loss: 4.1737 - val_mae: 7.3117 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.5600 - mae: 3.5191\n",
            "Epoch 00006: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.5600 - mae: 3.5191 - val_loss: 4.3187 - val_mae: 6.8258 - lr: 2.5000e-04\n",
            "Epoch 7/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.4962 - mae: 3.2720\n",
            "Epoch 00007: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.4962 - mae: 3.2720 - val_loss: 4.1365 - val_mae: 6.1848 - lr: 2.5000e-04\n",
            "Epoch 8/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.3806 - mae: 2.9868\n",
            "Epoch 00008: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.3806 - mae: 2.9868 - val_loss: 4.8935 - val_mae: 6.8165 - lr: 1.2500e-04\n",
            "Epoch 9/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.3501 - mae: 2.8240\n",
            "Epoch 00009: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.3501 - mae: 2.8240 - val_loss: 4.5340 - val_mae: 6.0530 - lr: 1.2500e-04\n",
            "Epoch 10/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.2860 - mae: 2.7070\n",
            "Epoch 00010: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.2860 - mae: 2.7070 - val_loss: 5.1419 - val_mae: 6.5349 - lr: 6.2500e-05\n",
            "Epoch 11/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.2605 - mae: 2.5917\n",
            "Epoch 00011: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.2605 - mae: 2.5917 - val_loss: 5.3088 - val_mae: 6.8799 - lr: 6.2500e-05\n",
            "Epoch 12/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.2343 - mae: 2.5785\n",
            "Epoch 00012: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 84ms/step - loss: 2.2343 - mae: 2.5785 - val_loss: 5.8304 - val_mae: 7.4160 - lr: 3.1250e-05\n",
            "Epoch 13/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.2315 - mae: 2.5457\n",
            "Epoch 00013: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.2315 - mae: 2.5457 - val_loss: 5.3107 - val_mae: 6.7070 - lr: 3.1250e-05\n",
            "Epoch 14/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.2061 - mae: 2.5011\n",
            "Epoch 00014: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.2061 - mae: 2.5011 - val_loss: 5.0247 - val_mae: 6.2964 - lr: 1.5625e-05\n",
            "Epoch 15/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.2043 - mae: 2.5131\n",
            "Epoch 00015: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.2043 - mae: 2.5131 - val_loss: 5.1891 - val_mae: 6.4220 - lr: 1.5625e-05\n",
            "Epoch 16/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1853 - mae: 2.4432\n",
            "Epoch 00016: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1853 - mae: 2.4432 - val_loss: 4.6930 - val_mae: 5.8383 - lr: 7.8125e-06\n",
            "Epoch 17/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1819 - mae: 2.4071\n",
            "Epoch 00017: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1819 - mae: 2.4071 - val_loss: 5.1812 - val_mae: 6.4101 - lr: 7.8125e-06\n",
            "Epoch 18/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1838 - mae: 2.4229\n",
            "Epoch 00018: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1838 - mae: 2.4229 - val_loss: 5.3032 - val_mae: 6.4721 - lr: 3.9063e-06\n",
            "Epoch 19/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1824 - mae: 2.4193\n",
            "Epoch 00019: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "238/238 [==============================] - 20s 86ms/step - loss: 2.1824 - mae: 2.4193 - val_loss: 5.3464 - val_mae: 6.4763 - lr: 3.9063e-06\n",
            "Epoch 20/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1763 - mae: 2.4222\n",
            "Epoch 00020: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1763 - mae: 2.4222 - val_loss: 5.0965 - val_mae: 6.2008 - lr: 1.9531e-06\n",
            "Epoch 21/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1655 - mae: 2.3925\n",
            "Epoch 00021: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1655 - mae: 2.3925 - val_loss: 5.1249 - val_mae: 6.2796 - lr: 1.9531e-06\n",
            "Epoch 22/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1756 - mae: 2.3592\n",
            "Epoch 00022: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1756 - mae: 2.3592 - val_loss: 5.1843 - val_mae: 6.3501 - lr: 9.7656e-07\n",
            "Epoch 23/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1756 - mae: 2.4091\n",
            "Epoch 00023: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1756 - mae: 2.4091 - val_loss: 5.1215 - val_mae: 6.1119 - lr: 9.7656e-07\n",
            "Epoch 24/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1770 - mae: 2.4164\n",
            "Epoch 00024: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1770 - mae: 2.4164 - val_loss: 5.0334 - val_mae: 6.0665 - lr: 4.8828e-07\n",
            "Epoch 25/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1805 - mae: 2.4049\n",
            "Epoch 00025: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1805 - mae: 2.4049 - val_loss: 5.3963 - val_mae: 6.4942 - lr: 4.8828e-07\n",
            "Epoch 26/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1677 - mae: 2.3883\n",
            "Epoch 00026: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1677 - mae: 2.3883 - val_loss: 5.5004 - val_mae: 6.6592 - lr: 2.4414e-07\n",
            "Epoch 27/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1766 - mae: 2.4121\n",
            "Epoch 00027: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1766 - mae: 2.4121 - val_loss: 5.4683 - val_mae: 6.6674 - lr: 2.4414e-07\n",
            "Epoch 28/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1675 - mae: 2.4039\n",
            "Epoch 00028: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 85ms/step - loss: 2.1675 - mae: 2.4039 - val_loss: 4.9649 - val_mae: 6.0116 - lr: 1.2207e-07\n",
            "Epoch 29/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1832 - mae: 2.4202\n",
            "Epoch 00029: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "238/238 [==============================] - 20s 84ms/step - loss: 2.1832 - mae: 2.4202 - val_loss: 5.3277 - val_mae: 6.4638 - lr: 1.2207e-07\n",
            "Epoch 30/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1709 - mae: 2.3604\n",
            "Epoch 00030: val_loss did not improve from 4.05457\n",
            "238/238 [==============================] - 20s 84ms/step - loss: 2.1709 - mae: 2.3604 - val_loss: 5.0817 - val_mae: 6.0474 - lr: 6.1035e-08\n",
            "Epoch 31/200\n",
            "238/238 [==============================] - ETA: 0s - loss: 2.1889 - mae: 2.4473Restoring model weights from the end of the best epoch: 1.\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 4.05457\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "238/238 [==============================] - 20s 84ms/step - loss: 2.1889 - mae: 2.4473 - val_loss: 5.3401 - val_mae: 6.5488 - lr: 6.1035e-08\n",
            "Epoch 00031: early stopping\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 3.8670 - mae: 12.5148\n",
            "Fit model on training data fold  3\n",
            "Epoch 1/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 18.5366 - mae: 23.3993\n",
            "Epoch 00001: val_loss improved from inf to 3.62272, saving model to /home/jupyter/BAMI/fold3.h5\n",
            "241/241 [==============================] - 34s 103ms/step - loss: 18.5366 - mae: 23.3993 - val_loss: 3.6227 - val_mae: 9.6333 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.5733 - mae: 9.5996\n",
            "Epoch 00002: val_loss improved from 3.62272 to 3.28376, saving model to /home/jupyter/BAMI/fold3.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 3.5733 - mae: 9.5996 - val_loss: 3.2838 - val_mae: 8.1991 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.3963 - mae: 8.5177\n",
            "Epoch 00003: val_loss improved from 3.28376 to 3.21755, saving model to /home/jupyter/BAMI/fold3.h5\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 3.3963 - mae: 8.5177 - val_loss: 3.2176 - val_mae: 7.6035 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.2887 - mae: 7.4430\n",
            "Epoch 00004: val_loss did not improve from 3.21755\n",
            "241/241 [==============================] - 20s 84ms/step - loss: 3.2887 - mae: 7.4430 - val_loss: 3.4537 - val_mae: 8.0378 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.1544 - mae: 6.3569\n",
            "Epoch 00005: val_loss improved from 3.21755 to 2.97975, saving model to /home/jupyter/BAMI/fold3.h5\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 3.1544 - mae: 6.3569 - val_loss: 2.9798 - val_mae: 5.7878 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.0508 - mae: 5.8379\n",
            "Epoch 00006: val_loss did not improve from 2.97975\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 3.0508 - mae: 5.8379 - val_loss: 3.0216 - val_mae: 5.6631 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.9713 - mae: 5.2926\n",
            "Epoch 00007: val_loss improved from 2.97975 to 2.86854, saving model to /home/jupyter/BAMI/fold3.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.9713 - mae: 5.2926 - val_loss: 2.8685 - val_mae: 5.0292 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.9067 - mae: 4.8825\n",
            "Epoch 00008: val_loss improved from 2.86854 to 2.66816, saving model to /home/jupyter/BAMI/fold3.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.9067 - mae: 4.8825 - val_loss: 2.6682 - val_mae: 4.3479 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.8325 - mae: 4.5434\n",
            "Epoch 00009: val_loss did not improve from 2.66816\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.8325 - mae: 4.5434 - val_loss: 3.1262 - val_mae: 4.9868 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.8093 - mae: 4.3771\n",
            "Epoch 00010: val_loss did not improve from 2.66816\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.8093 - mae: 4.3771 - val_loss: 2.9064 - val_mae: 4.3766 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.5326 - mae: 3.4002\n",
            "Epoch 00011: val_loss did not improve from 2.66816\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.5326 - mae: 3.4002 - val_loss: 2.8553 - val_mae: 3.7808 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.4913 - mae: 3.2042\n",
            "Epoch 00012: val_loss improved from 2.66816 to 2.46665, saving model to /home/jupyter/BAMI/fold3.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.4913 - mae: 3.2042 - val_loss: 2.4667 - val_mae: 3.2030 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.4329 - mae: 2.9921\n",
            "Epoch 00013: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.4329 - mae: 2.9921 - val_loss: 2.4671 - val_mae: 3.1153 - lr: 5.0000e-04\n",
            "Epoch 14/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.4087 - mae: 3.0107\n",
            "Epoch 00014: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.4087 - mae: 3.0107 - val_loss: 2.8983 - val_mae: 3.7678 - lr: 5.0000e-04\n",
            "Epoch 15/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2739 - mae: 2.6333\n",
            "Epoch 00015: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.2739 - mae: 2.6333 - val_loss: 2.7088 - val_mae: 3.2652 - lr: 2.5000e-04\n",
            "Epoch 16/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2441 - mae: 2.5010\n",
            "Epoch 00016: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.2441 - mae: 2.5010 - val_loss: 2.5025 - val_mae: 2.9597 - lr: 2.5000e-04\n",
            "Epoch 17/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1809 - mae: 2.3983\n",
            "Epoch 00017: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1809 - mae: 2.3983 - val_loss: 2.5562 - val_mae: 2.7653 - lr: 1.2500e-04\n",
            "Epoch 18/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1581 - mae: 2.3008\n",
            "Epoch 00018: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1581 - mae: 2.3008 - val_loss: 2.6010 - val_mae: 2.7963 - lr: 1.2500e-04\n",
            "Epoch 19/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1224 - mae: 2.2195\n",
            "Epoch 00019: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1224 - mae: 2.2195 - val_loss: 2.6419 - val_mae: 2.8060 - lr: 6.2500e-05\n",
            "Epoch 20/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1070 - mae: 2.2331\n",
            "Epoch 00020: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1070 - mae: 2.2331 - val_loss: 2.5470 - val_mae: 2.6241 - lr: 6.2500e-05\n",
            "Epoch 21/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0929 - mae: 2.1625\n",
            "Epoch 00021: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0929 - mae: 2.1625 - val_loss: 2.6657 - val_mae: 2.7692 - lr: 3.1250e-05\n",
            "Epoch 22/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0970 - mae: 2.1675\n",
            "Epoch 00022: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0970 - mae: 2.1675 - val_loss: 2.6917 - val_mae: 2.8701 - lr: 3.1250e-05\n",
            "Epoch 23/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0738 - mae: 2.1636\n",
            "Epoch 00023: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0738 - mae: 2.1636 - val_loss: 2.8263 - val_mae: 2.9311 - lr: 1.5625e-05\n",
            "Epoch 24/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0676 - mae: 2.1302\n",
            "Epoch 00024: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0676 - mae: 2.1302 - val_loss: 2.6692 - val_mae: 2.7172 - lr: 1.5625e-05\n",
            "Epoch 25/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0694 - mae: 2.1223\n",
            "Epoch 00025: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0694 - mae: 2.1223 - val_loss: 2.6411 - val_mae: 2.7228 - lr: 7.8125e-06\n",
            "Epoch 26/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0696 - mae: 2.0944\n",
            "Epoch 00026: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0696 - mae: 2.0944 - val_loss: 2.7152 - val_mae: 2.8308 - lr: 7.8125e-06\n",
            "Epoch 27/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0593 - mae: 2.1192\n",
            "Epoch 00027: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0593 - mae: 2.1192 - val_loss: 2.6639 - val_mae: 2.6872 - lr: 3.9063e-06\n",
            "Epoch 28/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0542 - mae: 2.0957\n",
            "Epoch 00028: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0542 - mae: 2.0957 - val_loss: 2.7039 - val_mae: 2.8504 - lr: 3.9063e-06\n",
            "Epoch 29/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0549 - mae: 2.1054\n",
            "Epoch 00029: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0549 - mae: 2.1054 - val_loss: 2.6233 - val_mae: 2.6221 - lr: 1.9531e-06\n",
            "Epoch 30/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0590 - mae: 2.0980\n",
            "Epoch 00030: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0590 - mae: 2.0980 - val_loss: 2.5904 - val_mae: 2.7170 - lr: 1.9531e-06\n",
            "Epoch 31/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0567 - mae: 2.0809\n",
            "Epoch 00031: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0567 - mae: 2.0809 - val_loss: 2.6393 - val_mae: 2.6610 - lr: 9.7656e-07\n",
            "Epoch 32/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0619 - mae: 2.0904\n",
            "Epoch 00032: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0619 - mae: 2.0904 - val_loss: 2.6461 - val_mae: 2.7600 - lr: 9.7656e-07\n",
            "Epoch 33/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0581 - mae: 2.1171\n",
            "Epoch 00033: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0581 - mae: 2.1171 - val_loss: 2.6749 - val_mae: 2.7943 - lr: 4.8828e-07\n",
            "Epoch 34/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0533 - mae: 2.0649\n",
            "Epoch 00034: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0533 - mae: 2.0649 - val_loss: 2.7363 - val_mae: 2.7897 - lr: 4.8828e-07\n",
            "Epoch 35/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0468 - mae: 2.1074\n",
            "Epoch 00035: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0468 - mae: 2.1074 - val_loss: 2.9085 - val_mae: 3.0325 - lr: 2.4414e-07\n",
            "Epoch 36/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0536 - mae: 2.0994\n",
            "Epoch 00036: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0536 - mae: 2.0994 - val_loss: 2.6724 - val_mae: 2.8083 - lr: 2.4414e-07\n",
            "Epoch 37/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0601 - mae: 2.0815\n",
            "Epoch 00037: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0601 - mae: 2.0815 - val_loss: 2.6655 - val_mae: 2.6877 - lr: 1.2207e-07\n",
            "Epoch 38/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0556 - mae: 2.1107\n",
            "Epoch 00038: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0556 - mae: 2.1107 - val_loss: 2.7178 - val_mae: 2.8230 - lr: 1.2207e-07\n",
            "Epoch 39/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0635 - mae: 2.1286\n",
            "Epoch 00039: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0635 - mae: 2.1286 - val_loss: 2.8149 - val_mae: 2.8210 - lr: 6.1035e-08\n",
            "Epoch 40/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0550 - mae: 2.0883\n",
            "Epoch 00040: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0550 - mae: 2.0883 - val_loss: 2.6633 - val_mae: 2.7591 - lr: 6.1035e-08\n",
            "Epoch 41/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0583 - mae: 2.1150\n",
            "Epoch 00041: val_loss did not improve from 2.46665\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0583 - mae: 2.1150 - val_loss: 2.6614 - val_mae: 2.7829 - lr: 3.0518e-08\n",
            "Epoch 42/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0515 - mae: 2.0793Restoring model weights from the end of the best epoch: 12.\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 2.46665\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0515 - mae: 2.0793 - val_loss: 2.7321 - val_mae: 2.7101 - lr: 3.0518e-08\n",
            "Epoch 00042: early stopping\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 2.2305 - mae: 2.7062\n",
            "Fit model on training data fold  4\n",
            "Epoch 1/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 17.5356 - mae: 25.2892\n",
            "Epoch 00001: val_loss improved from inf to 3.85924, saving model to /home/jupyter/BAMI/fold4.h5\n",
            "241/241 [==============================] - 36s 110ms/step - loss: 17.5356 - mae: 25.2892 - val_loss: 3.8592 - val_mae: 11.5520 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.5447 - mae: 9.8012\n",
            "Epoch 00002: val_loss improved from 3.85924 to 3.35607, saving model to /home/jupyter/BAMI/fold4.h5\n",
            "241/241 [==============================] - 23s 94ms/step - loss: 3.5447 - mae: 9.8012 - val_loss: 3.3561 - val_mae: 8.3711 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.3318 - mae: 8.1057\n",
            "Epoch 00003: val_loss improved from 3.35607 to 2.94084, saving model to /home/jupyter/BAMI/fold4.h5\n",
            "241/241 [==============================] - 22s 92ms/step - loss: 3.3318 - mae: 8.1057 - val_loss: 2.9408 - val_mae: 6.2422 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.1459 - mae: 6.5812\n",
            "Epoch 00004: val_loss improved from 2.94084 to 2.86831, saving model to /home/jupyter/BAMI/fold4.h5\n",
            "241/241 [==============================] - 22s 90ms/step - loss: 3.1459 - mae: 6.5812 - val_loss: 2.8683 - val_mae: 5.6365 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.0806 - mae: 5.9330\n",
            "Epoch 00005: val_loss improved from 2.86831 to 2.70400, saving model to /home/jupyter/BAMI/fold4.h5\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 3.0806 - mae: 5.9330 - val_loss: 2.7040 - val_mae: 4.9100 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.9540 - mae: 5.2397\n",
            "Epoch 00006: val_loss improved from 2.70400 to 2.56402, saving model to /home/jupyter/BAMI/fold4.h5\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 2.9540 - mae: 5.2397 - val_loss: 2.5640 - val_mae: 4.2069 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.8879 - mae: 4.8430\n",
            "Epoch 00007: val_loss improved from 2.56402 to 2.49815, saving model to /home/jupyter/BAMI/fold4.h5\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.8879 - mae: 4.8430 - val_loss: 2.4981 - val_mae: 4.0552 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.7889 - mae: 4.3625\n",
            "Epoch 00008: val_loss improved from 2.49815 to 2.44187, saving model to /home/jupyter/BAMI/fold4.h5\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.7889 - mae: 4.3625 - val_loss: 2.4419 - val_mae: 3.4590 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.7655 - mae: 4.2650\n",
            "Epoch 00009: val_loss did not improve from 2.44187\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.7655 - mae: 4.2650 - val_loss: 2.5668 - val_mae: 3.7518 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.6794 - mae: 4.0293\n",
            "Epoch 00010: val_loss did not improve from 2.44187\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.6794 - mae: 4.0293 - val_loss: 2.5747 - val_mae: 3.5839 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.3909 - mae: 2.9981\n",
            "Epoch 00011: val_loss improved from 2.44187 to 2.34758, saving model to /home/jupyter/BAMI/fold4.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.3909 - mae: 2.9981 - val_loss: 2.3476 - val_mae: 2.9319 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.3579 - mae: 2.8421\n",
            "Epoch 00012: val_loss improved from 2.34758 to 2.20700, saving model to /home/jupyter/BAMI/fold4.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.3579 - mae: 2.8421 - val_loss: 2.2070 - val_mae: 2.5763 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.3373 - mae: 2.7902\n",
            "Epoch 00013: val_loss improved from 2.20700 to 2.12983, saving model to /home/jupyter/BAMI/fold4.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.3373 - mae: 2.7902 - val_loss: 2.1298 - val_mae: 2.4838 - lr: 5.0000e-04\n",
            "Epoch 14/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.3070 - mae: 2.7253\n",
            "Epoch 00014: val_loss did not improve from 2.12983\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.3070 - mae: 2.7253 - val_loss: 2.2420 - val_mae: 2.6417 - lr: 5.0000e-04\n",
            "Epoch 15/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2805 - mae: 2.6702\n",
            "Epoch 00015: val_loss did not improve from 2.12983\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.2805 - mae: 2.6702 - val_loss: 2.1723 - val_mae: 2.5570 - lr: 5.0000e-04\n",
            "Epoch 16/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1679 - mae: 2.3980\n",
            "Epoch 00016: val_loss improved from 2.12983 to 2.01225, saving model to /home/jupyter/BAMI/fold4.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1679 - mae: 2.3980 - val_loss: 2.0123 - val_mae: 2.1984 - lr: 2.5000e-04\n",
            "Epoch 17/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1326 - mae: 2.2873\n",
            "Epoch 00017: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1326 - mae: 2.2873 - val_loss: 2.1214 - val_mae: 2.1951 - lr: 2.5000e-04\n",
            "Epoch 18/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1087 - mae: 2.2283\n",
            "Epoch 00018: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1087 - mae: 2.2283 - val_loss: 2.0942 - val_mae: 2.2010 - lr: 2.5000e-04\n",
            "Epoch 19/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0531 - mae: 2.1180\n",
            "Epoch 00019: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0531 - mae: 2.1180 - val_loss: 2.1458 - val_mae: 2.2246 - lr: 1.2500e-04\n",
            "Epoch 20/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0370 - mae: 2.0973\n",
            "Epoch 00020: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0370 - mae: 2.0973 - val_loss: 2.0680 - val_mae: 2.0995 - lr: 1.2500e-04\n",
            "Epoch 21/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9968 - mae: 2.0318\n",
            "Epoch 00021: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9968 - mae: 2.0318 - val_loss: 2.0510 - val_mae: 1.9596 - lr: 6.2500e-05\n",
            "Epoch 22/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9832 - mae: 1.9406\n",
            "Epoch 00022: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9832 - mae: 1.9406 - val_loss: 2.1563 - val_mae: 2.1323 - lr: 6.2500e-05\n",
            "Epoch 23/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9724 - mae: 1.9609\n",
            "Epoch 00023: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 1.9724 - mae: 1.9609 - val_loss: 2.1120 - val_mae: 2.0537 - lr: 3.1250e-05\n",
            "Epoch 24/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9555 - mae: 1.9106\n",
            "Epoch 00024: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9555 - mae: 1.9106 - val_loss: 2.1301 - val_mae: 2.1062 - lr: 3.1250e-05\n",
            "Epoch 25/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9418 - mae: 1.8933\n",
            "Epoch 00025: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9418 - mae: 1.8933 - val_loss: 2.0835 - val_mae: 1.9865 - lr: 1.5625e-05\n",
            "Epoch 26/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9483 - mae: 1.8994\n",
            "Epoch 00026: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9483 - mae: 1.8994 - val_loss: 2.1430 - val_mae: 1.9822 - lr: 1.5625e-05\n",
            "Epoch 27/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9422 - mae: 1.9107\n",
            "Epoch 00027: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 1.9422 - mae: 1.9107 - val_loss: 2.1050 - val_mae: 1.9813 - lr: 7.8125e-06\n",
            "Epoch 28/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9546 - mae: 1.9051\n",
            "Epoch 00028: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 1.9546 - mae: 1.9051 - val_loss: 2.0898 - val_mae: 1.9283 - lr: 7.8125e-06\n",
            "Epoch 29/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9330 - mae: 1.8887\n",
            "Epoch 00029: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9330 - mae: 1.8887 - val_loss: 2.0872 - val_mae: 2.0632 - lr: 3.9063e-06\n",
            "Epoch 30/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9283 - mae: 1.8889\n",
            "Epoch 00030: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9283 - mae: 1.8889 - val_loss: 2.0856 - val_mae: 2.0390 - lr: 3.9063e-06\n",
            "Epoch 31/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9311 - mae: 1.8812\n",
            "Epoch 00031: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 1.9311 - mae: 1.8812 - val_loss: 2.1537 - val_mae: 2.0045 - lr: 1.9531e-06\n",
            "Epoch 32/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9340 - mae: 1.8700\n",
            "Epoch 00032: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 1.9340 - mae: 1.8700 - val_loss: 2.1509 - val_mae: 2.0702 - lr: 1.9531e-06\n",
            "Epoch 33/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9339 - mae: 1.8428\n",
            "Epoch 00033: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9339 - mae: 1.8428 - val_loss: 2.0581 - val_mae: 1.9573 - lr: 9.7656e-07\n",
            "Epoch 34/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9332 - mae: 1.8880\n",
            "Epoch 00034: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 1.9332 - mae: 1.8880 - val_loss: 2.1030 - val_mae: 1.9265 - lr: 9.7656e-07\n",
            "Epoch 35/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9188 - mae: 1.8418\n",
            "Epoch 00035: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 1.9188 - mae: 1.8418 - val_loss: 2.1128 - val_mae: 1.9875 - lr: 4.8828e-07\n",
            "Epoch 36/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9396 - mae: 1.8811\n",
            "Epoch 00036: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9396 - mae: 1.8811 - val_loss: 2.0923 - val_mae: 1.9645 - lr: 4.8828e-07\n",
            "Epoch 37/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9302 - mae: 1.8844\n",
            "Epoch 00037: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9302 - mae: 1.8844 - val_loss: 2.1059 - val_mae: 2.0626 - lr: 2.4414e-07\n",
            "Epoch 38/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9136 - mae: 1.8525\n",
            "Epoch 00038: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 1.9136 - mae: 1.8525 - val_loss: 2.0911 - val_mae: 2.0216 - lr: 2.4414e-07\n",
            "Epoch 39/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9275 - mae: 1.8675\n",
            "Epoch 00039: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9275 - mae: 1.8675 - val_loss: 2.1204 - val_mae: 1.9747 - lr: 1.2207e-07\n",
            "Epoch 40/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9298 - mae: 1.8642\n",
            "Epoch 00040: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 1.9298 - mae: 1.8642 - val_loss: 2.1144 - val_mae: 1.9887 - lr: 1.2207e-07\n",
            "Epoch 41/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9271 - mae: 1.8555\n",
            "Epoch 00041: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 1.9271 - mae: 1.8555 - val_loss: 2.0827 - val_mae: 2.0137 - lr: 6.1035e-08\n",
            "Epoch 42/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9271 - mae: 1.8728\n",
            "Epoch 00042: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9271 - mae: 1.8728 - val_loss: 2.0645 - val_mae: 1.9620 - lr: 6.1035e-08\n",
            "Epoch 43/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9305 - mae: 1.8795\n",
            "Epoch 00043: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9305 - mae: 1.8795 - val_loss: 2.1836 - val_mae: 2.0016 - lr: 3.0518e-08\n",
            "Epoch 44/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9297 - mae: 1.8675\n",
            "Epoch 00044: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 1.9297 - mae: 1.8675 - val_loss: 2.1279 - val_mae: 2.0882 - lr: 3.0518e-08\n",
            "Epoch 45/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9305 - mae: 1.8674\n",
            "Epoch 00045: val_loss did not improve from 2.01225\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 1.9305 - mae: 1.8674 - val_loss: 2.1025 - val_mae: 2.0334 - lr: 1.5259e-08\n",
            "Epoch 46/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9286 - mae: 1.8946Restoring model weights from the end of the best epoch: 16.\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 2.01225\n",
            "\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 1.9286 - mae: 1.8946 - val_loss: 2.1006 - val_mae: 2.0425 - lr: 1.5259e-08\n",
            "Epoch 00046: early stopping\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 2.5503 - mae: 2.7418\n",
            "Fit model on training data fold  5\n",
            "Epoch 1/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 24.1554 - mae: 31.1945\n",
            "Epoch 00001: val_loss improved from inf to 3.45444, saving model to /home/jupyter/BAMI/fold5.h5\n",
            "241/241 [==============================] - 36s 111ms/step - loss: 24.1554 - mae: 31.1945 - val_loss: 3.4544 - val_mae: 9.6259 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.4567 - mae: 9.5171\n",
            "Epoch 00002: val_loss improved from 3.45444 to 3.19293, saving model to /home/jupyter/BAMI/fold5.h5\n",
            "241/241 [==============================] - 21s 89ms/step - loss: 3.4567 - mae: 9.5171 - val_loss: 3.1929 - val_mae: 7.3329 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.1773 - mae: 6.9379\n",
            "Epoch 00003: val_loss did not improve from 3.19293\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 3.1773 - mae: 6.9379 - val_loss: 3.3134 - val_mae: 6.4480 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.0572 - mae: 5.8244\n",
            "Epoch 00004: val_loss did not improve from 3.19293\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 3.0572 - mae: 5.8244 - val_loss: 3.2329 - val_mae: 5.7914 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.7093 - mae: 4.2990\n",
            "Epoch 00005: val_loss improved from 3.19293 to 2.66430, saving model to /home/jupyter/BAMI/fold5.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.7093 - mae: 4.2990 - val_loss: 2.6643 - val_mae: 4.0140 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.6415 - mae: 3.7576\n",
            "Epoch 00006: val_loss did not improve from 2.66430\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.6415 - mae: 3.7576 - val_loss: 2.7083 - val_mae: 3.9010 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.5699 - mae: 3.5462\n",
            "Epoch 00007: val_loss did not improve from 2.66430\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.5699 - mae: 3.5462 - val_loss: 2.6986 - val_mae: 3.8448 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.3745 - mae: 2.9680\n",
            "Epoch 00008: val_loss improved from 2.66430 to 2.51802, saving model to /home/jupyter/BAMI/fold5.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.3745 - mae: 2.9680 - val_loss: 2.5180 - val_mae: 3.0424 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.4020 - mae: 2.9175\n",
            "Epoch 00009: val_loss improved from 2.51802 to 2.48598, saving model to /home/jupyter/BAMI/fold5.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.4020 - mae: 2.9175 - val_loss: 2.4860 - val_mae: 3.1007 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.3330 - mae: 2.7724\n",
            "Epoch 00010: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.3330 - mae: 2.7724 - val_loss: 2.5987 - val_mae: 3.1429 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2768 - mae: 2.6309\n",
            "Epoch 00011: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.2768 - mae: 2.6309 - val_loss: 2.5952 - val_mae: 3.0286 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2085 - mae: 2.4819\n",
            "Epoch 00012: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.2085 - mae: 2.4819 - val_loss: 2.5334 - val_mae: 2.8443 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1781 - mae: 2.3779\n",
            "Epoch 00013: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1781 - mae: 2.3779 - val_loss: 2.5145 - val_mae: 2.7623 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1402 - mae: 2.2927\n",
            "Epoch 00014: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1402 - mae: 2.2927 - val_loss: 2.5551 - val_mae: 2.8222 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1216 - mae: 2.1959\n",
            "Epoch 00015: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1216 - mae: 2.1959 - val_loss: 2.5885 - val_mae: 2.8586 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1030 - mae: 2.2489\n",
            "Epoch 00016: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1030 - mae: 2.2489 - val_loss: 2.5845 - val_mae: 2.8698 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0906 - mae: 2.1676\n",
            "Epoch 00017: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0906 - mae: 2.1676 - val_loss: 2.6242 - val_mae: 2.7136 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0810 - mae: 2.1531\n",
            "Epoch 00018: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0810 - mae: 2.1531 - val_loss: 2.6255 - val_mae: 2.9003 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0687 - mae: 2.1443\n",
            "Epoch 00019: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0687 - mae: 2.1443 - val_loss: 2.6725 - val_mae: 2.8776 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0707 - mae: 2.1460\n",
            "Epoch 00020: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0707 - mae: 2.1460 - val_loss: 2.6334 - val_mae: 2.7665 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0692 - mae: 2.1211\n",
            "Epoch 00021: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0692 - mae: 2.1211 - val_loss: 2.6439 - val_mae: 2.7956 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0536 - mae: 2.1350\n",
            "Epoch 00022: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0536 - mae: 2.1350 - val_loss: 2.6490 - val_mae: 2.7887 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0581 - mae: 2.1086\n",
            "Epoch 00023: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0581 - mae: 2.1086 - val_loss: 2.6611 - val_mae: 2.8309 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0587 - mae: 2.1355\n",
            "Epoch 00024: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0587 - mae: 2.1355 - val_loss: 2.5900 - val_mae: 2.8704 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0580 - mae: 2.1240\n",
            "Epoch 00025: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0580 - mae: 2.1240 - val_loss: 2.6098 - val_mae: 2.7072 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0478 - mae: 2.1083\n",
            "Epoch 00026: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0478 - mae: 2.1083 - val_loss: 2.6681 - val_mae: 2.8044 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0479 - mae: 2.1080\n",
            "Epoch 00027: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0479 - mae: 2.1080 - val_loss: 2.7155 - val_mae: 2.9110 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0608 - mae: 2.1233\n",
            "Epoch 00028: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0608 - mae: 2.1233 - val_loss: 2.6184 - val_mae: 2.8395 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0577 - mae: 2.1054\n",
            "Epoch 00029: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0577 - mae: 2.1054 - val_loss: 2.6215 - val_mae: 2.8451 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0587 - mae: 2.1184\n",
            "Epoch 00030: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0587 - mae: 2.1184 - val_loss: 2.6502 - val_mae: 2.6655 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0527 - mae: 2.0755\n",
            "Epoch 00031: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0527 - mae: 2.0755 - val_loss: 2.6327 - val_mae: 2.7889 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0493 - mae: 2.1150\n",
            "Epoch 00032: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0493 - mae: 2.1150 - val_loss: 2.6214 - val_mae: 2.7723 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0503 - mae: 2.1005\n",
            "Epoch 00033: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0503 - mae: 2.1005 - val_loss: 2.6162 - val_mae: 2.7859 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0588 - mae: 2.1220\n",
            "Epoch 00034: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0588 - mae: 2.1220 - val_loss: 2.6201 - val_mae: 2.8924 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0595 - mae: 2.1011\n",
            "Epoch 00035: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0595 - mae: 2.1011 - val_loss: 2.6450 - val_mae: 2.7022 - lr: 6.1035e-08\n",
            "Epoch 36/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0585 - mae: 2.1275\n",
            "Epoch 00036: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0585 - mae: 2.1275 - val_loss: 2.5885 - val_mae: 2.6680 - lr: 3.0518e-08\n",
            "Epoch 37/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0506 - mae: 2.0859\n",
            "Epoch 00037: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0506 - mae: 2.0859 - val_loss: 2.6601 - val_mae: 2.7715 - lr: 3.0518e-08\n",
            "Epoch 38/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0560 - mae: 2.1140\n",
            "Epoch 00038: val_loss did not improve from 2.48598\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0560 - mae: 2.1140 - val_loss: 2.6684 - val_mae: 2.7513 - lr: 1.5259e-08\n",
            "Epoch 39/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0537 - mae: 2.1013Restoring model weights from the end of the best epoch: 9.\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 2.48598\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0537 - mae: 2.1013 - val_loss: 2.6221 - val_mae: 2.7525 - lr: 1.5259e-08\n",
            "Epoch 00039: early stopping\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 2.1719 - mae: 2.6752\n",
            "Fit model on training data fold  6\n",
            "Epoch 1/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 17.6441 - mae: 25.3324\n",
            "Epoch 00001: val_loss improved from inf to 4.07418, saving model to /home/jupyter/BAMI/fold6.h5\n",
            "245/245 [==============================] - 34s 102ms/step - loss: 17.6441 - mae: 25.3324 - val_loss: 4.0742 - val_mae: 12.7871 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 3.6442 - mae: 10.3021\n",
            "Epoch 00002: val_loss improved from 4.07418 to 3.26102, saving model to /home/jupyter/BAMI/fold6.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 3.6442 - mae: 10.3021 - val_loss: 3.2610 - val_mae: 8.3073 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 3.3226 - mae: 7.9084\n",
            "Epoch 00003: val_loss did not improve from 3.26102\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 3.3227 - mae: 7.9081 - val_loss: 4.1040 - val_mae: 9.8115 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 3.1638 - mae: 6.6043\n",
            "Epoch 00004: val_loss improved from 3.26102 to 3.12652, saving model to /home/jupyter/BAMI/fold6.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 3.1643 - mae: 6.6061 - val_loss: 3.1265 - val_mae: 6.4203 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 3.0600 - mae: 5.8471\n",
            "Epoch 00005: val_loss improved from 3.12652 to 2.99453, saving model to /home/jupyter/BAMI/fold6.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 3.0600 - mae: 5.8471 - val_loss: 2.9945 - val_mae: 5.4550 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.9351 - mae: 5.0905\n",
            "Epoch 00006: val_loss did not improve from 2.99453\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.9350 - mae: 5.0903 - val_loss: 3.3642 - val_mae: 6.0717 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 3.0438 - mae: 5.4318\n",
            "Epoch 00007: val_loss improved from 2.99453 to 2.95903, saving model to /home/jupyter/BAMI/fold6.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 3.0435 - mae: 5.4323 - val_loss: 2.9590 - val_mae: 5.4962 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 3.3378 - mae: 7.1308\n",
            "Epoch 00008: val_loss did not improve from 2.95903\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 3.3376 - mae: 7.1304 - val_loss: 3.3836 - val_mae: 7.0936 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.9495 - mae: 5.2002\n",
            "Epoch 00009: val_loss improved from 2.95903 to 2.79968, saving model to /home/jupyter/BAMI/fold6.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.9493 - mae: 5.2011 - val_loss: 2.7997 - val_mae: 4.4750 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.8236 - mae: 4.5477\n",
            "Epoch 00010: val_loss did not improve from 2.79968\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.8234 - mae: 4.5465 - val_loss: 3.2882 - val_mae: 5.5229 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.7519 - mae: 4.2704\n",
            "Epoch 00011: val_loss did not improve from 2.79968\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.7526 - mae: 4.2720 - val_loss: 2.8498 - val_mae: 4.5191 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.4766 - mae: 3.2502\n",
            "Epoch 00012: val_loss improved from 2.79968 to 2.63246, saving model to /home/jupyter/BAMI/fold6.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.4766 - mae: 3.2502 - val_loss: 2.6325 - val_mae: 3.3208 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.4561 - mae: 3.0881\n",
            "Epoch 00013: val_loss did not improve from 2.63246\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.4560 - mae: 3.0875 - val_loss: 3.0765 - val_mae: 4.0852 - lr: 5.0000e-04\n",
            "Epoch 14/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.4646 - mae: 3.1501\n",
            "Epoch 00014: val_loss did not improve from 2.63246\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.4648 - mae: 3.1495 - val_loss: 3.6371 - val_mae: 5.0285 - lr: 5.0000e-04\n",
            "Epoch 15/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.3415 - mae: 2.8340\n",
            "Epoch 00015: val_loss improved from 2.63246 to 2.57158, saving model to /home/jupyter/BAMI/fold6.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.3424 - mae: 2.8365 - val_loss: 2.5716 - val_mae: 3.1413 - lr: 2.5000e-04\n",
            "Epoch 16/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.3015 - mae: 2.6556\n",
            "Epoch 00016: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.3021 - mae: 2.6562 - val_loss: 3.4054 - val_mae: 4.0516 - lr: 2.5000e-04\n",
            "Epoch 17/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.2806 - mae: 2.6130\n",
            "Epoch 00017: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.2807 - mae: 2.6126 - val_loss: 2.7732 - val_mae: 3.3657 - lr: 2.5000e-04\n",
            "Epoch 18/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.2324 - mae: 2.5593\n",
            "Epoch 00018: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.2322 - mae: 2.5587 - val_loss: 2.9487 - val_mae: 3.2634 - lr: 1.2500e-04\n",
            "Epoch 19/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.2145 - mae: 2.4442\n",
            "Epoch 00019: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.2145 - mae: 2.4445 - val_loss: 2.6437 - val_mae: 2.9522 - lr: 1.2500e-04\n",
            "Epoch 20/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1938 - mae: 2.4282\n",
            "Epoch 00020: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1935 - mae: 2.4287 - val_loss: 2.6353 - val_mae: 2.9408 - lr: 6.2500e-05\n",
            "Epoch 21/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1800 - mae: 2.3545\n",
            "Epoch 00021: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1800 - mae: 2.3553 - val_loss: 2.7324 - val_mae: 3.0825 - lr: 6.2500e-05\n",
            "Epoch 22/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1653 - mae: 2.3361\n",
            "Epoch 00022: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1651 - mae: 2.3360 - val_loss: 2.6263 - val_mae: 2.8063 - lr: 3.1250e-05\n",
            "Epoch 23/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1611 - mae: 2.3223\n",
            "Epoch 00023: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1615 - mae: 2.3223 - val_loss: 2.8606 - val_mae: 3.0483 - lr: 3.1250e-05\n",
            "Epoch 24/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1463 - mae: 2.2687\n",
            "Epoch 00024: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1464 - mae: 2.2697 - val_loss: 2.6723 - val_mae: 2.8341 - lr: 1.5625e-05\n",
            "Epoch 25/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1483 - mae: 2.2989\n",
            "Epoch 00025: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1483 - mae: 2.2990 - val_loss: 2.5886 - val_mae: 2.8740 - lr: 1.5625e-05\n",
            "Epoch 26/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1441 - mae: 2.3354\n",
            "Epoch 00026: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1513 - mae: 2.3453 - val_loss: 2.7222 - val_mae: 3.0446 - lr: 7.8125e-06\n",
            "Epoch 27/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1401 - mae: 2.3118\n",
            "Epoch 00027: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1402 - mae: 2.3110 - val_loss: 2.7902 - val_mae: 3.0324 - lr: 7.8125e-06\n",
            "Epoch 28/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.1412 - mae: 2.2863\n",
            "Epoch 00028: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1412 - mae: 2.2863 - val_loss: 2.8331 - val_mae: 3.0322 - lr: 3.9063e-06\n",
            "Epoch 29/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1305 - mae: 2.2978\n",
            "Epoch 00029: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1314 - mae: 2.2989 - val_loss: 2.6529 - val_mae: 2.8838 - lr: 3.9063e-06\n",
            "Epoch 30/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1363 - mae: 2.2998\n",
            "Epoch 00030: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1361 - mae: 2.2997 - val_loss: 2.7457 - val_mae: 2.9832 - lr: 1.9531e-06\n",
            "Epoch 31/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.1429 - mae: 2.2906\n",
            "Epoch 00031: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1429 - mae: 2.2906 - val_loss: 2.6434 - val_mae: 2.8404 - lr: 1.9531e-06\n",
            "Epoch 32/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1376 - mae: 2.2732\n",
            "Epoch 00032: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1374 - mae: 2.2732 - val_loss: 2.6416 - val_mae: 2.9075 - lr: 9.7656e-07\n",
            "Epoch 33/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1333 - mae: 2.2693\n",
            "Epoch 00033: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1337 - mae: 2.2696 - val_loss: 2.7142 - val_mae: 2.9302 - lr: 9.7656e-07\n",
            "Epoch 34/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1362 - mae: 2.3114\n",
            "Epoch 00034: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1362 - mae: 2.3116 - val_loss: 2.8186 - val_mae: 2.9747 - lr: 4.8828e-07\n",
            "Epoch 35/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1268 - mae: 2.2713\n",
            "Epoch 00035: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1265 - mae: 2.2709 - val_loss: 2.8226 - val_mae: 3.0337 - lr: 4.8828e-07\n",
            "Epoch 36/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1246 - mae: 2.2297\n",
            "Epoch 00036: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1247 - mae: 2.2295 - val_loss: 2.8050 - val_mae: 3.0951 - lr: 2.4414e-07\n",
            "Epoch 37/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1309 - mae: 2.2682\n",
            "Epoch 00037: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1315 - mae: 2.2688 - val_loss: 2.6833 - val_mae: 2.9498 - lr: 2.4414e-07\n",
            "Epoch 38/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1407 - mae: 2.2749\n",
            "Epoch 00038: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1408 - mae: 2.2755 - val_loss: 2.6628 - val_mae: 2.9177 - lr: 1.2207e-07\n",
            "Epoch 39/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1320 - mae: 2.2728\n",
            "Epoch 00039: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1341 - mae: 2.2749 - val_loss: 2.7862 - val_mae: 3.0601 - lr: 1.2207e-07\n",
            "Epoch 40/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1298 - mae: 2.2623\n",
            "Epoch 00040: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1297 - mae: 2.2627 - val_loss: 2.8461 - val_mae: 2.9807 - lr: 6.1035e-08\n",
            "Epoch 41/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1359 - mae: 2.2618\n",
            "Epoch 00041: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1357 - mae: 2.2616 - val_loss: 2.6629 - val_mae: 2.8845 - lr: 6.1035e-08\n",
            "Epoch 42/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1242 - mae: 2.2528\n",
            "Epoch 00042: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1245 - mae: 2.2530 - val_loss: 2.7277 - val_mae: 2.8703 - lr: 3.0518e-08\n",
            "Epoch 43/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1333 - mae: 2.2954\n",
            "Epoch 00043: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1338 - mae: 2.2960 - val_loss: 2.7605 - val_mae: 3.0652 - lr: 3.0518e-08\n",
            "Epoch 44/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1255 - mae: 2.2560\n",
            "Epoch 00044: val_loss did not improve from 2.57158\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1258 - mae: 2.2560 - val_loss: 2.6698 - val_mae: 2.8764 - lr: 1.5259e-08\n",
            "Epoch 45/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1404 - mae: 2.2508Restoring model weights from the end of the best epoch: 15.\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 2.57158\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1404 - mae: 2.2508 - val_loss: 2.7779 - val_mae: 2.9702 - lr: 1.5259e-08\n",
            "Epoch 00045: early stopping\n",
            "10/10 [==============================] - 0s 34ms/step - loss: 2.0845 - mae: 2.3299\n",
            "Fit model on training data fold  7\n",
            "Epoch 1/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 16.4497 - mae: 30.4941\n",
            "Epoch 00001: val_loss improved from inf to 4.10000, saving model to /home/jupyter/BAMI/fold7.h5\n",
            "245/245 [==============================] - 34s 100ms/step - loss: 16.4437 - mae: 30.4847 - val_loss: 4.1000 - val_mae: 14.7458 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 3.8071 - mae: 12.7360\n",
            "Epoch 00002: val_loss did not improve from 4.10000\n",
            "245/245 [==============================] - 20s 83ms/step - loss: 3.8074 - mae: 12.7343 - val_loss: 4.3346 - val_mae: 15.5566 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 3.4529 - mae: 9.5179\n",
            "Epoch 00003: val_loss improved from 4.10000 to 3.40290, saving model to /home/jupyter/BAMI/fold7.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 3.4530 - mae: 9.5194 - val_loss: 3.4029 - val_mae: 8.4388 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 3.2316 - mae: 7.3037\n",
            "Epoch 00004: val_loss did not improve from 3.40290\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 3.2317 - mae: 7.3035 - val_loss: 4.3247 - val_mae: 10.6285 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 3.1206 - mae: 6.3255\n",
            "Epoch 00005: val_loss did not improve from 3.40290\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 3.1207 - mae: 6.3251 - val_loss: 3.8800 - val_mae: 8.2373 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.7934 - mae: 4.7182\n",
            "Epoch 00006: val_loss improved from 3.40290 to 2.87186, saving model to /home/jupyter/BAMI/fold7.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.7935 - mae: 4.7180 - val_loss: 2.8719 - val_mae: 4.5057 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.7291 - mae: 4.1214\n",
            "Epoch 00007: val_loss did not improve from 2.87186\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.7291 - mae: 4.1214 - val_loss: 3.7043 - val_mae: 6.2391 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.6280 - mae: 3.7859\n",
            "Epoch 00008: val_loss did not improve from 2.87186\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.6305 - mae: 3.7909 - val_loss: 5.0052 - val_mae: 8.1263 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.5137 - mae: 3.4513\n",
            "Epoch 00009: val_loss improved from 2.87186 to 2.70265, saving model to /home/jupyter/BAMI/fold7.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.5137 - mae: 3.4504 - val_loss: 2.7027 - val_mae: 3.4289 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.4143 - mae: 3.0415\n",
            "Epoch 00010: val_loss did not improve from 2.70265\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.4148 - mae: 3.0417 - val_loss: 2.9175 - val_mae: 3.7088 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.3846 - mae: 2.9178\n",
            "Epoch 00011: val_loss improved from 2.70265 to 2.54313, saving model to /home/jupyter/BAMI/fold7.h5\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.3846 - mae: 2.9175 - val_loss: 2.5431 - val_mae: 3.0193 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.3393 - mae: 2.7629\n",
            "Epoch 00012: val_loss did not improve from 2.54313\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.3393 - mae: 2.7634 - val_loss: 2.9251 - val_mae: 3.3309 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.3293 - mae: 2.7269\n",
            "Epoch 00013: val_loss did not improve from 2.54313\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.3296 - mae: 2.7274 - val_loss: 2.7628 - val_mae: 3.1503 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.2530 - mae: 2.5761\n",
            "Epoch 00014: val_loss did not improve from 2.54313\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.2527 - mae: 2.5755 - val_loss: 2.6300 - val_mae: 3.0194 - lr: 1.2500e-04\n",
            "Epoch 15/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.2218 - mae: 2.4685\n",
            "Epoch 00015: val_loss improved from 2.54313 to 2.44980, saving model to /home/jupyter/BAMI/fold7.h5\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.2224 - mae: 2.4683 - val_loss: 2.4498 - val_mae: 2.7967 - lr: 1.2500e-04\n",
            "Epoch 16/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.2133 - mae: 2.4378\n",
            "Epoch 00016: val_loss improved from 2.44980 to 2.28458, saving model to /home/jupyter/BAMI/fold7.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.2133 - mae: 2.4378 - val_loss: 2.2846 - val_mae: 2.4545 - lr: 1.2500e-04\n",
            "Epoch 17/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1810 - mae: 2.3639\n",
            "Epoch 00017: val_loss did not improve from 2.28458\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1817 - mae: 2.3656 - val_loss: 2.4271 - val_mae: 2.7379 - lr: 1.2500e-04\n",
            "Epoch 18/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1816 - mae: 2.3448\n",
            "Epoch 00018: val_loss did not improve from 2.28458\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1819 - mae: 2.3453 - val_loss: 2.3504 - val_mae: 2.4294 - lr: 1.2500e-04\n",
            "Epoch 19/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1376 - mae: 2.2822\n",
            "Epoch 00019: val_loss did not improve from 2.28458\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1376 - mae: 2.2819 - val_loss: 2.2959 - val_mae: 2.4540 - lr: 6.2500e-05\n",
            "Epoch 20/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1217 - mae: 2.2176\n",
            "Epoch 00020: val_loss did not improve from 2.28458\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "245/245 [==============================] - 20s 83ms/step - loss: 2.1218 - mae: 2.2170 - val_loss: 2.3685 - val_mae: 2.4199 - lr: 6.2500e-05\n",
            "Epoch 21/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1118 - mae: 2.1951\n",
            "Epoch 00021: val_loss did not improve from 2.28458\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1116 - mae: 2.1951 - val_loss: 2.3246 - val_mae: 2.3763 - lr: 3.1250e-05\n",
            "Epoch 22/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0877 - mae: 2.1785\n",
            "Epoch 00022: val_loss improved from 2.28458 to 2.23037, saving model to /home/jupyter/BAMI/fold7.h5\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0876 - mae: 2.1780 - val_loss: 2.2304 - val_mae: 2.2346 - lr: 3.1250e-05\n",
            "Epoch 23/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.0842 - mae: 2.1405\n",
            "Epoch 00023: val_loss improved from 2.23037 to 2.20112, saving model to /home/jupyter/BAMI/fold7.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0842 - mae: 2.1405 - val_loss: 2.2011 - val_mae: 2.1901 - lr: 3.1250e-05\n",
            "Epoch 24/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0769 - mae: 2.1280\n",
            "Epoch 00024: val_loss did not improve from 2.20112\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0767 - mae: 2.1281 - val_loss: 2.3904 - val_mae: 2.4531 - lr: 3.1250e-05\n",
            "Epoch 25/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0713 - mae: 2.1052\n",
            "Epoch 00025: val_loss did not improve from 2.20112\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0711 - mae: 2.1061 - val_loss: 2.3591 - val_mae: 2.4531 - lr: 3.1250e-05\n",
            "Epoch 26/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.0706 - mae: 2.1135\n",
            "Epoch 00026: val_loss did not improve from 2.20112\n",
            "245/245 [==============================] - 20s 83ms/step - loss: 2.0706 - mae: 2.1135 - val_loss: 2.2290 - val_mae: 2.2501 - lr: 1.5625e-05\n",
            "Epoch 27/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0669 - mae: 2.1081\n",
            "Epoch 00027: val_loss did not improve from 2.20112\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0668 - mae: 2.1081 - val_loss: 2.3109 - val_mae: 2.3439 - lr: 1.5625e-05\n",
            "Epoch 28/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0570 - mae: 2.0658\n",
            "Epoch 00028: val_loss did not improve from 2.20112\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0569 - mae: 2.0652 - val_loss: 2.4330 - val_mae: 2.4761 - lr: 7.8125e-06\n",
            "Epoch 29/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0582 - mae: 2.0712\n",
            "Epoch 00029: val_loss did not improve from 2.20112\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "245/245 [==============================] - 20s 84ms/step - loss: 2.0606 - mae: 2.0738 - val_loss: 2.4172 - val_mae: 2.5060 - lr: 7.8125e-06\n",
            "Epoch 30/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.0506 - mae: 2.0983\n",
            "Epoch 00030: val_loss improved from 2.20112 to 2.19091, saving model to /home/jupyter/BAMI/fold7.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.0506 - mae: 2.0983 - val_loss: 2.1909 - val_mae: 2.1881 - lr: 3.9063e-06\n",
            "Epoch 31/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0448 - mae: 2.0750\n",
            "Epoch 00031: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0448 - mae: 2.0747 - val_loss: 2.3622 - val_mae: 2.3567 - lr: 3.9063e-06\n",
            "Epoch 32/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0503 - mae: 2.0697\n",
            "Epoch 00032: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "245/245 [==============================] - 20s 83ms/step - loss: 2.0506 - mae: 2.0694 - val_loss: 2.2788 - val_mae: 2.3270 - lr: 3.9063e-06\n",
            "Epoch 33/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0480 - mae: 2.0478\n",
            "Epoch 00033: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0479 - mae: 2.0476 - val_loss: 2.3352 - val_mae: 2.3718 - lr: 1.9531e-06\n",
            "Epoch 34/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0409 - mae: 2.0676\n",
            "Epoch 00034: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0407 - mae: 2.0674 - val_loss: 2.3689 - val_mae: 2.4566 - lr: 1.9531e-06\n",
            "Epoch 35/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0497 - mae: 2.0693\n",
            "Epoch 00035: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 20s 84ms/step - loss: 2.0499 - mae: 2.0698 - val_loss: 2.5396 - val_mae: 2.6250 - lr: 9.7656e-07\n",
            "Epoch 36/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0503 - mae: 2.0456\n",
            "Epoch 00036: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "245/245 [==============================] - 20s 83ms/step - loss: 2.0529 - mae: 2.0479 - val_loss: 2.4029 - val_mae: 2.3535 - lr: 9.7656e-07\n",
            "Epoch 37/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0479 - mae: 2.0605\n",
            "Epoch 00037: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 20s 84ms/step - loss: 2.0482 - mae: 2.0610 - val_loss: 2.3943 - val_mae: 2.4084 - lr: 4.8828e-07\n",
            "Epoch 38/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0530 - mae: 2.0923\n",
            "Epoch 00038: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0528 - mae: 2.0928 - val_loss: 2.3782 - val_mae: 2.4246 - lr: 4.8828e-07\n",
            "Epoch 39/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0495 - mae: 2.0616\n",
            "Epoch 00039: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0494 - mae: 2.0613 - val_loss: 2.3548 - val_mae: 2.3399 - lr: 2.4414e-07\n",
            "Epoch 40/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0436 - mae: 2.0564\n",
            "Epoch 00040: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0438 - mae: 2.0564 - val_loss: 2.2708 - val_mae: 2.3639 - lr: 2.4414e-07\n",
            "Epoch 41/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.0443 - mae: 2.0503\n",
            "Epoch 00041: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0443 - mae: 2.0503 - val_loss: 2.3936 - val_mae: 2.4155 - lr: 1.2207e-07\n",
            "Epoch 42/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0410 - mae: 2.0700\n",
            "Epoch 00042: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0409 - mae: 2.0700 - val_loss: 2.3079 - val_mae: 2.2339 - lr: 1.2207e-07\n",
            "Epoch 43/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0423 - mae: 2.0817\n",
            "Epoch 00043: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 20s 84ms/step - loss: 2.0435 - mae: 2.0824 - val_loss: 2.3716 - val_mae: 2.4096 - lr: 6.1035e-08\n",
            "Epoch 44/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0411 - mae: 2.0537\n",
            "Epoch 00044: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0410 - mae: 2.0541 - val_loss: 2.3539 - val_mae: 2.3321 - lr: 6.1035e-08\n",
            "Epoch 45/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0466 - mae: 2.0611\n",
            "Epoch 00045: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0471 - mae: 2.0614 - val_loss: 2.4032 - val_mae: 2.4888 - lr: 3.0518e-08\n",
            "Epoch 46/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0484 - mae: 2.0717\n",
            "Epoch 00046: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "245/245 [==============================] - 20s 84ms/step - loss: 2.0483 - mae: 2.0719 - val_loss: 2.2855 - val_mae: 2.2658 - lr: 3.0518e-08\n",
            "Epoch 47/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0395 - mae: 2.0673\n",
            "Epoch 00047: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 20s 83ms/step - loss: 2.0402 - mae: 2.0680 - val_loss: 2.5549 - val_mae: 2.4973 - lr: 1.5259e-08\n",
            "Epoch 48/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0437 - mae: 2.0519\n",
            "Epoch 00048: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00048: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0441 - mae: 2.0530 - val_loss: 2.3707 - val_mae: 2.4057 - lr: 1.5259e-08\n",
            "Epoch 49/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0407 - mae: 2.0698\n",
            "Epoch 00049: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0412 - mae: 2.0699 - val_loss: 2.3346 - val_mae: 2.4137 - lr: 7.6294e-09\n",
            "Epoch 50/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0416 - mae: 2.0733\n",
            "Epoch 00050: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0415 - mae: 2.0731 - val_loss: 2.3565 - val_mae: 2.3818 - lr: 7.6294e-09\n",
            "Epoch 51/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0360 - mae: 2.0651\n",
            "Epoch 00051: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0369 - mae: 2.0659 - val_loss: 2.3675 - val_mae: 2.4005 - lr: 3.8147e-09\n",
            "Epoch 52/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0423 - mae: 2.0750\n",
            "Epoch 00052: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0421 - mae: 2.0749 - val_loss: 2.3137 - val_mae: 2.2832 - lr: 3.8147e-09\n",
            "Epoch 53/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0406 - mae: 2.0622\n",
            "Epoch 00053: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0407 - mae: 2.0622 - val_loss: 2.4173 - val_mae: 2.3728 - lr: 1.9073e-09\n",
            "Epoch 54/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0405 - mae: 2.0891\n",
            "Epoch 00054: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00054: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
            "245/245 [==============================] - 20s 84ms/step - loss: 2.0404 - mae: 2.0892 - val_loss: 2.3251 - val_mae: 2.2801 - lr: 1.9073e-09\n",
            "Epoch 55/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.0466 - mae: 2.0654\n",
            "Epoch 00055: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 20s 84ms/step - loss: 2.0466 - mae: 2.0654 - val_loss: 2.2061 - val_mae: 2.1736 - lr: 9.5367e-10\n",
            "Epoch 56/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0450 - mae: 2.0395\n",
            "Epoch 00056: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00056: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
            "245/245 [==============================] - 20s 84ms/step - loss: 2.0448 - mae: 2.0399 - val_loss: 2.4104 - val_mae: 2.3507 - lr: 9.5367e-10\n",
            "Epoch 57/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0447 - mae: 2.0778\n",
            "Epoch 00057: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0446 - mae: 2.0780 - val_loss: 2.2355 - val_mae: 2.1996 - lr: 4.7684e-10\n",
            "Epoch 58/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0424 - mae: 2.0565\n",
            "Epoch 00058: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00058: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
            "245/245 [==============================] - 20s 83ms/step - loss: 2.0425 - mae: 2.0569 - val_loss: 2.2768 - val_mae: 2.2446 - lr: 4.7684e-10\n",
            "Epoch 59/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0439 - mae: 2.0608\n",
            "Epoch 00059: val_loss did not improve from 2.19091\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0449 - mae: 2.0618 - val_loss: 2.3245 - val_mae: 2.2523 - lr: 2.3842e-10\n",
            "Epoch 60/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0399 - mae: 2.0588Restoring model weights from the end of the best epoch: 30.\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 2.19091\n",
            "\n",
            "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0403 - mae: 2.0590 - val_loss: 2.3648 - val_mae: 2.4014 - lr: 2.3842e-10\n",
            "Epoch 00060: early stopping\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 2.4158 - mae: 2.4688\n",
            "Fit model on training data fold  8\n",
            "Epoch 1/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 15.6700 - mae: 23.2841\n",
            "Epoch 00001: val_loss improved from inf to 3.90408, saving model to /home/jupyter/BAMI/fold8.h5\n",
            "245/245 [==============================] - 34s 102ms/step - loss: 15.6700 - mae: 23.2841 - val_loss: 3.9041 - val_mae: 11.8671 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 3.4195 - mae: 8.8513\n",
            "Epoch 00002: val_loss improved from 3.90408 to 3.22504, saving model to /home/jupyter/BAMI/fold8.h5\n",
            "245/245 [==============================] - 22s 91ms/step - loss: 3.4195 - mae: 8.8486 - val_loss: 3.2250 - val_mae: 7.3970 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 3.2131 - mae: 7.2214\n",
            "Epoch 00003: val_loss did not improve from 3.22504\n",
            "245/245 [==============================] - 22s 89ms/step - loss: 3.2131 - mae: 7.2214 - val_loss: 3.4189 - val_mae: 8.0130 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 3.0477 - mae: 5.9048\n",
            "Epoch 00004: val_loss did not improve from 3.22504\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "245/245 [==============================] - 21s 87ms/step - loss: 3.0478 - mae: 5.9063 - val_loss: 4.3759 - val_mae: 9.3493 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.7730 - mae: 4.5996\n",
            "Epoch 00005: val_loss improved from 3.22504 to 2.79570, saving model to /home/jupyter/BAMI/fold8.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.7733 - mae: 4.6006 - val_loss: 2.7957 - val_mae: 4.4511 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.6883 - mae: 3.9724\n",
            "Epoch 00006: val_loss did not improve from 2.79570\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.6883 - mae: 3.9726 - val_loss: 3.2993 - val_mae: 5.3102 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.7405 - mae: 4.1293\n",
            "Epoch 00007: val_loss improved from 2.79570 to 2.50296, saving model to /home/jupyter/BAMI/fold8.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.7403 - mae: 4.1298 - val_loss: 2.5030 - val_mae: 3.6483 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.5978 - mae: 3.6010\n",
            "Epoch 00008: val_loss did not improve from 2.50296\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.5978 - mae: 3.6010 - val_loss: 2.5066 - val_mae: 3.5702 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.5593 - mae: 3.4796\n",
            "Epoch 00009: val_loss did not improve from 2.50296\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.5600 - mae: 3.4806 - val_loss: 2.9884 - val_mae: 4.2309 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.3697 - mae: 2.9406\n",
            "Epoch 00010: val_loss improved from 2.50296 to 2.28949, saving model to /home/jupyter/BAMI/fold8.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.3697 - mae: 2.9406 - val_loss: 2.2895 - val_mae: 2.7350 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.3357 - mae: 2.7402\n",
            "Epoch 00011: val_loss improved from 2.28949 to 2.28631, saving model to /home/jupyter/BAMI/fold8.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.3361 - mae: 2.7416 - val_loss: 2.2863 - val_mae: 2.5530 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.3016 - mae: 2.6868\n",
            "Epoch 00012: val_loss did not improve from 2.28631\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.3017 - mae: 2.6869 - val_loss: 2.5685 - val_mae: 3.2633 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.2902 - mae: 2.6286\n",
            "Epoch 00013: val_loss did not improve from 2.28631\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.2902 - mae: 2.6286 - val_loss: 2.3995 - val_mae: 2.8698 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1975 - mae: 2.4705\n",
            "Epoch 00014: val_loss improved from 2.28631 to 2.28256, saving model to /home/jupyter/BAMI/fold8.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1974 - mae: 2.4699 - val_loss: 2.2826 - val_mae: 2.4440 - lr: 1.2500e-04\n",
            "Epoch 15/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.1769 - mae: 2.3528\n",
            "Epoch 00015: val_loss did not improve from 2.28256\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1769 - mae: 2.3528 - val_loss: 2.4754 - val_mae: 2.7101 - lr: 1.2500e-04\n",
            "Epoch 16/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1818 - mae: 2.3961\n",
            "Epoch 00016: val_loss improved from 2.28256 to 2.25621, saving model to /home/jupyter/BAMI/fold8.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1817 - mae: 2.3961 - val_loss: 2.2562 - val_mae: 2.3809 - lr: 1.2500e-04\n",
            "Epoch 17/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1467 - mae: 2.3199\n",
            "Epoch 00017: val_loss did not improve from 2.25621\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.1470 - mae: 2.3206 - val_loss: 2.3464 - val_mae: 2.6898 - lr: 1.2500e-04\n",
            "Epoch 18/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1325 - mae: 2.2825\n",
            "Epoch 00018: val_loss improved from 2.25621 to 2.23800, saving model to /home/jupyter/BAMI/fold8.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1324 - mae: 2.2821 - val_loss: 2.2380 - val_mae: 2.4287 - lr: 1.2500e-04\n",
            "Epoch 19/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.1133 - mae: 2.2526\n",
            "Epoch 00019: val_loss did not improve from 2.23800\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1133 - mae: 2.2526 - val_loss: 2.4182 - val_mae: 2.4940 - lr: 1.2500e-04\n",
            "Epoch 20/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1052 - mae: 2.2034\n",
            "Epoch 00020: val_loss did not improve from 2.23800\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1154 - mae: 2.2129 - val_loss: 2.5464 - val_mae: 2.8361 - lr: 1.2500e-04\n",
            "Epoch 21/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0715 - mae: 2.2150\n",
            "Epoch 00021: val_loss improved from 2.23800 to 2.23204, saving model to /home/jupyter/BAMI/fold8.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.0714 - mae: 2.2150 - val_loss: 2.2320 - val_mae: 2.3270 - lr: 6.2500e-05\n",
            "Epoch 22/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0507 - mae: 2.0569\n",
            "Epoch 00022: val_loss improved from 2.23204 to 2.21781, saving model to /home/jupyter/BAMI/fold8.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0507 - mae: 2.0563 - val_loss: 2.2178 - val_mae: 2.2721 - lr: 6.2500e-05\n",
            "Epoch 23/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0281 - mae: 2.0299\n",
            "Epoch 00023: val_loss did not improve from 2.21781\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.0282 - mae: 2.0299 - val_loss: 2.2573 - val_mae: 2.2789 - lr: 6.2500e-05\n",
            "Epoch 24/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0287 - mae: 2.0456\n",
            "Epoch 00024: val_loss improved from 2.21781 to 2.20454, saving model to /home/jupyter/BAMI/fold8.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0299 - mae: 2.0469 - val_loss: 2.2045 - val_mae: 2.2477 - lr: 6.2500e-05\n",
            "Epoch 25/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0177 - mae: 2.0290\n",
            "Epoch 00025: val_loss did not improve from 2.20454\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0175 - mae: 2.0294 - val_loss: 2.2262 - val_mae: 2.2293 - lr: 6.2500e-05\n",
            "Epoch 26/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0065 - mae: 2.0462\n",
            "Epoch 00026: val_loss did not improve from 2.20454\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0066 - mae: 2.0462 - val_loss: 2.3381 - val_mae: 2.2233 - lr: 6.2500e-05\n",
            "Epoch 27/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9873 - mae: 1.9595\n",
            "Epoch 00027: val_loss did not improve from 2.20454\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9871 - mae: 1.9602 - val_loss: 2.2977 - val_mae: 2.2363 - lr: 3.1250e-05\n",
            "Epoch 28/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 1.9702 - mae: 1.9688\n",
            "Epoch 00028: val_loss did not improve from 2.20454\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9702 - mae: 1.9688 - val_loss: 2.2578 - val_mae: 2.1785 - lr: 3.1250e-05\n",
            "Epoch 29/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 1.9567 - mae: 1.9099\n",
            "Epoch 00029: val_loss did not improve from 2.20454\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9567 - mae: 1.9099 - val_loss: 2.2376 - val_mae: 2.1508 - lr: 1.5625e-05\n",
            "Epoch 30/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9504 - mae: 1.9042\n",
            "Epoch 00030: val_loss did not improve from 2.20454\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9501 - mae: 1.9044 - val_loss: 2.2725 - val_mae: 2.1825 - lr: 1.5625e-05\n",
            "Epoch 31/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9525 - mae: 1.8949\n",
            "Epoch 00031: val_loss did not improve from 2.20454\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9524 - mae: 1.8944 - val_loss: 2.3305 - val_mae: 2.3357 - lr: 7.8125e-06\n",
            "Epoch 32/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9460 - mae: 1.8879\n",
            "Epoch 00032: val_loss did not improve from 2.20454\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9469 - mae: 1.8883 - val_loss: 2.2312 - val_mae: 2.1129 - lr: 7.8125e-06\n",
            "Epoch 33/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9405 - mae: 1.8783\n",
            "Epoch 00033: val_loss improved from 2.20454 to 2.20441, saving model to /home/jupyter/BAMI/fold8.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9421 - mae: 1.8803 - val_loss: 2.2044 - val_mae: 2.1508 - lr: 3.9063e-06\n",
            "Epoch 34/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9387 - mae: 1.8782\n",
            "Epoch 00034: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 20s 84ms/step - loss: 1.9385 - mae: 1.8782 - val_loss: 2.2450 - val_mae: 2.0319 - lr: 3.9063e-06\n",
            "Epoch 35/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9420 - mae: 1.8691\n",
            "Epoch 00035: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9422 - mae: 1.8692 - val_loss: 2.2913 - val_mae: 2.2178 - lr: 3.9063e-06\n",
            "Epoch 36/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 1.9411 - mae: 1.8725\n",
            "Epoch 00036: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9411 - mae: 1.8725 - val_loss: 2.2866 - val_mae: 2.1235 - lr: 1.9531e-06\n",
            "Epoch 37/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9305 - mae: 1.8504\n",
            "Epoch 00037: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 1.9308 - mae: 1.8503 - val_loss: 2.2547 - val_mae: 2.0713 - lr: 1.9531e-06\n",
            "Epoch 38/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9388 - mae: 1.8394\n",
            "Epoch 00038: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 1.9386 - mae: 1.8398 - val_loss: 2.2728 - val_mae: 2.1617 - lr: 9.7656e-07\n",
            "Epoch 39/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9296 - mae: 1.8651\n",
            "Epoch 00039: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9298 - mae: 1.8652 - val_loss: 2.2440 - val_mae: 2.1747 - lr: 9.7656e-07\n",
            "Epoch 40/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 1.9364 - mae: 1.8699\n",
            "Epoch 00040: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9364 - mae: 1.8699 - val_loss: 2.2460 - val_mae: 2.1398 - lr: 4.8828e-07\n",
            "Epoch 41/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9425 - mae: 1.8712\n",
            "Epoch 00041: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9441 - mae: 1.8731 - val_loss: 2.2875 - val_mae: 2.2034 - lr: 4.8828e-07\n",
            "Epoch 42/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9291 - mae: 1.8682\n",
            "Epoch 00042: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 1.9297 - mae: 1.8687 - val_loss: 2.2802 - val_mae: 2.2785 - lr: 2.4414e-07\n",
            "Epoch 43/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9267 - mae: 1.8466\n",
            "Epoch 00043: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 1.9270 - mae: 1.8468 - val_loss: 2.2999 - val_mae: 2.1995 - lr: 2.4414e-07\n",
            "Epoch 44/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9386 - mae: 1.8489\n",
            "Epoch 00044: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 1.9388 - mae: 1.8493 - val_loss: 2.3138 - val_mae: 2.2486 - lr: 1.2207e-07\n",
            "Epoch 45/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 1.9365 - mae: 1.8847\n",
            "Epoch 00045: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 1.9365 - mae: 1.8847 - val_loss: 2.2311 - val_mae: 2.2323 - lr: 1.2207e-07\n",
            "Epoch 46/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9287 - mae: 1.8684\n",
            "Epoch 00046: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 1.9309 - mae: 1.8705 - val_loss: 2.2737 - val_mae: 2.2194 - lr: 6.1035e-08\n",
            "Epoch 47/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9343 - mae: 1.8602\n",
            "Epoch 00047: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 1.9343 - mae: 1.8604 - val_loss: 2.2668 - val_mae: 2.1954 - lr: 6.1035e-08\n",
            "Epoch 48/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9397 - mae: 1.8865\n",
            "Epoch 00048: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9396 - mae: 1.8862 - val_loss: 2.2703 - val_mae: 2.1199 - lr: 3.0518e-08\n",
            "Epoch 49/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9319 - mae: 1.8659\n",
            "Epoch 00049: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "245/245 [==============================] - 20s 83ms/step - loss: 1.9344 - mae: 1.8683 - val_loss: 2.2901 - val_mae: 2.1330 - lr: 3.0518e-08\n",
            "Epoch 50/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9293 - mae: 1.8264\n",
            "Epoch 00050: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 1.9297 - mae: 1.8270 - val_loss: 2.3056 - val_mae: 2.1490 - lr: 1.5259e-08\n",
            "Epoch 51/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9273 - mae: 1.8634\n",
            "Epoch 00051: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9276 - mae: 1.8634 - val_loss: 2.2854 - val_mae: 2.0872 - lr: 1.5259e-08\n",
            "Epoch 52/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9253 - mae: 1.8530\n",
            "Epoch 00052: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 1.9251 - mae: 1.8528 - val_loss: 2.2401 - val_mae: 2.0943 - lr: 7.6294e-09\n",
            "Epoch 53/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9332 - mae: 1.8783\n",
            "Epoch 00053: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00053: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9336 - mae: 1.8792 - val_loss: 2.2341 - val_mae: 2.0804 - lr: 7.6294e-09\n",
            "Epoch 54/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9364 - mae: 1.8441\n",
            "Epoch 00054: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9365 - mae: 1.8443 - val_loss: 2.2667 - val_mae: 2.1979 - lr: 3.8147e-09\n",
            "Epoch 55/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 1.9215 - mae: 1.8510\n",
            "Epoch 00055: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9215 - mae: 1.8510 - val_loss: 2.2273 - val_mae: 2.1650 - lr: 3.8147e-09\n",
            "Epoch 56/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 1.9335 - mae: 1.8434\n",
            "Epoch 00056: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9335 - mae: 1.8434 - val_loss: 2.3049 - val_mae: 2.2643 - lr: 1.9073e-09\n",
            "Epoch 57/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9419 - mae: 1.8714\n",
            "Epoch 00057: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9453 - mae: 1.8750 - val_loss: 2.2405 - val_mae: 2.1069 - lr: 1.9073e-09\n",
            "Epoch 58/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 1.9326 - mae: 1.8622\n",
            "Epoch 00058: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9326 - mae: 1.8622 - val_loss: 2.2401 - val_mae: 2.1336 - lr: 9.5367e-10\n",
            "Epoch 59/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9343 - mae: 1.8680\n",
            "Epoch 00059: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00059: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9342 - mae: 1.8678 - val_loss: 2.2685 - val_mae: 2.1764 - lr: 9.5367e-10\n",
            "Epoch 60/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9296 - mae: 1.8383\n",
            "Epoch 00060: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 1.9296 - mae: 1.8385 - val_loss: 2.2889 - val_mae: 2.2367 - lr: 4.7684e-10\n",
            "Epoch 61/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9318 - mae: 1.8865\n",
            "Epoch 00061: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 1.9409 - mae: 1.8960 - val_loss: 2.2676 - val_mae: 2.1266 - lr: 4.7684e-10\n",
            "Epoch 62/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9379 - mae: 1.8726\n",
            "Epoch 00062: val_loss did not improve from 2.20441\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9385 - mae: 1.8731 - val_loss: 2.2569 - val_mae: 2.2545 - lr: 2.3842e-10\n",
            "Epoch 63/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9435 - mae: 1.8703Restoring model weights from the end of the best epoch: 33.\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 2.20441\n",
            "\n",
            "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9504 - mae: 1.8757 - val_loss: 2.2477 - val_mae: 2.1494 - lr: 2.3842e-10\n",
            "Epoch 00063: early stopping\n",
            "14/14 [==============================] - 0s 23ms/step - loss: 2.9889 - mae: 2.8907\n",
            "Fit model on training data fold  9\n",
            "Epoch 1/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 43.4296 - mae: 38.1934\n",
            "Epoch 00001: val_loss improved from inf to 3.84029, saving model to /home/jupyter/BAMI/fold9.h5\n",
            "241/241 [==============================] - 35s 105ms/step - loss: 43.4296 - mae: 38.1934 - val_loss: 3.8403 - val_mae: 9.5835 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.3159 - mae: 7.9646\n",
            "Epoch 00002: val_loss improved from 3.84029 to 3.27317, saving model to /home/jupyter/BAMI/fold9.h5\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 3.3159 - mae: 7.9646 - val_loss: 3.2732 - val_mae: 6.9427 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.1006 - mae: 6.4207\n",
            "Epoch 00003: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 3.1006 - mae: 6.4207 - val_loss: 4.8743 - val_mae: 10.4434 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.0269 - mae: 5.6738\n",
            "Epoch 00004: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 3.0269 - mae: 5.6738 - val_loss: 3.5850 - val_mae: 6.7262 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.6747 - mae: 4.2431\n",
            "Epoch 00005: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.6747 - mae: 4.2431 - val_loss: 3.8880 - val_mae: 6.2850 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.6179 - mae: 3.7658\n",
            "Epoch 00006: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.6179 - mae: 3.7658 - val_loss: 3.4572 - val_mae: 5.1834 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.4006 - mae: 3.1580\n",
            "Epoch 00007: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.4006 - mae: 3.1580 - val_loss: 3.9266 - val_mae: 5.1910 - lr: 2.5000e-04\n",
            "Epoch 8/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.3824 - mae: 2.9435\n",
            "Epoch 00008: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.3824 - mae: 2.9435 - val_loss: 3.9757 - val_mae: 5.1644 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2658 - mae: 2.6622\n",
            "Epoch 00009: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.2658 - mae: 2.6622 - val_loss: 3.6620 - val_mae: 4.4768 - lr: 1.2500e-04\n",
            "Epoch 10/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2439 - mae: 2.5947\n",
            "Epoch 00010: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.2439 - mae: 2.5947 - val_loss: 4.0306 - val_mae: 4.6750 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1812 - mae: 2.4015\n",
            "Epoch 00011: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1812 - mae: 2.4015 - val_loss: 4.4007 - val_mae: 4.9624 - lr: 6.2500e-05\n",
            "Epoch 12/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1747 - mae: 2.3700\n",
            "Epoch 00012: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1747 - mae: 2.3700 - val_loss: 3.8376 - val_mae: 4.2080 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1427 - mae: 2.3301\n",
            "Epoch 00013: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1427 - mae: 2.3301 - val_loss: 4.3111 - val_mae: 4.7850 - lr: 3.1250e-05\n",
            "Epoch 14/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1291 - mae: 2.2967\n",
            "Epoch 00014: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1291 - mae: 2.2967 - val_loss: 4.1136 - val_mae: 4.4823 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1163 - mae: 2.2701\n",
            "Epoch 00015: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1163 - mae: 2.2701 - val_loss: 4.3733 - val_mae: 4.8009 - lr: 1.5625e-05\n",
            "Epoch 16/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1113 - mae: 2.2277\n",
            "Epoch 00016: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1113 - mae: 2.2277 - val_loss: 4.2823 - val_mae: 4.6431 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1094 - mae: 2.2344\n",
            "Epoch 00017: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1094 - mae: 2.2344 - val_loss: 4.2149 - val_mae: 4.5307 - lr: 7.8125e-06\n",
            "Epoch 18/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0989 - mae: 2.1959\n",
            "Epoch 00018: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0989 - mae: 2.1959 - val_loss: 4.1208 - val_mae: 4.3748 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0963 - mae: 2.1992\n",
            "Epoch 00019: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0963 - mae: 2.1992 - val_loss: 4.8493 - val_mae: 5.2008 - lr: 3.9063e-06\n",
            "Epoch 20/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0945 - mae: 2.1861\n",
            "Epoch 00020: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0945 - mae: 2.1861 - val_loss: 4.0578 - val_mae: 4.3365 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0927 - mae: 2.2224\n",
            "Epoch 00021: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0927 - mae: 2.2224 - val_loss: 4.6319 - val_mae: 4.7980 - lr: 1.9531e-06\n",
            "Epoch 22/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0880 - mae: 2.1920\n",
            "Epoch 00022: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0880 - mae: 2.1920 - val_loss: 4.6048 - val_mae: 4.7930 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1009 - mae: 2.2303\n",
            "Epoch 00023: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1009 - mae: 2.2303 - val_loss: 4.3690 - val_mae: 4.6192 - lr: 9.7656e-07\n",
            "Epoch 24/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1023 - mae: 2.1971\n",
            "Epoch 00024: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1023 - mae: 2.1971 - val_loss: 4.2570 - val_mae: 4.5091 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0898 - mae: 2.1601\n",
            "Epoch 00025: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0898 - mae: 2.1601 - val_loss: 4.6109 - val_mae: 4.9224 - lr: 4.8828e-07\n",
            "Epoch 26/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0885 - mae: 2.1810\n",
            "Epoch 00026: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0885 - mae: 2.1810 - val_loss: 3.9291 - val_mae: 4.0148 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0956 - mae: 2.1737\n",
            "Epoch 00027: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0956 - mae: 2.1737 - val_loss: 3.8956 - val_mae: 4.1223 - lr: 2.4414e-07\n",
            "Epoch 28/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0958 - mae: 2.1878\n",
            "Epoch 00028: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0958 - mae: 2.1878 - val_loss: 4.3570 - val_mae: 4.5510 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0935 - mae: 2.1802\n",
            "Epoch 00029: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0935 - mae: 2.1802 - val_loss: 3.9682 - val_mae: 4.2404 - lr: 1.2207e-07\n",
            "Epoch 30/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0915 - mae: 2.1751\n",
            "Epoch 00030: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0915 - mae: 2.1751 - val_loss: 3.9942 - val_mae: 4.2129 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0893 - mae: 2.1673\n",
            "Epoch 00031: val_loss did not improve from 3.27317\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0893 - mae: 2.1673 - val_loss: 4.9910 - val_mae: 5.2107 - lr: 6.1035e-08\n",
            "Epoch 32/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0866 - mae: 2.1393Restoring model weights from the end of the best epoch: 2.\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 3.27317\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0866 - mae: 2.1393 - val_loss: 3.9719 - val_mae: 4.2407 - lr: 6.1035e-08\n",
            "Epoch 00032: early stopping\n",
            "14/14 [==============================] - 0s 23ms/step - loss: 3.8744 - mae: 10.9724\n",
            "Fit model on training data fold  10\n",
            "Epoch 1/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 15.4625 - mae: 20.2161\n",
            "Epoch 00001: val_loss improved from inf to 3.38220, saving model to /home/jupyter/BAMI/fold10.h5\n",
            "241/241 [==============================] - 35s 106ms/step - loss: 15.4625 - mae: 20.2161 - val_loss: 3.3822 - val_mae: 8.3794 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.3128 - mae: 7.5561\n",
            "Epoch 00002: val_loss did not improve from 3.38220\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 3.3128 - mae: 7.5561 - val_loss: 3.4110 - val_mae: 7.6310 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.1366 - mae: 6.5563\n",
            "Epoch 00003: val_loss improved from 3.38220 to 3.31998, saving model to /home/jupyter/BAMI/fold10.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 3.1366 - mae: 6.5563 - val_loss: 3.3200 - val_mae: 6.8747 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.0093 - mae: 5.6666\n",
            "Epoch 00004: val_loss improved from 3.31998 to 3.05514, saving model to /home/jupyter/BAMI/fold10.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 3.0093 - mae: 5.6666 - val_loss: 3.0551 - val_mae: 5.7558 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.9015 - mae: 4.9787\n",
            "Epoch 00005: val_loss improved from 3.05514 to 2.78494, saving model to /home/jupyter/BAMI/fold10.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.9015 - mae: 4.9787 - val_loss: 2.7849 - val_mae: 4.6716 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.8383 - mae: 4.6458\n",
            "Epoch 00006: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.8383 - mae: 4.6458 - val_loss: 3.1808 - val_mae: 5.3883 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.7500 - mae: 4.2732\n",
            "Epoch 00007: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.7500 - mae: 4.2732 - val_loss: 3.0141 - val_mae: 4.6387 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.4532 - mae: 3.2383\n",
            "Epoch 00008: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.4532 - mae: 3.2383 - val_loss: 3.2265 - val_mae: 4.4333 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.4229 - mae: 3.0590\n",
            "Epoch 00009: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.4229 - mae: 3.0590 - val_loss: 3.4294 - val_mae: 4.8578 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2678 - mae: 2.6751\n",
            "Epoch 00010: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.2678 - mae: 2.6751 - val_loss: 3.2881 - val_mae: 4.0516 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2269 - mae: 2.5391\n",
            "Epoch 00011: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.2269 - mae: 2.5391 - val_loss: 3.4143 - val_mae: 4.0660 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1627 - mae: 2.3496\n",
            "Epoch 00012: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1627 - mae: 2.3496 - val_loss: 3.6461 - val_mae: 4.1289 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1517 - mae: 2.3385\n",
            "Epoch 00013: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1517 - mae: 2.3385 - val_loss: 3.3781 - val_mae: 3.8652 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1162 - mae: 2.2310\n",
            "Epoch 00014: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 20s 84ms/step - loss: 2.1162 - mae: 2.2310 - val_loss: 3.6650 - val_mae: 4.1428 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1062 - mae: 2.2070\n",
            "Epoch 00015: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1062 - mae: 2.2070 - val_loss: 3.6114 - val_mae: 3.9116 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0812 - mae: 2.1607\n",
            "Epoch 00016: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0812 - mae: 2.1607 - val_loss: 3.7321 - val_mae: 3.9037 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0743 - mae: 2.1705\n",
            "Epoch 00017: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0743 - mae: 2.1705 - val_loss: 3.8565 - val_mae: 4.0728 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0638 - mae: 2.1591\n",
            "Epoch 00018: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0638 - mae: 2.1591 - val_loss: 3.6248 - val_mae: 3.8204 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0568 - mae: 2.1370\n",
            "Epoch 00019: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0568 - mae: 2.1370 - val_loss: 3.7480 - val_mae: 3.9676 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0477 - mae: 2.1022\n",
            "Epoch 00020: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 20s 84ms/step - loss: 2.0477 - mae: 2.1022 - val_loss: 3.7157 - val_mae: 3.9137 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0471 - mae: 2.0935\n",
            "Epoch 00021: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0471 - mae: 2.0935 - val_loss: 3.6590 - val_mae: 3.7798 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0518 - mae: 2.1183\n",
            "Epoch 00022: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0518 - mae: 2.1183 - val_loss: 3.6522 - val_mae: 3.8302 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0425 - mae: 2.0799\n",
            "Epoch 00023: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0425 - mae: 2.0799 - val_loss: 3.6536 - val_mae: 3.8571 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0441 - mae: 2.0864\n",
            "Epoch 00024: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0441 - mae: 2.0864 - val_loss: 3.7777 - val_mae: 3.8211 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0433 - mae: 2.0693\n",
            "Epoch 00025: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0433 - mae: 2.0693 - val_loss: 3.8210 - val_mae: 3.9297 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0427 - mae: 2.0780\n",
            "Epoch 00026: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0427 - mae: 2.0780 - val_loss: 3.6868 - val_mae: 3.8068 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0414 - mae: 2.0833\n",
            "Epoch 00027: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0414 - mae: 2.0833 - val_loss: 3.7369 - val_mae: 3.8857 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0450 - mae: 2.0719\n",
            "Epoch 00028: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0450 - mae: 2.0719 - val_loss: 3.7632 - val_mae: 3.9307 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0446 - mae: 2.0777\n",
            "Epoch 00029: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0446 - mae: 2.0777 - val_loss: 3.6779 - val_mae: 3.9182 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0413 - mae: 2.0737\n",
            "Epoch 00030: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0413 - mae: 2.0737 - val_loss: 3.6775 - val_mae: 3.6942 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0410 - mae: 2.0906\n",
            "Epoch 00031: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0410 - mae: 2.0906 - val_loss: 3.7490 - val_mae: 3.9466 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0472 - mae: 2.0940\n",
            "Epoch 00032: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 20s 84ms/step - loss: 2.0472 - mae: 2.0940 - val_loss: 3.8656 - val_mae: 3.9179 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0393 - mae: 2.0689\n",
            "Epoch 00033: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0393 - mae: 2.0689 - val_loss: 3.7148 - val_mae: 3.9124 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0393 - mae: 2.0768\n",
            "Epoch 00034: val_loss did not improve from 2.78494\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0393 - mae: 2.0768 - val_loss: 3.6497 - val_mae: 3.7492 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0453 - mae: 2.1033Restoring model weights from the end of the best epoch: 5.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 2.78494\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.0453 - mae: 2.1033 - val_loss: 3.6895 - val_mae: 3.8324 - lr: 6.1035e-08\n",
            "Epoch 00035: early stopping\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 4.9159 - mae: 8.8683\n",
            "Fit model on training data fold  11\n",
            "Epoch 1/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 35.6209 - mae: 35.0517\n",
            "Epoch 00001: val_loss improved from inf to 4.60693, saving model to /home/jupyter/BAMI/fold11.h5\n",
            "241/241 [==============================] - 35s 105ms/step - loss: 35.6209 - mae: 35.0517 - val_loss: 4.6069 - val_mae: 14.8091 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.5258 - mae: 9.3269\n",
            "Epoch 00002: val_loss improved from 4.60693 to 4.16479, saving model to /home/jupyter/BAMI/fold11.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 3.5258 - mae: 9.3269 - val_loss: 4.1648 - val_mae: 11.2661 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.2476 - mae: 7.5094\n",
            "Epoch 00003: val_loss improved from 4.16479 to 3.57282, saving model to /home/jupyter/BAMI/fold11.h5\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 3.2476 - mae: 7.5094 - val_loss: 3.5728 - val_mae: 7.6386 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.0852 - mae: 6.1581\n",
            "Epoch 00004: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 3.0852 - mae: 6.1581 - val_loss: 4.5430 - val_mae: 9.0441 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.9515 - mae: 5.2653\n",
            "Epoch 00005: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.9515 - mae: 5.2653 - val_loss: 3.7964 - val_mae: 7.0721 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.6211 - mae: 3.9281\n",
            "Epoch 00006: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.6211 - mae: 3.9281 - val_loss: 4.3579 - val_mae: 6.9477 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.5490 - mae: 3.4399\n",
            "Epoch 00007: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.5490 - mae: 3.4399 - val_loss: 4.4077 - val_mae: 6.2106 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.3891 - mae: 3.0216\n",
            "Epoch 00008: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.3891 - mae: 3.0216 - val_loss: 4.7842 - val_mae: 6.1483 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.3352 - mae: 2.7504\n",
            "Epoch 00009: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.3352 - mae: 2.7504 - val_loss: 4.3851 - val_mae: 5.7215 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2719 - mae: 2.6260\n",
            "Epoch 00010: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.2719 - mae: 2.6260 - val_loss: 4.6854 - val_mae: 5.5658 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2238 - mae: 2.5318\n",
            "Epoch 00011: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.2238 - mae: 2.5318 - val_loss: 4.6383 - val_mae: 5.4220 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1959 - mae: 2.4226\n",
            "Epoch 00012: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1959 - mae: 2.4226 - val_loss: 4.6189 - val_mae: 5.3885 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1731 - mae: 2.3928\n",
            "Epoch 00013: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1731 - mae: 2.3928 - val_loss: 4.7740 - val_mae: 5.3564 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1571 - mae: 2.3352\n",
            "Epoch 00014: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1571 - mae: 2.3352 - val_loss: 4.9535 - val_mae: 5.6034 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1467 - mae: 2.2901\n",
            "Epoch 00015: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1467 - mae: 2.2901 - val_loss: 5.5132 - val_mae: 6.0318 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1378 - mae: 2.2747\n",
            "Epoch 00016: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1378 - mae: 2.2747 - val_loss: 5.3385 - val_mae: 5.7455 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1311 - mae: 2.2573\n",
            "Epoch 00017: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1311 - mae: 2.2573 - val_loss: 5.4227 - val_mae: 5.9148 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1239 - mae: 2.2449\n",
            "Epoch 00018: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1239 - mae: 2.2449 - val_loss: 5.3032 - val_mae: 5.7919 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1177 - mae: 2.2603\n",
            "Epoch 00019: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1177 - mae: 2.2603 - val_loss: 5.1406 - val_mae: 5.5638 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1196 - mae: 2.2455\n",
            "Epoch 00020: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1196 - mae: 2.2455 - val_loss: 5.2682 - val_mae: 5.7222 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1148 - mae: 2.2085\n",
            "Epoch 00021: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1148 - mae: 2.2085 - val_loss: 5.0862 - val_mae: 5.4848 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1162 - mae: 2.1983\n",
            "Epoch 00022: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1162 - mae: 2.1983 - val_loss: 5.2289 - val_mae: 5.6844 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1059 - mae: 2.2176\n",
            "Epoch 00023: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1059 - mae: 2.2176 - val_loss: 5.5084 - val_mae: 5.8865 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1155 - mae: 2.2513\n",
            "Epoch 00024: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1155 - mae: 2.2513 - val_loss: 5.0100 - val_mae: 5.4279 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1116 - mae: 2.2040\n",
            "Epoch 00025: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1116 - mae: 2.2040 - val_loss: 5.6922 - val_mae: 5.8659 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1218 - mae: 2.1849\n",
            "Epoch 00026: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1218 - mae: 2.1849 - val_loss: 5.6625 - val_mae: 6.0889 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1051 - mae: 2.2112\n",
            "Epoch 00027: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1051 - mae: 2.2112 - val_loss: 5.2754 - val_mae: 5.6415 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1151 - mae: 2.2292\n",
            "Epoch 00028: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1151 - mae: 2.2292 - val_loss: 5.2151 - val_mae: 5.6049 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1130 - mae: 2.1841\n",
            "Epoch 00029: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1130 - mae: 2.1841 - val_loss: 4.8400 - val_mae: 5.2199 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1128 - mae: 2.1731\n",
            "Epoch 00030: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1128 - mae: 2.1731 - val_loss: 5.4675 - val_mae: 5.9042 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1125 - mae: 2.2214\n",
            "Epoch 00031: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1125 - mae: 2.2214 - val_loss: 5.0386 - val_mae: 5.4290 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1017 - mae: 2.2102\n",
            "Epoch 00032: val_loss did not improve from 3.57282\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1017 - mae: 2.2102 - val_loss: 5.1662 - val_mae: 5.5129 - lr: 6.1035e-08\n",
            "Epoch 33/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1039 - mae: 2.1804Restoring model weights from the end of the best epoch: 3.\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 3.57282\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1039 - mae: 2.1804 - val_loss: 5.2505 - val_mae: 5.5721 - lr: 6.1035e-08\n",
            "Epoch 00033: early stopping\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 2.9234 - mae: 5.9692\n",
            "Fit model on training data fold  12\n",
            "Epoch 1/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 28.6790 - mae: 45.0506\n",
            "Epoch 00001: val_loss improved from inf to 4.12035, saving model to /home/jupyter/BAMI/fold12.h5\n",
            "241/241 [==============================] - 35s 104ms/step - loss: 28.6790 - mae: 45.0506 - val_loss: 4.1203 - val_mae: 15.9210 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.8227 - mae: 13.7293\n",
            "Epoch 00002: val_loss improved from 4.12035 to 4.03527, saving model to /home/jupyter/BAMI/fold12.h5\n",
            "241/241 [==============================] - 23s 95ms/step - loss: 3.8227 - mae: 13.7293 - val_loss: 4.0353 - val_mae: 12.9837 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.3585 - mae: 8.5740\n",
            "Epoch 00003: val_loss improved from 4.03527 to 3.63843, saving model to /home/jupyter/BAMI/fold12.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 3.3585 - mae: 8.5740 - val_loss: 3.6384 - val_mae: 8.7656 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.1213 - mae: 6.4873\n",
            "Epoch 00004: val_loss did not improve from 3.63843\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 3.1213 - mae: 6.4873 - val_loss: 3.6449 - val_mae: 7.2114 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.9469 - mae: 5.2882\n",
            "Epoch 00005: val_loss improved from 3.63843 to 3.58566, saving model to /home/jupyter/BAMI/fold12.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.9469 - mae: 5.2882 - val_loss: 3.5857 - val_mae: 6.5383 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.8006 - mae: 4.4733\n",
            "Epoch 00006: val_loss improved from 3.58566 to 3.56337, saving model to /home/jupyter/BAMI/fold12.h5\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.8006 - mae: 4.4733 - val_loss: 3.5634 - val_mae: 5.5928 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.7009 - mae: 4.0393\n",
            "Epoch 00007: val_loss did not improve from 3.56337\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.7009 - mae: 4.0393 - val_loss: 3.6958 - val_mae: 5.8385 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.6393 - mae: 3.7734\n",
            "Epoch 00008: val_loss improved from 3.56337 to 3.28046, saving model to /home/jupyter/BAMI/fold12.h5\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.6393 - mae: 3.7734 - val_loss: 3.2805 - val_mae: 5.0804 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.5862 - mae: 3.5998\n",
            "Epoch 00009: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.5862 - mae: 3.5998 - val_loss: 4.8236 - val_mae: 7.0760 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.5168 - mae: 3.3131\n",
            "Epoch 00010: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.5168 - mae: 3.3131 - val_loss: 3.8779 - val_mae: 5.3423 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2599 - mae: 2.6206\n",
            "Epoch 00011: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.2599 - mae: 2.6206 - val_loss: 3.8098 - val_mae: 4.4632 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2317 - mae: 2.5165\n",
            "Epoch 00012: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.2317 - mae: 2.5165 - val_loss: 3.5106 - val_mae: 4.0899 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1188 - mae: 2.2644\n",
            "Epoch 00013: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1188 - mae: 2.2644 - val_loss: 3.8920 - val_mae: 4.2619 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0956 - mae: 2.1862\n",
            "Epoch 00014: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0956 - mae: 2.1862 - val_loss: 3.9283 - val_mae: 4.2064 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0483 - mae: 2.0891\n",
            "Epoch 00015: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0483 - mae: 2.0891 - val_loss: 3.7898 - val_mae: 3.9184 - lr: 1.2500e-04\n",
            "Epoch 16/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0272 - mae: 2.0496\n",
            "Epoch 00016: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0272 - mae: 2.0496 - val_loss: 3.8806 - val_mae: 3.8521 - lr: 1.2500e-04\n",
            "Epoch 17/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9981 - mae: 1.9912\n",
            "Epoch 00017: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9981 - mae: 1.9912 - val_loss: 3.9871 - val_mae: 4.0118 - lr: 6.2500e-05\n",
            "Epoch 18/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9934 - mae: 1.9622\n",
            "Epoch 00018: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9934 - mae: 1.9622 - val_loss: 4.3605 - val_mae: 4.5407 - lr: 6.2500e-05\n",
            "Epoch 19/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9817 - mae: 1.9650\n",
            "Epoch 00019: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9817 - mae: 1.9650 - val_loss: 4.2633 - val_mae: 4.1604 - lr: 3.1250e-05\n",
            "Epoch 20/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9593 - mae: 1.9176\n",
            "Epoch 00020: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9593 - mae: 1.9176 - val_loss: 4.7031 - val_mae: 4.5499 - lr: 3.1250e-05\n",
            "Epoch 21/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9618 - mae: 1.9314\n",
            "Epoch 00021: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9618 - mae: 1.9314 - val_loss: 4.5117 - val_mae: 4.3662 - lr: 1.5625e-05\n",
            "Epoch 22/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9549 - mae: 1.8981\n",
            "Epoch 00022: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 1.9549 - mae: 1.8981 - val_loss: 4.7657 - val_mae: 4.6295 - lr: 1.5625e-05\n",
            "Epoch 23/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9546 - mae: 1.9102\n",
            "Epoch 00023: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9546 - mae: 1.9102 - val_loss: 4.4551 - val_mae: 4.2941 - lr: 7.8125e-06\n",
            "Epoch 24/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9424 - mae: 1.8998\n",
            "Epoch 00024: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 1.9424 - mae: 1.8998 - val_loss: 4.3769 - val_mae: 4.2314 - lr: 7.8125e-06\n",
            "Epoch 25/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9426 - mae: 1.9062\n",
            "Epoch 00025: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9426 - mae: 1.9062 - val_loss: 4.3772 - val_mae: 4.1249 - lr: 3.9063e-06\n",
            "Epoch 26/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9412 - mae: 1.8600\n",
            "Epoch 00026: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9412 - mae: 1.8600 - val_loss: 4.2523 - val_mae: 4.0756 - lr: 3.9063e-06\n",
            "Epoch 27/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9418 - mae: 1.8705\n",
            "Epoch 00027: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9418 - mae: 1.8705 - val_loss: 4.5674 - val_mae: 4.3292 - lr: 1.9531e-06\n",
            "Epoch 28/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9411 - mae: 1.8717\n",
            "Epoch 00028: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9411 - mae: 1.8717 - val_loss: 5.0153 - val_mae: 4.7756 - lr: 1.9531e-06\n",
            "Epoch 29/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9431 - mae: 1.8759\n",
            "Epoch 00029: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9431 - mae: 1.8759 - val_loss: 4.2848 - val_mae: 4.1632 - lr: 9.7656e-07\n",
            "Epoch 30/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9442 - mae: 1.8730\n",
            "Epoch 00030: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9442 - mae: 1.8730 - val_loss: 4.1253 - val_mae: 3.9685 - lr: 9.7656e-07\n",
            "Epoch 31/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9381 - mae: 1.8886\n",
            "Epoch 00031: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9381 - mae: 1.8886 - val_loss: 4.1107 - val_mae: 3.9393 - lr: 4.8828e-07\n",
            "Epoch 32/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9422 - mae: 1.8920\n",
            "Epoch 00032: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9422 - mae: 1.8920 - val_loss: 4.2899 - val_mae: 4.0636 - lr: 4.8828e-07\n",
            "Epoch 33/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9383 - mae: 1.8880\n",
            "Epoch 00033: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 1.9383 - mae: 1.8880 - val_loss: 4.3507 - val_mae: 4.1348 - lr: 2.4414e-07\n",
            "Epoch 34/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9369 - mae: 1.8611\n",
            "Epoch 00034: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 1.9369 - mae: 1.8611 - val_loss: 4.3419 - val_mae: 4.2472 - lr: 2.4414e-07\n",
            "Epoch 35/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9389 - mae: 1.8907\n",
            "Epoch 00035: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9389 - mae: 1.8907 - val_loss: 4.3755 - val_mae: 4.1812 - lr: 1.2207e-07\n",
            "Epoch 36/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9391 - mae: 1.8939\n",
            "Epoch 00036: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 1.9391 - mae: 1.8939 - val_loss: 4.3109 - val_mae: 4.1664 - lr: 1.2207e-07\n",
            "Epoch 37/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9414 - mae: 1.9252\n",
            "Epoch 00037: val_loss did not improve from 3.28046\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 1.9414 - mae: 1.9252 - val_loss: 4.6607 - val_mae: 4.5494 - lr: 6.1035e-08\n",
            "Epoch 38/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9406 - mae: 1.8779Restoring model weights from the end of the best epoch: 8.\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 3.28046\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 1.9406 - mae: 1.8779 - val_loss: 4.3703 - val_mae: 4.2573 - lr: 6.1035e-08\n",
            "Epoch 00038: early stopping\n",
            "10/10 [==============================] - 0s 34ms/step - loss: 5.5483 - mae: 9.6440\n",
            "Fit model on training data fold  13\n",
            "Epoch 1/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 18.9018 - mae: 32.5606\n",
            "Epoch 00001: val_loss improved from inf to 4.51817, saving model to /home/jupyter/BAMI/fold13.h5\n",
            "241/241 [==============================] - 35s 105ms/step - loss: 18.9018 - mae: 32.5606 - val_loss: 4.5182 - val_mae: 17.9606 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.6809 - mae: 11.7438\n",
            "Epoch 00002: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 3.6809 - mae: 11.7438 - val_loss: 4.9023 - val_mae: 16.5813 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.3352 - mae: 8.1327\n",
            "Epoch 00003: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 3.3352 - mae: 8.1327 - val_loss: 5.5348 - val_mae: 14.6191 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.9048 - mae: 5.2258\n",
            "Epoch 00004: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 2.9048 - mae: 5.2258 - val_loss: 4.6228 - val_mae: 9.4157 - lr: 5.0000e-04\n",
            "Epoch 5/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.7645 - mae: 4.4065\n",
            "Epoch 00005: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 2.7645 - mae: 4.4065 - val_loss: 4.7246 - val_mae: 9.1795 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.5153 - mae: 3.5660\n",
            "Epoch 00006: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 2.5153 - mae: 3.5660 - val_loss: 5.4916 - val_mae: 8.6921 - lr: 2.5000e-04\n",
            "Epoch 7/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.4507 - mae: 3.1974\n",
            "Epoch 00007: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 2.4507 - mae: 3.1974 - val_loss: 5.2921 - val_mae: 7.9507 - lr: 2.5000e-04\n",
            "Epoch 8/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.3346 - mae: 2.8777\n",
            "Epoch 00008: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 2.3346 - mae: 2.8777 - val_loss: 5.4221 - val_mae: 7.9264 - lr: 1.2500e-04\n",
            "Epoch 9/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2994 - mae: 2.7484\n",
            "Epoch 00009: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 2.2994 - mae: 2.7484 - val_loss: 5.0485 - val_mae: 7.3096 - lr: 1.2500e-04\n",
            "Epoch 10/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2271 - mae: 2.5930\n",
            "Epoch 00010: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 2.2271 - mae: 2.5930 - val_loss: 5.6996 - val_mae: 7.8381 - lr: 6.2500e-05\n",
            "Epoch 11/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2081 - mae: 2.5197\n",
            "Epoch 00011: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 2.2081 - mae: 2.5197 - val_loss: 5.6275 - val_mae: 7.3654 - lr: 6.2500e-05\n",
            "Epoch 12/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1732 - mae: 2.4620\n",
            "Epoch 00012: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1732 - mae: 2.4620 - val_loss: 5.7045 - val_mae: 7.4227 - lr: 3.1250e-05\n",
            "Epoch 13/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1670 - mae: 2.4632\n",
            "Epoch 00013: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1670 - mae: 2.4632 - val_loss: 5.4403 - val_mae: 7.0020 - lr: 3.1250e-05\n",
            "Epoch 14/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1461 - mae: 2.4056\n",
            "Epoch 00014: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1461 - mae: 2.4056 - val_loss: 5.6542 - val_mae: 7.1658 - lr: 1.5625e-05\n",
            "Epoch 15/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1479 - mae: 2.4133\n",
            "Epoch 00015: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1479 - mae: 2.4133 - val_loss: 5.6335 - val_mae: 7.2734 - lr: 1.5625e-05\n",
            "Epoch 16/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1331 - mae: 2.3414\n",
            "Epoch 00016: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1331 - mae: 2.3414 - val_loss: 5.7658 - val_mae: 7.1689 - lr: 7.8125e-06\n",
            "Epoch 17/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1212 - mae: 2.3553\n",
            "Epoch 00017: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1212 - mae: 2.3553 - val_loss: 5.8729 - val_mae: 7.2535 - lr: 7.8125e-06\n",
            "Epoch 18/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1227 - mae: 2.3234\n",
            "Epoch 00018: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1227 - mae: 2.3234 - val_loss: 5.8206 - val_mae: 7.2719 - lr: 3.9063e-06\n",
            "Epoch 19/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1096 - mae: 2.3026\n",
            "Epoch 00019: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1096 - mae: 2.3026 - val_loss: 5.8389 - val_mae: 7.2754 - lr: 3.9063e-06\n",
            "Epoch 20/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1206 - mae: 2.3007\n",
            "Epoch 00020: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1206 - mae: 2.3007 - val_loss: 5.9574 - val_mae: 7.2131 - lr: 1.9531e-06\n",
            "Epoch 21/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1087 - mae: 2.2750\n",
            "Epoch 00021: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1087 - mae: 2.2750 - val_loss: 5.9765 - val_mae: 7.3438 - lr: 1.9531e-06\n",
            "Epoch 22/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1122 - mae: 2.2967\n",
            "Epoch 00022: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1122 - mae: 2.2967 - val_loss: 5.8966 - val_mae: 7.1921 - lr: 9.7656e-07\n",
            "Epoch 23/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1031 - mae: 2.3232\n",
            "Epoch 00023: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1031 - mae: 2.3232 - val_loss: 5.8051 - val_mae: 7.2124 - lr: 9.7656e-07\n",
            "Epoch 24/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1075 - mae: 2.2847\n",
            "Epoch 00024: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1075 - mae: 2.2847 - val_loss: 5.8543 - val_mae: 7.3450 - lr: 4.8828e-07\n",
            "Epoch 25/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1059 - mae: 2.2896\n",
            "Epoch 00025: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1059 - mae: 2.2896 - val_loss: 5.8657 - val_mae: 7.0398 - lr: 4.8828e-07\n",
            "Epoch 26/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1087 - mae: 2.2669\n",
            "Epoch 00026: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1087 - mae: 2.2669 - val_loss: 6.0488 - val_mae: 7.4898 - lr: 2.4414e-07\n",
            "Epoch 27/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1129 - mae: 2.2982\n",
            "Epoch 00027: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1129 - mae: 2.2982 - val_loss: 5.7785 - val_mae: 7.1122 - lr: 2.4414e-07\n",
            "Epoch 28/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1174 - mae: 2.3159\n",
            "Epoch 00028: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.1174 - mae: 2.3159 - val_loss: 5.8267 - val_mae: 7.1174 - lr: 1.2207e-07\n",
            "Epoch 29/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1071 - mae: 2.3212\n",
            "Epoch 00029: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1071 - mae: 2.3212 - val_loss: 5.8222 - val_mae: 7.1930 - lr: 1.2207e-07\n",
            "Epoch 30/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1095 - mae: 2.3123\n",
            "Epoch 00030: val_loss did not improve from 4.51817\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1095 - mae: 2.3123 - val_loss: 5.8200 - val_mae: 7.1888 - lr: 6.1035e-08\n",
            "Epoch 31/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1117 - mae: 2.3162Restoring model weights from the end of the best epoch: 1.\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 4.51817\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "241/241 [==============================] - 20s 85ms/step - loss: 2.1117 - mae: 2.3162 - val_loss: 5.8870 - val_mae: 7.2425 - lr: 6.1035e-08\n",
            "Epoch 00031: early stopping\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 4.0300 - mae: 14.5957\n",
            "Fit model on training data fold  14\n",
            "Epoch 1/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 23.9543 - mae: 30.6263\n",
            "Epoch 00001: val_loss improved from inf to 4.36023, saving model to /home/jupyter/BAMI/fold14.h5\n",
            "241/241 [==============================] - 34s 103ms/step - loss: 23.9543 - mae: 30.6263 - val_loss: 4.3602 - val_mae: 14.6173 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.4492 - mae: 9.2794\n",
            "Epoch 00002: val_loss improved from 4.36023 to 4.18750, saving model to /home/jupyter/BAMI/fold14.h5\n",
            "241/241 [==============================] - 22s 91ms/step - loss: 3.4492 - mae: 9.2794 - val_loss: 4.1875 - val_mae: 11.4672 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.1745 - mae: 6.7427\n",
            "Epoch 00003: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 3.1745 - mae: 6.7427 - val_loss: 4.8347 - val_mae: 11.4063 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.0125 - mae: 5.6240\n",
            "Epoch 00004: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "241/241 [==============================] - 21s 89ms/step - loss: 3.0125 - mae: 5.6240 - val_loss: 5.6700 - val_mae: 12.6518 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.6406 - mae: 3.9427\n",
            "Epoch 00005: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 2.6406 - mae: 3.9427 - val_loss: 4.8513 - val_mae: 8.3556 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.5659 - mae: 3.5916\n",
            "Epoch 00006: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 2.5659 - mae: 3.5916 - val_loss: 5.0373 - val_mae: 8.2656 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.3589 - mae: 2.9205\n",
            "Epoch 00007: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 89ms/step - loss: 2.3589 - mae: 2.9205 - val_loss: 5.2628 - val_mae: 7.3050 - lr: 2.5000e-04\n",
            "Epoch 8/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.3167 - mae: 2.7530\n",
            "Epoch 00008: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.3167 - mae: 2.7530 - val_loss: 5.6585 - val_mae: 7.4264 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2200 - mae: 2.5226\n",
            "Epoch 00009: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.2200 - mae: 2.5226 - val_loss: 5.8647 - val_mae: 7.4570 - lr: 1.2500e-04\n",
            "Epoch 10/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1651 - mae: 2.4125\n",
            "Epoch 00010: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1651 - mae: 2.4125 - val_loss: 5.5964 - val_mae: 6.7538 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1434 - mae: 2.3270\n",
            "Epoch 00011: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1434 - mae: 2.3270 - val_loss: 5.8960 - val_mae: 6.9479 - lr: 6.2500e-05\n",
            "Epoch 12/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1087 - mae: 2.2308\n",
            "Epoch 00012: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1087 - mae: 2.2308 - val_loss: 5.7639 - val_mae: 6.6859 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0800 - mae: 2.1780\n",
            "Epoch 00013: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.0800 - mae: 2.1780 - val_loss: 6.1742 - val_mae: 6.9444 - lr: 3.1250e-05\n",
            "Epoch 14/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0911 - mae: 2.1606\n",
            "Epoch 00014: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0911 - mae: 2.1606 - val_loss: 6.0632 - val_mae: 6.6920 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0653 - mae: 2.1364\n",
            "Epoch 00015: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0653 - mae: 2.1364 - val_loss: 5.9126 - val_mae: 6.5593 - lr: 1.5625e-05\n",
            "Epoch 16/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0649 - mae: 2.1608\n",
            "Epoch 00016: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0649 - mae: 2.1608 - val_loss: 6.2142 - val_mae: 6.8946 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0557 - mae: 2.1373\n",
            "Epoch 00017: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0557 - mae: 2.1373 - val_loss: 6.1133 - val_mae: 6.7057 - lr: 7.8125e-06\n",
            "Epoch 18/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0507 - mae: 2.1123\n",
            "Epoch 00018: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0507 - mae: 2.1123 - val_loss: 6.3901 - val_mae: 6.9147 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0498 - mae: 2.0875\n",
            "Epoch 00019: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0498 - mae: 2.0875 - val_loss: 6.1720 - val_mae: 6.7406 - lr: 3.9063e-06\n",
            "Epoch 20/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0549 - mae: 2.1118\n",
            "Epoch 00020: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0549 - mae: 2.1118 - val_loss: 6.2370 - val_mae: 6.8737 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0403 - mae: 2.0717\n",
            "Epoch 00021: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0403 - mae: 2.0717 - val_loss: 6.4423 - val_mae: 6.9790 - lr: 1.9531e-06\n",
            "Epoch 22/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0510 - mae: 2.0916\n",
            "Epoch 00022: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0510 - mae: 2.0916 - val_loss: 6.2566 - val_mae: 6.6733 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0429 - mae: 2.1098\n",
            "Epoch 00023: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0429 - mae: 2.1098 - val_loss: 6.3255 - val_mae: 6.9559 - lr: 9.7656e-07\n",
            "Epoch 24/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0329 - mae: 2.0867\n",
            "Epoch 00024: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0329 - mae: 2.0867 - val_loss: 6.5233 - val_mae: 7.2325 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0369 - mae: 2.0710\n",
            "Epoch 00025: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0369 - mae: 2.0710 - val_loss: 6.2448 - val_mae: 6.8678 - lr: 4.8828e-07\n",
            "Epoch 26/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0456 - mae: 2.1156\n",
            "Epoch 00026: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0456 - mae: 2.1156 - val_loss: 6.1034 - val_mae: 6.4932 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0396 - mae: 2.1057\n",
            "Epoch 00027: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0396 - mae: 2.1057 - val_loss: 6.1306 - val_mae: 6.6323 - lr: 2.4414e-07\n",
            "Epoch 28/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0422 - mae: 2.0767\n",
            "Epoch 00028: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "241/241 [==============================] - 21s 85ms/step - loss: 2.0422 - mae: 2.0767 - val_loss: 6.3047 - val_mae: 6.8809 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0421 - mae: 2.0728\n",
            "Epoch 00029: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0421 - mae: 2.0728 - val_loss: 6.1965 - val_mae: 6.6832 - lr: 1.2207e-07\n",
            "Epoch 30/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0385 - mae: 2.0815\n",
            "Epoch 00030: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0385 - mae: 2.0815 - val_loss: 6.3073 - val_mae: 6.6730 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0384 - mae: 2.0644\n",
            "Epoch 00031: val_loss did not improve from 4.18750\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0384 - mae: 2.0644 - val_loss: 6.1833 - val_mae: 6.7253 - lr: 6.1035e-08\n",
            "Epoch 32/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0387 - mae: 2.0608Restoring model weights from the end of the best epoch: 2.\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 4.18750\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0387 - mae: 2.0608 - val_loss: 6.3656 - val_mae: 6.9663 - lr: 6.1035e-08\n",
            "Epoch 00032: early stopping\n",
            "14/14 [==============================] - 0s 24ms/step - loss: 4.0496 - mae: 10.8521\n",
            "Fit model on training data fold  15\n",
            "Epoch 1/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 15.9535 - mae: 22.2978\n",
            "Epoch 00001: val_loss improved from inf to 6.27662, saving model to /home/jupyter/BAMI/fold15.h5\n",
            "241/241 [==============================] - 36s 111ms/step - loss: 15.9535 - mae: 22.2978 - val_loss: 6.2766 - val_mae: 21.0789 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.3623 - mae: 8.3127\n",
            "Epoch 00002: val_loss improved from 6.27662 to 5.06293, saving model to /home/jupyter/BAMI/fold15.h5\n",
            "241/241 [==============================] - 22s 91ms/step - loss: 3.3623 - mae: 8.3127 - val_loss: 5.0629 - val_mae: 14.6640 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.1127 - mae: 6.4141\n",
            "Epoch 00003: val_loss did not improve from 5.06293\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 3.1127 - mae: 6.4141 - val_loss: 6.0154 - val_mae: 15.4251 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.9610 - mae: 5.3678\n",
            "Epoch 00004: val_loss did not improve from 5.06293\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 2.9610 - mae: 5.3678 - val_loss: 8.0044 - val_mae: 17.6080 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.6600 - mae: 4.0792\n",
            "Epoch 00005: val_loss improved from 5.06293 to 4.81184, saving model to /home/jupyter/BAMI/fold15.h5\n",
            "241/241 [==============================] - 21s 89ms/step - loss: 2.6600 - mae: 4.0792 - val_loss: 4.8118 - val_mae: 8.3582 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.5824 - mae: 3.6471\n",
            "Epoch 00006: val_loss improved from 4.81184 to 4.73698, saving model to /home/jupyter/BAMI/fold15.h5\n",
            "241/241 [==============================] - 21s 89ms/step - loss: 2.5824 - mae: 3.6471 - val_loss: 4.7370 - val_mae: 7.7493 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.5315 - mae: 3.4442\n",
            "Epoch 00007: val_loss improved from 4.73698 to 3.60701, saving model to /home/jupyter/BAMI/fold15.h5\n",
            "241/241 [==============================] - 21s 89ms/step - loss: 2.5315 - mae: 3.4442 - val_loss: 3.6070 - val_mae: 5.7061 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.5018 - mae: 3.2736\n",
            "Epoch 00008: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 89ms/step - loss: 2.5018 - mae: 3.2736 - val_loss: 6.1633 - val_mae: 9.7231 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.4780 - mae: 3.2489\n",
            "Epoch 00009: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.4780 - mae: 3.2489 - val_loss: 5.2143 - val_mae: 7.9932 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2568 - mae: 2.6656\n",
            "Epoch 00010: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.2568 - mae: 2.6656 - val_loss: 4.9057 - val_mae: 6.5389 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2329 - mae: 2.5397\n",
            "Epoch 00011: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.2329 - mae: 2.5397 - val_loss: 4.5354 - val_mae: 5.8765 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1459 - mae: 2.3514\n",
            "Epoch 00012: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1459 - mae: 2.3514 - val_loss: 4.8194 - val_mae: 5.9391 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1309 - mae: 2.2819\n",
            "Epoch 00013: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1309 - mae: 2.2819 - val_loss: 5.0950 - val_mae: 6.3076 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0819 - mae: 2.1880\n",
            "Epoch 00014: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0819 - mae: 2.1880 - val_loss: 6.0447 - val_mae: 7.1662 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0704 - mae: 2.1704\n",
            "Epoch 00015: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0704 - mae: 2.1704 - val_loss: 6.1959 - val_mae: 7.1122 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0443 - mae: 2.1043\n",
            "Epoch 00016: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0443 - mae: 2.1043 - val_loss: 6.7617 - val_mae: 7.8377 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0401 - mae: 2.1055\n",
            "Epoch 00017: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.0401 - mae: 2.1055 - val_loss: 4.9593 - val_mae: 5.7447 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0317 - mae: 2.0801\n",
            "Epoch 00018: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0317 - mae: 2.0801 - val_loss: 5.2772 - val_mae: 5.9692 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0267 - mae: 2.0340\n",
            "Epoch 00019: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0267 - mae: 2.0340 - val_loss: 4.6405 - val_mae: 5.1278 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0231 - mae: 2.0275\n",
            "Epoch 00020: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0231 - mae: 2.0275 - val_loss: 5.5369 - val_mae: 6.3321 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0243 - mae: 2.0422\n",
            "Epoch 00021: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0243 - mae: 2.0422 - val_loss: 4.5311 - val_mae: 5.1518 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0124 - mae: 2.0293\n",
            "Epoch 00022: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0124 - mae: 2.0293 - val_loss: 5.6478 - val_mae: 6.4684 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0162 - mae: 2.0127\n",
            "Epoch 00023: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0162 - mae: 2.0127 - val_loss: 5.8966 - val_mae: 6.5772 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0088 - mae: 2.0196\n",
            "Epoch 00024: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0088 - mae: 2.0196 - val_loss: 5.4647 - val_mae: 6.1954 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0083 - mae: 2.0455\n",
            "Epoch 00025: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0083 - mae: 2.0455 - val_loss: 4.9814 - val_mae: 5.5274 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0110 - mae: 1.9952\n",
            "Epoch 00026: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0110 - mae: 1.9952 - val_loss: 6.3226 - val_mae: 7.2572 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0097 - mae: 2.0300\n",
            "Epoch 00027: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0097 - mae: 2.0300 - val_loss: 4.6714 - val_mae: 5.3857 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0060 - mae: 2.0187\n",
            "Epoch 00028: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0060 - mae: 2.0187 - val_loss: 3.7691 - val_mae: 4.1912 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 1.9987 - mae: 2.0159\n",
            "Epoch 00029: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 1.9987 - mae: 2.0159 - val_loss: 4.7726 - val_mae: 5.3020 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0071 - mae: 2.0390\n",
            "Epoch 00030: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0071 - mae: 2.0390 - val_loss: 4.3244 - val_mae: 4.9932 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0038 - mae: 2.0170\n",
            "Epoch 00031: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.0038 - mae: 2.0170 - val_loss: 4.2840 - val_mae: 4.7734 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0096 - mae: 2.0406\n",
            "Epoch 00032: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0096 - mae: 2.0406 - val_loss: 5.0984 - val_mae: 5.7574 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0030 - mae: 2.0261\n",
            "Epoch 00033: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0030 - mae: 2.0261 - val_loss: 4.2637 - val_mae: 4.8337 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0068 - mae: 2.0398\n",
            "Epoch 00034: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0068 - mae: 2.0398 - val_loss: 4.4529 - val_mae: 5.0426 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0106 - mae: 2.0271\n",
            "Epoch 00035: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0106 - mae: 2.0271 - val_loss: 7.7809 - val_mae: 8.9271 - lr: 6.1035e-08\n",
            "Epoch 36/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0067 - mae: 2.0566\n",
            "Epoch 00036: val_loss did not improve from 3.60701\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0067 - mae: 2.0566 - val_loss: 5.7026 - val_mae: 6.4892 - lr: 3.0518e-08\n",
            "Epoch 37/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0097 - mae: 2.0381Restoring model weights from the end of the best epoch: 7.\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 3.60701\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0097 - mae: 2.0381 - val_loss: 5.5062 - val_mae: 6.2856 - lr: 3.0518e-08\n",
            "Epoch 00037: early stopping\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 3.0800 - mae: 4.5372\n",
            "Fit model on training data fold  16\n",
            "Epoch 1/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 14.6382 - mae: 19.6653\n",
            "Epoch 00001: val_loss improved from inf to 5.46593, saving model to /home/jupyter/BAMI/fold16.h5\n",
            "241/241 [==============================] - 35s 109ms/step - loss: 14.6382 - mae: 19.6653 - val_loss: 5.4659 - val_mae: 17.4547 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.3870 - mae: 8.2908\n",
            "Epoch 00002: val_loss improved from 5.46593 to 5.04422, saving model to /home/jupyter/BAMI/fold16.h5\n",
            "241/241 [==============================] - 23s 95ms/step - loss: 3.3870 - mae: 8.2908 - val_loss: 5.0442 - val_mae: 14.8203 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.1655 - mae: 6.6555\n",
            "Epoch 00003: val_loss did not improve from 5.04422\n",
            "241/241 [==============================] - 22s 91ms/step - loss: 3.1655 - mae: 6.6555 - val_loss: 6.6362 - val_mae: 17.1024 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.0542 - mae: 5.8040\n",
            "Epoch 00004: val_loss improved from 5.04422 to 4.12529, saving model to /home/jupyter/BAMI/fold16.h5\n",
            "241/241 [==============================] - 22s 90ms/step - loss: 3.0542 - mae: 5.8040 - val_loss: 4.1253 - val_mae: 9.0448 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.9531 - mae: 5.2427\n",
            "Epoch 00005: val_loss improved from 4.12529 to 3.34141, saving model to /home/jupyter/BAMI/fold16.h5\n",
            "241/241 [==============================] - 22s 90ms/step - loss: 2.9531 - mae: 5.2427 - val_loss: 3.3414 - val_mae: 6.7836 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.9168 - mae: 4.9758\n",
            "Epoch 00006: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 89ms/step - loss: 2.9168 - mae: 4.9758 - val_loss: 4.3021 - val_mae: 8.6610 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.8180 - mae: 4.5663\n",
            "Epoch 00007: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.8180 - mae: 4.5663 - val_loss: 7.6680 - val_mae: 14.9153 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.5096 - mae: 3.4940\n",
            "Epoch 00008: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.5096 - mae: 3.4940 - val_loss: 3.4649 - val_mae: 5.2970 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.5073 - mae: 3.3542\n",
            "Epoch 00009: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.5073 - mae: 3.3542 - val_loss: 5.5306 - val_mae: 8.4898 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.3356 - mae: 2.8259\n",
            "Epoch 00010: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.3356 - mae: 2.8259 - val_loss: 4.9102 - val_mae: 6.7663 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2897 - mae: 2.6575\n",
            "Epoch 00011: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.2897 - mae: 2.6575 - val_loss: 4.9021 - val_mae: 6.4880 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2266 - mae: 2.5142\n",
            "Epoch 00012: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.2266 - mae: 2.5142 - val_loss: 5.8901 - val_mae: 7.6037 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2014 - mae: 2.4649\n",
            "Epoch 00013: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.2014 - mae: 2.4649 - val_loss: 4.4141 - val_mae: 5.4742 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1789 - mae: 2.3735\n",
            "Epoch 00014: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1789 - mae: 2.3735 - val_loss: 4.7688 - val_mae: 5.7961 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1575 - mae: 2.3344\n",
            "Epoch 00015: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1575 - mae: 2.3344 - val_loss: 4.9551 - val_mae: 5.8347 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1368 - mae: 2.2766\n",
            "Epoch 00016: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1368 - mae: 2.2766 - val_loss: 5.7047 - val_mae: 6.8376 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1314 - mae: 2.2675\n",
            "Epoch 00017: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1314 - mae: 2.2675 - val_loss: 4.9990 - val_mae: 5.9315 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1174 - mae: 2.2218\n",
            "Epoch 00018: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1174 - mae: 2.2218 - val_loss: 4.0871 - val_mae: 4.6385 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1045 - mae: 2.2214\n",
            "Epoch 00019: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1045 - mae: 2.2214 - val_loss: 5.3324 - val_mae: 6.3421 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1074 - mae: 2.2134\n",
            "Epoch 00020: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1074 - mae: 2.2134 - val_loss: 5.3600 - val_mae: 6.2152 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1005 - mae: 2.2061\n",
            "Epoch 00021: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1005 - mae: 2.2061 - val_loss: 4.9489 - val_mae: 5.7503 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0993 - mae: 2.2062\n",
            "Epoch 00022: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0993 - mae: 2.2062 - val_loss: 5.2975 - val_mae: 6.0805 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1017 - mae: 2.2084\n",
            "Epoch 00023: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1017 - mae: 2.2084 - val_loss: 4.1945 - val_mae: 4.7020 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1085 - mae: 2.1951\n",
            "Epoch 00024: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1085 - mae: 2.1951 - val_loss: 5.2237 - val_mae: 6.0843 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0981 - mae: 2.2308\n",
            "Epoch 00025: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0981 - mae: 2.2308 - val_loss: 4.1105 - val_mae: 4.6378 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1064 - mae: 2.1944\n",
            "Epoch 00026: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1064 - mae: 2.1944 - val_loss: 4.9335 - val_mae: 5.7366 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0966 - mae: 2.2141\n",
            "Epoch 00027: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0966 - mae: 2.2141 - val_loss: 4.0403 - val_mae: 4.5887 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1068 - mae: 2.1957\n",
            "Epoch 00028: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1068 - mae: 2.1957 - val_loss: 5.3904 - val_mae: 6.1906 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0946 - mae: 2.1819\n",
            "Epoch 00029: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0946 - mae: 2.1819 - val_loss: 4.4173 - val_mae: 4.9754 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0960 - mae: 2.1964\n",
            "Epoch 00030: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0960 - mae: 2.1964 - val_loss: 4.4665 - val_mae: 4.9166 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1017 - mae: 2.1883\n",
            "Epoch 00031: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.1017 - mae: 2.1883 - val_loss: 5.3063 - val_mae: 6.0701 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0990 - mae: 2.2102\n",
            "Epoch 00032: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0990 - mae: 2.2102 - val_loss: 5.3703 - val_mae: 6.0943 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0918 - mae: 2.1921\n",
            "Epoch 00033: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0918 - mae: 2.1921 - val_loss: 4.0025 - val_mae: 4.4322 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0998 - mae: 2.1996\n",
            "Epoch 00034: val_loss did not improve from 3.34141\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.0998 - mae: 2.1996 - val_loss: 5.5017 - val_mae: 6.3320 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.0916 - mae: 2.2030Restoring model weights from the end of the best epoch: 5.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 3.34141\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "241/241 [==============================] - 21s 86ms/step - loss: 2.0916 - mae: 2.2030 - val_loss: 4.8005 - val_mae: 5.5279 - lr: 6.1035e-08\n",
            "Epoch 00035: early stopping\n",
            "14/14 [==============================] - 0s 27ms/step - loss: 2.9923 - mae: 5.5449\n",
            "Fit model on training data fold  17\n",
            "Epoch 1/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 18.9779 - mae: 32.8120\n",
            "Epoch 00001: val_loss improved from inf to 5.13905, saving model to /home/jupyter/BAMI/fold17.h5\n",
            "241/241 [==============================] - 37s 112ms/step - loss: 18.9779 - mae: 32.8120 - val_loss: 5.1391 - val_mae: 22.9900 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.8796 - mae: 13.6860\n",
            "Epoch 00002: val_loss improved from 5.13905 to 4.17248, saving model to /home/jupyter/BAMI/fold17.h5\n",
            "241/241 [==============================] - 23s 94ms/step - loss: 3.8796 - mae: 13.6860 - val_loss: 4.1725 - val_mae: 15.0822 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.5832 - mae: 10.3888\n",
            "Epoch 00003: val_loss did not improve from 4.17248\n",
            "241/241 [==============================] - 22s 92ms/step - loss: 3.5832 - mae: 10.3888 - val_loss: 4.2512 - val_mae: 13.2301 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.3825 - mae: 8.3230\n",
            "Epoch 00004: val_loss improved from 4.17248 to 3.47068, saving model to /home/jupyter/BAMI/fold17.h5\n",
            "241/241 [==============================] - 22s 90ms/step - loss: 3.3825 - mae: 8.3230 - val_loss: 3.4707 - val_mae: 8.6557 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.2098 - mae: 6.8758\n",
            "Epoch 00005: val_loss did not improve from 3.47068\n",
            "241/241 [==============================] - 22s 92ms/step - loss: 3.2098 - mae: 6.8758 - val_loss: 3.8226 - val_mae: 9.1719 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 3.0586 - mae: 5.8508\n",
            "Epoch 00006: val_loss did not improve from 3.47068\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "241/241 [==============================] - 22s 89ms/step - loss: 3.0586 - mae: 5.8508 - val_loss: 3.6750 - val_mae: 7.6539 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.7025 - mae: 4.2552\n",
            "Epoch 00007: val_loss improved from 3.47068 to 3.43451, saving model to /home/jupyter/BAMI/fold17.h5\n",
            "241/241 [==============================] - 22s 93ms/step - loss: 2.7025 - mae: 4.2552 - val_loss: 3.4345 - val_mae: 5.7027 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.6559 - mae: 3.8587\n",
            "Epoch 00008: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 22s 92ms/step - loss: 2.6559 - mae: 3.8587 - val_loss: 3.6779 - val_mae: 6.0828 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.5775 - mae: 3.4510\n",
            "Epoch 00009: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "241/241 [==============================] - 22s 90ms/step - loss: 2.5775 - mae: 3.4510 - val_loss: 3.6670 - val_mae: 5.6266 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.4165 - mae: 3.0923\n",
            "Epoch 00010: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 22s 91ms/step - loss: 2.4165 - mae: 3.0923 - val_loss: 4.1424 - val_mae: 5.6835 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.3696 - mae: 2.9100\n",
            "Epoch 00011: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "241/241 [==============================] - 21s 88ms/step - loss: 2.3696 - mae: 2.9100 - val_loss: 3.8883 - val_mae: 5.1619 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2844 - mae: 2.7046\n",
            "Epoch 00012: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.2844 - mae: 2.7046 - val_loss: 3.7604 - val_mae: 4.6692 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.2561 - mae: 2.5888\n",
            "Epoch 00013: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.2561 - mae: 2.5888 - val_loss: 3.7195 - val_mae: 4.4087 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1959 - mae: 2.4691\n",
            "Epoch 00014: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1959 - mae: 2.4691 - val_loss: 3.8777 - val_mae: 4.5302 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1854 - mae: 2.4111\n",
            "Epoch 00015: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1854 - mae: 2.4111 - val_loss: 3.8051 - val_mae: 4.4151 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1630 - mae: 2.3351\n",
            "Epoch 00016: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1630 - mae: 2.3351 - val_loss: 4.1482 - val_mae: 4.8228 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1597 - mae: 2.3479\n",
            "Epoch 00017: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1597 - mae: 2.3479 - val_loss: 3.6979 - val_mae: 4.2074 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1429 - mae: 2.2930\n",
            "Epoch 00018: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1429 - mae: 2.2930 - val_loss: 3.6634 - val_mae: 4.0899 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1399 - mae: 2.3264\n",
            "Epoch 00019: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1399 - mae: 2.3264 - val_loss: 3.8407 - val_mae: 4.2678 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1344 - mae: 2.2805\n",
            "Epoch 00020: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1344 - mae: 2.2805 - val_loss: 3.8939 - val_mae: 4.2606 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1251 - mae: 2.2797\n",
            "Epoch 00021: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "241/241 [==============================] - 22s 90ms/step - loss: 2.1251 - mae: 2.2797 - val_loss: 3.8092 - val_mae: 4.3089 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1234 - mae: 2.2924\n",
            "Epoch 00022: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1234 - mae: 2.2924 - val_loss: 4.0142 - val_mae: 4.4657 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1205 - mae: 2.2575\n",
            "Epoch 00023: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1205 - mae: 2.2575 - val_loss: 3.9148 - val_mae: 4.3693 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1238 - mae: 2.2741\n",
            "Epoch 00024: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1238 - mae: 2.2741 - val_loss: 3.8909 - val_mae: 4.2823 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1250 - mae: 2.2508\n",
            "Epoch 00025: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1250 - mae: 2.2508 - val_loss: 3.9274 - val_mae: 4.3643 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1196 - mae: 2.2639\n",
            "Epoch 00026: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1196 - mae: 2.2639 - val_loss: 3.9876 - val_mae: 4.4147 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1126 - mae: 2.2548\n",
            "Epoch 00027: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1126 - mae: 2.2548 - val_loss: 3.8051 - val_mae: 4.2143 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1076 - mae: 2.2300\n",
            "Epoch 00028: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1076 - mae: 2.2300 - val_loss: 3.8967 - val_mae: 4.3819 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1103 - mae: 2.2356\n",
            "Epoch 00029: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1103 - mae: 2.2356 - val_loss: 4.1364 - val_mae: 4.6767 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1159 - mae: 2.2729\n",
            "Epoch 00030: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1159 - mae: 2.2729 - val_loss: 3.9232 - val_mae: 4.4895 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1227 - mae: 2.2489\n",
            "Epoch 00031: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1227 - mae: 2.2489 - val_loss: 3.9725 - val_mae: 4.4949 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1198 - mae: 2.2652\n",
            "Epoch 00032: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 22s 89ms/step - loss: 2.1198 - mae: 2.2652 - val_loss: 3.9827 - val_mae: 4.3693 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1185 - mae: 2.2664\n",
            "Epoch 00033: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1185 - mae: 2.2664 - val_loss: 3.8595 - val_mae: 4.2056 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1193 - mae: 2.2587\n",
            "Epoch 00034: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1193 - mae: 2.2587 - val_loss: 3.9501 - val_mae: 4.2943 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1188 - mae: 2.2847\n",
            "Epoch 00035: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1188 - mae: 2.2847 - val_loss: 3.8955 - val_mae: 4.3179 - lr: 6.1035e-08\n",
            "Epoch 36/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1186 - mae: 2.2391\n",
            "Epoch 00036: val_loss did not improve from 3.43451\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1186 - mae: 2.2391 - val_loss: 4.0564 - val_mae: 4.5601 - lr: 3.0518e-08\n",
            "Epoch 37/200\n",
            "241/241 [==============================] - ETA: 0s - loss: 2.1228 - mae: 2.2539Restoring model weights from the end of the best epoch: 7.\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 3.43451\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "241/241 [==============================] - 21s 87ms/step - loss: 2.1228 - mae: 2.2539 - val_loss: 4.2181 - val_mae: 4.6676 - lr: 3.0518e-08\n",
            "Epoch 00037: early stopping\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 4.8475 - mae: 8.6070\n",
            "Fit model on training data fold  18\n",
            "Epoch 1/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 19.6528 - mae: 28.2629\n",
            "Epoch 00001: val_loss improved from inf to 4.25353, saving model to /home/jupyter/BAMI/fold18.h5\n",
            "248/248 [==============================] - 37s 109ms/step - loss: 19.6528 - mae: 28.2629 - val_loss: 4.2535 - val_mae: 15.1518 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 3.7647 - mae: 12.1181\n",
            "Epoch 00002: val_loss improved from 4.25353 to 3.62976, saving model to /home/jupyter/BAMI/fold18.h5\n",
            "248/248 [==============================] - 24s 97ms/step - loss: 3.7647 - mae: 12.1181 - val_loss: 3.6298 - val_mae: 11.1266 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 3.3756 - mae: 8.8028\n",
            "Epoch 00003: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 24s 95ms/step - loss: 3.3756 - mae: 8.8028 - val_loss: 4.1427 - val_mae: 11.3573 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 3.2010 - mae: 6.8948\n",
            "Epoch 00004: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "248/248 [==============================] - 24s 95ms/step - loss: 3.2010 - mae: 6.8948 - val_loss: 4.1361 - val_mae: 9.8622 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.8305 - mae: 4.9652\n",
            "Epoch 00005: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 23s 95ms/step - loss: 2.8305 - mae: 4.9652 - val_loss: 3.8106 - val_mae: 7.0447 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.7472 - mae: 4.3445\n",
            "Epoch 00006: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "248/248 [==============================] - 23s 92ms/step - loss: 2.7472 - mae: 4.3445 - val_loss: 6.3653 - val_mae: 11.4588 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.5510 - mae: 3.6220\n",
            "Epoch 00007: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 22s 90ms/step - loss: 2.5510 - mae: 3.6220 - val_loss: 3.7457 - val_mae: 5.5156 - lr: 2.5000e-04\n",
            "Epoch 8/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.4803 - mae: 3.2520\n",
            "Epoch 00008: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "248/248 [==============================] - 23s 91ms/step - loss: 2.4803 - mae: 3.2520 - val_loss: 7.1822 - val_mae: 10.3312 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3673 - mae: 2.9101\n",
            "Epoch 00009: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.3673 - mae: 2.9101 - val_loss: 4.4670 - val_mae: 6.0854 - lr: 1.2500e-04\n",
            "Epoch 10/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3318 - mae: 2.7975\n",
            "Epoch 00010: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.3318 - mae: 2.7975 - val_loss: 3.6634 - val_mae: 4.5200 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.2887 - mae: 2.6833\n",
            "Epoch 00011: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.2887 - mae: 2.6833 - val_loss: 5.1501 - val_mae: 6.5015 - lr: 6.2500e-05\n",
            "Epoch 12/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.2551 - mae: 2.5634\n",
            "Epoch 00012: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.2551 - mae: 2.5634 - val_loss: 4.0153 - val_mae: 4.8772 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.2176 - mae: 2.4665\n",
            "Epoch 00013: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.2176 - mae: 2.4665 - val_loss: 5.6484 - val_mae: 6.5306 - lr: 3.1250e-05\n",
            "Epoch 14/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.2222 - mae: 2.4858\n",
            "Epoch 00014: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.2222 - mae: 2.4858 - val_loss: 7.6465 - val_mae: 8.9815 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.2030 - mae: 2.4240\n",
            "Epoch 00015: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.2030 - mae: 2.4240 - val_loss: 5.7035 - val_mae: 6.7146 - lr: 1.5625e-05\n",
            "Epoch 16/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.2128 - mae: 2.4448\n",
            "Epoch 00016: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.2128 - mae: 2.4448 - val_loss: 4.5869 - val_mae: 5.4481 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1873 - mae: 2.4078\n",
            "Epoch 00017: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.1873 - mae: 2.4078 - val_loss: 5.1715 - val_mae: 5.9614 - lr: 7.8125e-06\n",
            "Epoch 18/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1891 - mae: 2.4097\n",
            "Epoch 00018: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.1891 - mae: 2.4097 - val_loss: 6.4751 - val_mae: 7.5880 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1837 - mae: 2.3742\n",
            "Epoch 00019: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 22s 87ms/step - loss: 2.1837 - mae: 2.3742 - val_loss: 5.6555 - val_mae: 6.4563 - lr: 3.9063e-06\n",
            "Epoch 20/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1839 - mae: 2.3745\n",
            "Epoch 00020: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "248/248 [==============================] - 22s 87ms/step - loss: 2.1839 - mae: 2.3745 - val_loss: 5.2911 - val_mae: 6.0994 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1825 - mae: 2.3305\n",
            "Epoch 00021: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.1825 - mae: 2.3305 - val_loss: 6.1185 - val_mae: 6.9653 - lr: 1.9531e-06\n",
            "Epoch 22/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1875 - mae: 2.3539\n",
            "Epoch 00022: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "248/248 [==============================] - 21s 87ms/step - loss: 2.1875 - mae: 2.3539 - val_loss: 6.5425 - val_mae: 7.4507 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1783 - mae: 2.3455\n",
            "Epoch 00023: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.1783 - mae: 2.3455 - val_loss: 7.2470 - val_mae: 8.3734 - lr: 9.7656e-07\n",
            "Epoch 24/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1822 - mae: 2.3747\n",
            "Epoch 00024: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.1822 - mae: 2.3747 - val_loss: 4.2166 - val_mae: 4.6866 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1816 - mae: 2.3583\n",
            "Epoch 00025: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.1816 - mae: 2.3583 - val_loss: 6.9013 - val_mae: 7.9300 - lr: 4.8828e-07\n",
            "Epoch 26/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1759 - mae: 2.3750\n",
            "Epoch 00026: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "248/248 [==============================] - 21s 87ms/step - loss: 2.1759 - mae: 2.3750 - val_loss: 6.2114 - val_mae: 6.9940 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1920 - mae: 2.3997\n",
            "Epoch 00027: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 2.1920 - mae: 2.3997 - val_loss: 5.6508 - val_mae: 6.4300 - lr: 2.4414e-07\n",
            "Epoch 28/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1725 - mae: 2.3450\n",
            "Epoch 00028: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "248/248 [==============================] - 21s 84ms/step - loss: 2.1725 - mae: 2.3450 - val_loss: 6.3355 - val_mae: 7.1418 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1789 - mae: 2.3578\n",
            "Epoch 00029: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 21s 84ms/step - loss: 2.1789 - mae: 2.3578 - val_loss: 6.8917 - val_mae: 7.7801 - lr: 1.2207e-07\n",
            "Epoch 30/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1802 - mae: 2.3443\n",
            "Epoch 00030: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 2.1802 - mae: 2.3443 - val_loss: 6.7517 - val_mae: 7.7551 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1775 - mae: 2.3768\n",
            "Epoch 00031: val_loss did not improve from 3.62976\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 2.1775 - mae: 2.3768 - val_loss: 5.7865 - val_mae: 6.6280 - lr: 6.1035e-08\n",
            "Epoch 32/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1633 - mae: 2.3693Restoring model weights from the end of the best epoch: 2.\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 3.62976\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "248/248 [==============================] - 21s 84ms/step - loss: 2.1633 - mae: 2.3693 - val_loss: 6.4543 - val_mae: 7.3517 - lr: 6.1035e-08\n",
            "Epoch 00032: early stopping\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 4.5580 - mae: 14.8603\n",
            "Fit model on training data fold  19\n",
            "Epoch 1/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 13.7718 - mae: 26.8210\n",
            "Epoch 00001: val_loss improved from inf to 4.75637, saving model to /home/jupyter/BAMI/fold19.h5\n",
            "248/248 [==============================] - 35s 103ms/step - loss: 13.7718 - mae: 26.8210 - val_loss: 4.7564 - val_mae: 20.5711 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 3.9369 - mae: 14.2904\n",
            "Epoch 00002: val_loss did not improve from 4.75637\n",
            "248/248 [==============================] - 22s 87ms/step - loss: 3.9369 - mae: 14.2904 - val_loss: 5.0166 - val_mae: 21.5551 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 3.6684 - mae: 11.3999\n",
            "Epoch 00003: val_loss improved from 4.75637 to 3.66733, saving model to /home/jupyter/BAMI/fold19.h5\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 3.6684 - mae: 11.3999 - val_loss: 3.6673 - val_mae: 10.7201 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 3.5254 - mae: 9.5266\n",
            "Epoch 00004: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 84ms/step - loss: 3.5254 - mae: 9.5266 - val_loss: 4.5155 - val_mae: 12.9543 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 3.3748 - mae: 8.2105\n",
            "Epoch 00005: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 3.3748 - mae: 8.2105 - val_loss: 4.4892 - val_mae: 10.8402 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 3.1032 - mae: 6.2973\n",
            "Epoch 00006: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 3.1032 - mae: 6.2973 - val_loss: 4.1500 - val_mae: 8.4032 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 3.1029 - mae: 6.0807\n",
            "Epoch 00007: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 3.1029 - mae: 6.0807 - val_loss: 4.7449 - val_mae: 9.7460 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.8289 - mae: 4.7988\n",
            "Epoch 00008: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.8289 - mae: 4.7988 - val_loss: 6.6101 - val_mae: 12.9057 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.7429 - mae: 4.2470\n",
            "Epoch 00009: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.7429 - mae: 4.2470 - val_loss: 7.0851 - val_mae: 12.3914 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.6128 - mae: 3.7451\n",
            "Epoch 00010: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.6128 - mae: 3.7451 - val_loss: 6.3606 - val_mae: 9.9488 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.5523 - mae: 3.4911\n",
            "Epoch 00011: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 2.5523 - mae: 3.4911 - val_loss: 6.6595 - val_mae: 10.2783 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.4796 - mae: 3.2730\n",
            "Epoch 00012: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 2.4796 - mae: 3.2730 - val_loss: 7.0481 - val_mae: 10.1270 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.4658 - mae: 3.1600\n",
            "Epoch 00013: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.4658 - mae: 3.1600 - val_loss: 6.2160 - val_mae: 8.7144 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.4180 - mae: 3.0394\n",
            "Epoch 00014: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 87ms/step - loss: 2.4180 - mae: 3.0394 - val_loss: 6.1793 - val_mae: 8.4639 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3904 - mae: 2.9931\n",
            "Epoch 00015: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 2.3904 - mae: 2.9931 - val_loss: 7.0952 - val_mae: 9.4626 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3810 - mae: 2.9335\n",
            "Epoch 00016: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.3810 - mae: 2.9335 - val_loss: 6.9592 - val_mae: 9.3991 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3775 - mae: 2.8637\n",
            "Epoch 00017: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 2.3775 - mae: 2.8637 - val_loss: 6.7440 - val_mae: 8.9595 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3601 - mae: 2.9172\n",
            "Epoch 00018: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 2.3601 - mae: 2.9172 - val_loss: 6.8287 - val_mae: 8.9077 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3635 - mae: 2.8453\n",
            "Epoch 00019: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.3635 - mae: 2.8453 - val_loss: 6.9311 - val_mae: 9.3650 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3540 - mae: 2.8640\n",
            "Epoch 00020: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.3540 - mae: 2.8640 - val_loss: 8.0361 - val_mae: 10.2838 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3408 - mae: 2.8452\n",
            "Epoch 00021: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 2.3408 - mae: 2.8452 - val_loss: 7.0611 - val_mae: 9.2402 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3438 - mae: 2.8261\n",
            "Epoch 00022: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.3438 - mae: 2.8261 - val_loss: 7.3194 - val_mae: 9.4204 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3493 - mae: 2.8575\n",
            "Epoch 00023: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.3493 - mae: 2.8575 - val_loss: 6.8853 - val_mae: 9.1580 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3502 - mae: 2.8629\n",
            "Epoch 00024: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 2.3502 - mae: 2.8629 - val_loss: 6.8146 - val_mae: 8.9017 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3471 - mae: 2.8068\n",
            "Epoch 00025: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 2.3471 - mae: 2.8068 - val_loss: 7.6382 - val_mae: 10.0212 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3470 - mae: 2.8068\n",
            "Epoch 00026: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.3470 - mae: 2.8068 - val_loss: 7.4875 - val_mae: 9.7103 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3398 - mae: 2.8158\n",
            "Epoch 00027: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 2.3398 - mae: 2.8158 - val_loss: 7.6187 - val_mae: 9.9576 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3392 - mae: 2.8119\n",
            "Epoch 00028: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.3392 - mae: 2.8119 - val_loss: 7.7015 - val_mae: 9.8776 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3416 - mae: 2.7958\n",
            "Epoch 00029: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.3416 - mae: 2.7958 - val_loss: 6.7536 - val_mae: 8.9007 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3386 - mae: 2.7989\n",
            "Epoch 00030: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.3386 - mae: 2.7989 - val_loss: 6.4992 - val_mae: 8.5955 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3415 - mae: 2.7968\n",
            "Epoch 00031: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.3415 - mae: 2.7968 - val_loss: 6.8048 - val_mae: 8.9911 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3493 - mae: 2.8670\n",
            "Epoch 00032: val_loss did not improve from 3.66733\n",
            "248/248 [==============================] - 21s 86ms/step - loss: 2.3493 - mae: 2.8670 - val_loss: 6.9438 - val_mae: 9.2111 - lr: 6.1035e-08\n",
            "Epoch 33/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3465 - mae: 2.8370Restoring model weights from the end of the best epoch: 3.\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 3.66733\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "248/248 [==============================] - 21s 85ms/step - loss: 2.3465 - mae: 2.8370 - val_loss: 6.8678 - val_mae: 9.1286 - lr: 6.1035e-08\n",
            "Epoch 00033: early stopping\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 4.1455 - mae: 13.2299\n",
            "Fit model on training data fold  20\n",
            "Epoch 1/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 16.4272 - mae: 21.1636\n",
            "Epoch 00001: val_loss improved from inf to 3.75567, saving model to /home/jupyter/BAMI/fold20.h5\n",
            "248/248 [==============================] - 36s 108ms/step - loss: 16.4272 - mae: 21.1636 - val_loss: 3.7557 - val_mae: 9.3391 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 3.4682 - mae: 8.6144\n",
            "Epoch 00002: val_loss improved from 3.75567 to 3.31961, saving model to /home/jupyter/BAMI/fold20.h5\n",
            "248/248 [==============================] - 23s 94ms/step - loss: 3.4682 - mae: 8.6144 - val_loss: 3.3196 - val_mae: 7.4767 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 3.1013 - mae: 6.4509\n",
            "Epoch 00003: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 23s 92ms/step - loss: 3.1013 - mae: 6.4509 - val_loss: 3.9579 - val_mae: 8.2954 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.9998 - mae: 5.5996\n",
            "Epoch 00004: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "248/248 [==============================] - 23s 92ms/step - loss: 2.9998 - mae: 5.5996 - val_loss: 5.7425 - val_mae: 13.2473 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.6729 - mae: 4.1812\n",
            "Epoch 00005: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 23s 92ms/step - loss: 2.6729 - mae: 4.1812 - val_loss: 4.7910 - val_mae: 8.8899 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.6016 - mae: 3.7112\n",
            "Epoch 00006: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "248/248 [==============================] - 23s 92ms/step - loss: 2.6016 - mae: 3.7112 - val_loss: 4.2357 - val_mae: 6.9408 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3967 - mae: 3.0933\n",
            "Epoch 00007: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 23s 92ms/step - loss: 2.3967 - mae: 3.0933 - val_loss: 6.1574 - val_mae: 10.2971 - lr: 2.5000e-04\n",
            "Epoch 8/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.3668 - mae: 2.9375\n",
            "Epoch 00008: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "248/248 [==============================] - 22s 91ms/step - loss: 2.3668 - mae: 2.9375 - val_loss: 3.9305 - val_mae: 5.3235 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.2895 - mae: 2.7254\n",
            "Epoch 00009: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 23s 91ms/step - loss: 2.2895 - mae: 2.7254 - val_loss: 3.8308 - val_mae: 5.1693 - lr: 1.2500e-04\n",
            "Epoch 10/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.2436 - mae: 2.5766\n",
            "Epoch 00010: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.2436 - mae: 2.5766 - val_loss: 4.2767 - val_mae: 5.6974 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.2126 - mae: 2.5517\n",
            "Epoch 00011: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.2126 - mae: 2.5517 - val_loss: 3.8381 - val_mae: 4.7666 - lr: 6.2500e-05\n",
            "Epoch 12/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1924 - mae: 2.4706\n",
            "Epoch 00012: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "248/248 [==============================] - 22s 88ms/step - loss: 2.1924 - mae: 2.4706 - val_loss: 3.7902 - val_mae: 4.7225 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1543 - mae: 2.3844\n",
            "Epoch 00013: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.1543 - mae: 2.3844 - val_loss: 3.8152 - val_mae: 4.7495 - lr: 3.1250e-05\n",
            "Epoch 14/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1613 - mae: 2.3615\n",
            "Epoch 00014: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.1613 - mae: 2.3615 - val_loss: 3.7668 - val_mae: 4.7692 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1413 - mae: 2.3754\n",
            "Epoch 00015: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.1413 - mae: 2.3754 - val_loss: 4.3103 - val_mae: 5.3793 - lr: 1.5625e-05\n",
            "Epoch 16/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1513 - mae: 2.3482\n",
            "Epoch 00016: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.1513 - mae: 2.3482 - val_loss: 4.1112 - val_mae: 5.1310 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1412 - mae: 2.3122\n",
            "Epoch 00017: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.1412 - mae: 2.3122 - val_loss: 4.5344 - val_mae: 5.6242 - lr: 7.8125e-06\n",
            "Epoch 18/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1344 - mae: 2.3211\n",
            "Epoch 00018: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.1344 - mae: 2.3211 - val_loss: 4.3777 - val_mae: 5.4442 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1317 - mae: 2.3245\n",
            "Epoch 00019: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.1317 - mae: 2.3245 - val_loss: 4.7092 - val_mae: 5.8747 - lr: 3.9063e-06\n",
            "Epoch 20/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1249 - mae: 2.3048\n",
            "Epoch 00020: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.1249 - mae: 2.3048 - val_loss: 4.5299 - val_mae: 5.7002 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1272 - mae: 2.2589\n",
            "Epoch 00021: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.1272 - mae: 2.2589 - val_loss: 4.7227 - val_mae: 5.8683 - lr: 1.9531e-06\n",
            "Epoch 22/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1301 - mae: 2.3133\n",
            "Epoch 00022: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "248/248 [==============================] - 22s 89ms/step - loss: 2.1301 - mae: 2.3133 - val_loss: 4.3431 - val_mae: 5.3494 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1136 - mae: 2.2830\n",
            "Epoch 00023: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 22s 87ms/step - loss: 2.1136 - mae: 2.2830 - val_loss: 4.5661 - val_mae: 5.7217 - lr: 9.7656e-07\n",
            "Epoch 24/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1209 - mae: 2.3036\n",
            "Epoch 00024: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "248/248 [==============================] - 22s 88ms/step - loss: 2.1209 - mae: 2.3036 - val_loss: 4.0313 - val_mae: 4.9410 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1239 - mae: 2.2770\n",
            "Epoch 00025: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 22s 87ms/step - loss: 2.1239 - mae: 2.2770 - val_loss: 4.3751 - val_mae: 5.5256 - lr: 4.8828e-07\n",
            "Epoch 26/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1138 - mae: 2.2913\n",
            "Epoch 00026: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "248/248 [==============================] - 22s 88ms/step - loss: 2.1138 - mae: 2.2913 - val_loss: 4.4601 - val_mae: 5.3672 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1204 - mae: 2.2884\n",
            "Epoch 00027: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 22s 87ms/step - loss: 2.1204 - mae: 2.2884 - val_loss: 4.3838 - val_mae: 5.4047 - lr: 2.4414e-07\n",
            "Epoch 28/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1250 - mae: 2.3045\n",
            "Epoch 00028: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "248/248 [==============================] - 22s 87ms/step - loss: 2.1250 - mae: 2.3045 - val_loss: 4.6000 - val_mae: 5.7077 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1320 - mae: 2.2997\n",
            "Epoch 00029: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 22s 87ms/step - loss: 2.1320 - mae: 2.2997 - val_loss: 4.4972 - val_mae: 5.6981 - lr: 1.2207e-07\n",
            "Epoch 30/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1217 - mae: 2.2888\n",
            "Epoch 00030: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "248/248 [==============================] - 22s 87ms/step - loss: 2.1217 - mae: 2.2888 - val_loss: 4.2583 - val_mae: 5.2114 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1148 - mae: 2.2711\n",
            "Epoch 00031: val_loss did not improve from 3.31961\n",
            "248/248 [==============================] - 22s 87ms/step - loss: 2.1148 - mae: 2.2711 - val_loss: 4.6453 - val_mae: 5.8071 - lr: 6.1035e-08\n",
            "Epoch 32/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 2.1108 - mae: 2.2614Restoring model weights from the end of the best epoch: 2.\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 3.31961\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "248/248 [==============================] - 22s 87ms/step - loss: 2.1108 - mae: 2.2614 - val_loss: 4.2307 - val_mae: 5.1812 - lr: 6.1035e-08\n",
            "Epoch 00032: early stopping\n",
            "10/10 [==============================] - 0s 35ms/step - loss: 4.3701 - mae: 11.5983\n",
            "Fit model on training data fold  21\n",
            "Epoch 1/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 15.7484 - mae: 24.6001\n",
            "Epoch 00001: val_loss improved from inf to 6.81259, saving model to /home/jupyter/BAMI/fold21.h5\n",
            "245/245 [==============================] - 36s 108ms/step - loss: 15.7484 - mae: 24.6001 - val_loss: 6.8126 - val_mae: 18.3797 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 3.3656 - mae: 8.4864\n",
            "Epoch 00002: val_loss improved from 6.81259 to 3.46678, saving model to /home/jupyter/BAMI/fold21.h5\n",
            "245/245 [==============================] - 23s 93ms/step - loss: 3.3656 - mae: 8.4864 - val_loss: 3.4668 - val_mae: 8.2412 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 3.1249 - mae: 6.6306\n",
            "Epoch 00003: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 22s 90ms/step - loss: 3.1249 - mae: 6.6306 - val_loss: 3.6069 - val_mae: 8.7142 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 3.0467 - mae: 5.8061\n",
            "Epoch 00004: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "245/245 [==============================] - 22s 90ms/step - loss: 3.0467 - mae: 5.8061 - val_loss: 4.6399 - val_mae: 10.6063 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.7088 - mae: 4.3486\n",
            "Epoch 00005: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 22s 90ms/step - loss: 2.7088 - mae: 4.3486 - val_loss: 3.6685 - val_mae: 6.3103 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.6611 - mae: 3.9056\n",
            "Epoch 00006: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "245/245 [==============================] - 22s 90ms/step - loss: 2.6611 - mae: 3.9056 - val_loss: 7.7297 - val_mae: 12.2519 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.4865 - mae: 3.3680\n",
            "Epoch 00007: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 22s 90ms/step - loss: 2.4865 - mae: 3.3680 - val_loss: 3.6448 - val_mae: 5.3629 - lr: 2.5000e-04\n",
            "Epoch 8/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.4261 - mae: 3.0789\n",
            "Epoch 00008: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "245/245 [==============================] - 22s 90ms/step - loss: 2.4261 - mae: 3.0789 - val_loss: 4.7395 - val_mae: 6.5727 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.3253 - mae: 2.7995\n",
            "Epoch 00009: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 22s 90ms/step - loss: 2.3253 - mae: 2.7995 - val_loss: 3.7072 - val_mae: 4.8602 - lr: 1.2500e-04\n",
            "Epoch 10/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.3050 - mae: 2.6931\n",
            "Epoch 00010: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "245/245 [==============================] - 22s 90ms/step - loss: 2.3048 - mae: 2.6924 - val_loss: 4.4305 - val_mae: 5.5842 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.2454 - mae: 2.5346\n",
            "Epoch 00011: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 22s 90ms/step - loss: 2.2454 - mae: 2.5346 - val_loss: 3.5790 - val_mae: 4.2365 - lr: 6.2500e-05\n",
            "Epoch 12/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.2154 - mae: 2.4842\n",
            "Epoch 00012: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "245/245 [==============================] - 22s 90ms/step - loss: 2.2152 - mae: 2.4837 - val_loss: 3.8864 - val_mae: 4.5933 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1919 - mae: 2.4076\n",
            "Epoch 00013: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 22s 88ms/step - loss: 2.1919 - mae: 2.4077 - val_loss: 3.8542 - val_mae: 4.5789 - lr: 3.1250e-05\n",
            "Epoch 14/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.1879 - mae: 2.3747\n",
            "Epoch 00014: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "245/245 [==============================] - 22s 88ms/step - loss: 2.1879 - mae: 2.3747 - val_loss: 3.7229 - val_mae: 4.3665 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.1786 - mae: 2.3594\n",
            "Epoch 00015: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 22s 88ms/step - loss: 2.1786 - mae: 2.3594 - val_loss: 3.9461 - val_mae: 4.7107 - lr: 1.5625e-05\n",
            "Epoch 16/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.1572 - mae: 2.3442\n",
            "Epoch 00016: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1572 - mae: 2.3442 - val_loss: 3.9056 - val_mae: 4.6162 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1585 - mae: 2.3289\n",
            "Epoch 00017: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1602 - mae: 2.3310 - val_loss: 4.6432 - val_mae: 5.5109 - lr: 7.8125e-06\n",
            "Epoch 18/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1421 - mae: 2.3245\n",
            "Epoch 00018: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1433 - mae: 2.3266 - val_loss: 6.4760 - val_mae: 7.6543 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1602 - mae: 2.3012\n",
            "Epoch 00019: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 21s 87ms/step - loss: 2.1607 - mae: 2.3024 - val_loss: 4.0783 - val_mae: 4.7268 - lr: 3.9063e-06\n",
            "Epoch 20/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1497 - mae: 2.2773\n",
            "Epoch 00020: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1495 - mae: 2.2770 - val_loss: 4.1924 - val_mae: 4.7668 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1436 - mae: 2.3171\n",
            "Epoch 00021: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1447 - mae: 2.3183 - val_loss: 4.4446 - val_mae: 5.1017 - lr: 1.9531e-06\n",
            "Epoch 22/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1415 - mae: 2.3086\n",
            "Epoch 00022: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1412 - mae: 2.3083 - val_loss: 4.1951 - val_mae: 4.8628 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1444 - mae: 2.2836\n",
            "Epoch 00023: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1448 - mae: 2.2840 - val_loss: 7.0731 - val_mae: 8.3999 - lr: 9.7656e-07\n",
            "Epoch 24/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1472 - mae: 2.3081\n",
            "Epoch 00024: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1471 - mae: 2.3076 - val_loss: 4.5222 - val_mae: 5.1872 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.1480 - mae: 2.2925\n",
            "Epoch 00025: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1480 - mae: 2.2925 - val_loss: 4.3791 - val_mae: 5.1484 - lr: 4.8828e-07\n",
            "Epoch 26/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1444 - mae: 2.2700\n",
            "Epoch 00026: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1442 - mae: 2.2696 - val_loss: 4.3159 - val_mae: 5.0810 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.1448 - mae: 2.2847\n",
            "Epoch 00027: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1448 - mae: 2.2847 - val_loss: 4.3464 - val_mae: 5.0591 - lr: 2.4414e-07\n",
            "Epoch 28/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1498 - mae: 2.3197\n",
            "Epoch 00028: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1498 - mae: 2.3196 - val_loss: 4.2257 - val_mae: 4.9755 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.1398 - mae: 2.2819\n",
            "Epoch 00029: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1398 - mae: 2.2819 - val_loss: 4.2128 - val_mae: 4.8240 - lr: 1.2207e-07\n",
            "Epoch 30/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1414 - mae: 2.2816\n",
            "Epoch 00030: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1418 - mae: 2.2826 - val_loss: 3.9687 - val_mae: 4.4169 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.1497 - mae: 2.2933\n",
            "Epoch 00031: val_loss did not improve from 3.46678\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1497 - mae: 2.2933 - val_loss: 6.8429 - val_mae: 7.9537 - lr: 6.1035e-08\n",
            "Epoch 32/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1387 - mae: 2.3009Restoring model weights from the end of the best epoch: 2.\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 3.46678\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1440 - mae: 2.3069 - val_loss: 4.2817 - val_mae: 4.8507 - lr: 6.1035e-08\n",
            "Epoch 00032: early stopping\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 2.9300 - mae: 6.1307\n",
            "Fit model on training data fold  22\n",
            "Epoch 1/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 15.3996 - mae: 26.9004\n",
            "Epoch 00001: val_loss improved from inf to 4.58506, saving model to /home/jupyter/BAMI/fold22.h5\n",
            "245/245 [==============================] - 37s 110ms/step - loss: 15.3937 - mae: 26.8917 - val_loss: 4.5851 - val_mae: 16.3878 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 3.4808 - mae: 9.7733\n",
            "Epoch 00002: val_loss improved from 4.58506 to 4.41484, saving model to /home/jupyter/BAMI/fold22.h5\n",
            "245/245 [==============================] - 23s 93ms/step - loss: 3.4808 - mae: 9.7733 - val_loss: 4.4148 - val_mae: 12.5721 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 3.1662 - mae: 7.0812\n",
            "Epoch 00003: val_loss improved from 4.41484 to 3.53768, saving model to /home/jupyter/BAMI/fold22.h5\n",
            "245/245 [==============================] - 23s 92ms/step - loss: 3.1662 - mae: 7.0812 - val_loss: 3.5377 - val_mae: 7.5501 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 3.1024 - mae: 6.2030\n",
            "Epoch 00004: val_loss improved from 3.53768 to 3.51060, saving model to /home/jupyter/BAMI/fold22.h5\n",
            "245/245 [==============================] - 22s 92ms/step - loss: 3.1024 - mae: 6.2030 - val_loss: 3.5106 - val_mae: 7.1416 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.9697 - mae: 5.3149\n",
            "Epoch 00005: val_loss did not improve from 3.51060\n",
            "245/245 [==============================] - 21s 87ms/step - loss: 2.9699 - mae: 5.3158 - val_loss: 3.6787 - val_mae: 7.0357 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.8872 - mae: 4.8722\n",
            "Epoch 00006: val_loss did not improve from 3.51060\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "245/245 [==============================] - 21s 87ms/step - loss: 2.8874 - mae: 4.8741 - val_loss: 3.5934 - val_mae: 6.7518 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.5719 - mae: 3.6526\n",
            "Epoch 00007: val_loss did not improve from 3.51060\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.5732 - mae: 3.6547 - val_loss: 4.4480 - val_mae: 7.2692 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.4989 - mae: 3.2818\n",
            "Epoch 00008: val_loss improved from 3.51060 to 3.20458, saving model to /home/jupyter/BAMI/fold22.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.4988 - mae: 3.2823 - val_loss: 3.2046 - val_mae: 4.4554 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.4789 - mae: 3.2176\n",
            "Epoch 00009: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 84ms/step - loss: 2.4788 - mae: 3.2181 - val_loss: 3.7273 - val_mae: 5.0529 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.4364 - mae: 3.1007\n",
            "Epoch 00010: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.4363 - mae: 3.1008 - val_loss: 4.9432 - val_mae: 6.5118 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.2611 - mae: 2.5870\n",
            "Epoch 00011: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.2620 - mae: 2.5892 - val_loss: 4.5020 - val_mae: 5.6735 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.2339 - mae: 2.5001\n",
            "Epoch 00012: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.2339 - mae: 2.5001 - val_loss: 4.0114 - val_mae: 4.9099 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1585 - mae: 2.3233\n",
            "Epoch 00013: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1585 - mae: 2.3229 - val_loss: 3.4719 - val_mae: 3.9402 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1483 - mae: 2.3195\n",
            "Epoch 00014: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.1483 - mae: 2.3197 - val_loss: 3.5770 - val_mae: 4.1588 - lr: 1.2500e-04\n",
            "Epoch 15/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.0990 - mae: 2.1883\n",
            "Epoch 00015: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0990 - mae: 2.1883 - val_loss: 3.4418 - val_mae: 3.7957 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1009 - mae: 2.1866\n",
            "Epoch 00016: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1017 - mae: 2.1877 - val_loss: 3.5289 - val_mae: 3.9076 - lr: 6.2500e-05\n",
            "Epoch 17/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0713 - mae: 2.1463\n",
            "Epoch 00017: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0721 - mae: 2.1466 - val_loss: 3.6342 - val_mae: 3.8746 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0653 - mae: 2.1308\n",
            "Epoch 00018: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0658 - mae: 2.1308 - val_loss: 4.3777 - val_mae: 4.6911 - lr: 3.1250e-05\n",
            "Epoch 19/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.0513 - mae: 2.0932\n",
            "Epoch 00019: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0513 - mae: 2.0932 - val_loss: 3.6078 - val_mae: 3.7832 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0488 - mae: 2.1021\n",
            "Epoch 00020: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0507 - mae: 2.1035 - val_loss: 4.9678 - val_mae: 5.2183 - lr: 1.5625e-05\n",
            "Epoch 21/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0409 - mae: 2.0629\n",
            "Epoch 00021: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0414 - mae: 2.0642 - val_loss: 4.0578 - val_mae: 4.3007 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0379 - mae: 2.0607\n",
            "Epoch 00022: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.0379 - mae: 2.0613 - val_loss: 3.2354 - val_mae: 3.4300 - lr: 7.8125e-06\n",
            "Epoch 23/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0384 - mae: 2.0668\n",
            "Epoch 00023: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0383 - mae: 2.0666 - val_loss: 4.3897 - val_mae: 4.5486 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0307 - mae: 2.0643\n",
            "Epoch 00024: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.0306 - mae: 2.0644 - val_loss: 3.5004 - val_mae: 3.7483 - lr: 3.9063e-06\n",
            "Epoch 25/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0287 - mae: 2.0592\n",
            "Epoch 00025: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.0286 - mae: 2.0588 - val_loss: 4.0273 - val_mae: 4.2557 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0185 - mae: 2.0561\n",
            "Epoch 00026: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0182 - mae: 2.0559 - val_loss: 4.1972 - val_mae: 4.4041 - lr: 1.9531e-06\n",
            "Epoch 27/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0317 - mae: 2.0524\n",
            "Epoch 00027: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0324 - mae: 2.0532 - val_loss: 3.9015 - val_mae: 4.1284 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0295 - mae: 2.0514\n",
            "Epoch 00028: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0299 - mae: 2.0514 - val_loss: 3.7949 - val_mae: 3.9182 - lr: 9.7656e-07\n",
            "Epoch 29/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0365 - mae: 2.0527\n",
            "Epoch 00029: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0365 - mae: 2.0525 - val_loss: 4.2281 - val_mae: 4.4231 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.0339 - mae: 2.0231\n",
            "Epoch 00030: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0339 - mae: 2.0231 - val_loss: 4.1316 - val_mae: 4.2927 - lr: 4.8828e-07\n",
            "Epoch 31/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0358 - mae: 2.0376\n",
            "Epoch 00031: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0359 - mae: 2.0384 - val_loss: 4.3765 - val_mae: 4.6652 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0392 - mae: 2.0523\n",
            "Epoch 00032: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0392 - mae: 2.0521 - val_loss: 4.4055 - val_mae: 4.7292 - lr: 2.4414e-07\n",
            "Epoch 33/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0338 - mae: 2.0495\n",
            "Epoch 00033: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0337 - mae: 2.0491 - val_loss: 3.3779 - val_mae: 3.4704 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0363 - mae: 2.0686\n",
            "Epoch 00034: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0367 - mae: 2.0688 - val_loss: 3.7532 - val_mae: 3.8666 - lr: 1.2207e-07\n",
            "Epoch 35/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.0351 - mae: 2.0568\n",
            "Epoch 00035: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0351 - mae: 2.0568 - val_loss: 10.8814 - val_mae: 11.0963 - lr: 6.1035e-08\n",
            "Epoch 36/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0332 - mae: 2.0443\n",
            "Epoch 00036: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0421 - mae: 2.0518 - val_loss: 3.5157 - val_mae: 3.6408 - lr: 6.1035e-08\n",
            "Epoch 37/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.0276 - mae: 2.0753\n",
            "Epoch 00037: val_loss did not improve from 3.20458\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0276 - mae: 2.0753 - val_loss: 3.6624 - val_mae: 3.7593 - lr: 3.0518e-08\n",
            "Epoch 38/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0293 - mae: 2.0531Restoring model weights from the end of the best epoch: 8.\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 3.20458\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0295 - mae: 2.0535 - val_loss: 3.8680 - val_mae: 3.9950 - lr: 3.0518e-08\n",
            "Epoch 00038: early stopping\n",
            "10/10 [==============================] - 0s 35ms/step - loss: 2.5051 - mae: 3.3842\n",
            "Fit model on training data fold  23\n",
            "Epoch 1/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 37.1167 - mae: 47.4189\n",
            "Epoch 00001: val_loss improved from inf to 4.18242, saving model to /home/jupyter/BAMI/fold23.h5\n",
            "245/245 [==============================] - 36s 108ms/step - loss: 37.1167 - mae: 47.4189 - val_loss: 4.1824 - val_mae: 13.4002 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 3.6561 - mae: 10.4261\n",
            "Epoch 00002: val_loss improved from 4.18242 to 4.04965, saving model to /home/jupyter/BAMI/fold23.h5\n",
            "245/245 [==============================] - 22s 90ms/step - loss: 3.6564 - mae: 10.4284 - val_loss: 4.0497 - val_mae: 12.4470 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 3.3874 - mae: 8.4378\n",
            "Epoch 00003: val_loss improved from 4.04965 to 3.26691, saving model to /home/jupyter/BAMI/fold23.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 3.3872 - mae: 8.4385 - val_loss: 3.2669 - val_mae: 7.6657 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 3.1969 - mae: 6.8100\n",
            "Epoch 00004: val_loss did not improve from 3.26691\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 3.1968 - mae: 6.8117 - val_loss: 3.3094 - val_mae: 6.9358 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 3.0279 - mae: 5.6210\n",
            "Epoch 00005: val_loss did not improve from 3.26691\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 3.0279 - mae: 5.6210 - val_loss: 3.8048 - val_mae: 7.9333 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.7549 - mae: 4.4362\n",
            "Epoch 00006: val_loss improved from 3.26691 to 3.25352, saving model to /home/jupyter/BAMI/fold23.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.7549 - mae: 4.4362 - val_loss: 3.2535 - val_mae: 5.3679 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.6589 - mae: 3.8725\n",
            "Epoch 00007: val_loss improved from 3.25352 to 2.64632, saving model to /home/jupyter/BAMI/fold23.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.6591 - mae: 3.8724 - val_loss: 2.6463 - val_mae: 3.9533 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.5956 - mae: 3.7434\n",
            "Epoch 00008: val_loss improved from 2.64632 to 2.51315, saving model to /home/jupyter/BAMI/fold23.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.5955 - mae: 3.7432 - val_loss: 2.5132 - val_mae: 3.3349 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.5288 - mae: 3.3942\n",
            "Epoch 00009: val_loss did not improve from 2.51315\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.5286 - mae: 3.3935 - val_loss: 2.5197 - val_mae: 3.2611 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.4805 - mae: 3.2917\n",
            "Epoch 00010: val_loss did not improve from 2.51315\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.4806 - mae: 3.2908 - val_loss: 2.6418 - val_mae: 3.2899 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.3399 - mae: 2.8429\n",
            "Epoch 00011: val_loss did not improve from 2.51315\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.3400 - mae: 2.8430 - val_loss: 2.6999 - val_mae: 3.2888 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.2897 - mae: 2.7152\n",
            "Epoch 00012: val_loss improved from 2.51315 to 2.32616, saving model to /home/jupyter/BAMI/fold23.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.2899 - mae: 2.7148 - val_loss: 2.3262 - val_mae: 2.6297 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.2637 - mae: 2.6017\n",
            "Epoch 00013: val_loss improved from 2.32616 to 2.29168, saving model to /home/jupyter/BAMI/fold23.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.2644 - mae: 2.6026 - val_loss: 2.2917 - val_mae: 2.6762 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.2528 - mae: 2.6104\n",
            "Epoch 00014: val_loss improved from 2.29168 to 2.23877, saving model to /home/jupyter/BAMI/fold23.h5\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 2.2533 - mae: 2.6113 - val_loss: 2.2388 - val_mae: 2.6183 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.2147 - mae: 2.5355\n",
            "Epoch 00015: val_loss did not improve from 2.23877\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.2148 - mae: 2.5359 - val_loss: 2.9064 - val_mae: 3.3383 - lr: 2.5000e-04\n",
            "Epoch 16/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.2265 - mae: 2.5580\n",
            "Epoch 00016: val_loss did not improve from 2.23877\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.2280 - mae: 2.5598 - val_loss: 2.3096 - val_mae: 2.5706 - lr: 2.5000e-04\n",
            "Epoch 17/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1434 - mae: 2.3749\n",
            "Epoch 00017: val_loss did not improve from 2.23877\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1436 - mae: 2.3750 - val_loss: 2.3116 - val_mae: 2.4418 - lr: 1.2500e-04\n",
            "Epoch 18/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.1115 - mae: 2.2843\n",
            "Epoch 00018: val_loss did not improve from 2.23877\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.1113 - mae: 2.2842 - val_loss: 2.2770 - val_mae: 2.4014 - lr: 1.2500e-04\n",
            "Epoch 19/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0789 - mae: 2.2078\n",
            "Epoch 00019: val_loss did not improve from 2.23877\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0788 - mae: 2.2074 - val_loss: 2.2439 - val_mae: 2.3895 - lr: 6.2500e-05\n",
            "Epoch 20/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0524 - mae: 2.1266\n",
            "Epoch 00020: val_loss improved from 2.23877 to 2.23010, saving model to /home/jupyter/BAMI/fold23.h5\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0525 - mae: 2.1264 - val_loss: 2.2301 - val_mae: 2.1682 - lr: 6.2500e-05\n",
            "Epoch 21/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0338 - mae: 2.1147\n",
            "Epoch 00021: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0344 - mae: 2.1144 - val_loss: 2.3003 - val_mae: 2.3449 - lr: 6.2500e-05\n",
            "Epoch 22/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.0402 - mae: 2.1193\n",
            "Epoch 00022: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0402 - mae: 2.1193 - val_loss: 2.3776 - val_mae: 2.3010 - lr: 6.2500e-05\n",
            "Epoch 23/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 2.0133 - mae: 2.0911\n",
            "Epoch 00023: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0133 - mae: 2.0911 - val_loss: 2.2430 - val_mae: 2.3184 - lr: 3.1250e-05\n",
            "Epoch 24/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 2.0133 - mae: 2.0551\n",
            "Epoch 00024: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 2.0133 - mae: 2.0551 - val_loss: 2.3044 - val_mae: 2.3339 - lr: 3.1250e-05\n",
            "Epoch 25/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9938 - mae: 2.0294\n",
            "Epoch 00025: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9946 - mae: 2.0303 - val_loss: 2.2581 - val_mae: 2.2133 - lr: 1.5625e-05\n",
            "Epoch 26/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9933 - mae: 2.0642\n",
            "Epoch 00026: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9934 - mae: 2.0642 - val_loss: 2.3089 - val_mae: 2.3428 - lr: 1.5625e-05\n",
            "Epoch 27/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9828 - mae: 2.0206\n",
            "Epoch 00027: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9830 - mae: 2.0215 - val_loss: 2.3039 - val_mae: 2.2277 - lr: 7.8125e-06\n",
            "Epoch 28/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9876 - mae: 2.0017\n",
            "Epoch 00028: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9875 - mae: 2.0022 - val_loss: 2.2721 - val_mae: 2.1045 - lr: 7.8125e-06\n",
            "Epoch 29/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 1.9760 - mae: 1.9912\n",
            "Epoch 00029: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9760 - mae: 1.9912 - val_loss: 2.2625 - val_mae: 2.1899 - lr: 3.9063e-06\n",
            "Epoch 30/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9790 - mae: 1.9863\n",
            "Epoch 00030: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9803 - mae: 1.9881 - val_loss: 2.3456 - val_mae: 2.2828 - lr: 3.9063e-06\n",
            "Epoch 31/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9876 - mae: 2.0172\n",
            "Epoch 00031: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9878 - mae: 2.0170 - val_loss: 2.2868 - val_mae: 2.2165 - lr: 1.9531e-06\n",
            "Epoch 32/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9749 - mae: 2.0179\n",
            "Epoch 00032: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9754 - mae: 2.0182 - val_loss: 2.3066 - val_mae: 2.2647 - lr: 1.9531e-06\n",
            "Epoch 33/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 1.9727 - mae: 1.9736\n",
            "Epoch 00033: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9727 - mae: 1.9736 - val_loss: 2.2353 - val_mae: 2.2349 - lr: 9.7656e-07\n",
            "Epoch 34/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9721 - mae: 2.0097\n",
            "Epoch 00034: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9731 - mae: 2.0100 - val_loss: 2.2843 - val_mae: 2.1205 - lr: 9.7656e-07\n",
            "Epoch 35/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9789 - mae: 2.0085\n",
            "Epoch 00035: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9789 - mae: 2.0092 - val_loss: 2.2445 - val_mae: 2.1652 - lr: 4.8828e-07\n",
            "Epoch 36/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9727 - mae: 1.9894\n",
            "Epoch 00036: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9737 - mae: 1.9908 - val_loss: 2.2971 - val_mae: 2.2829 - lr: 4.8828e-07\n",
            "Epoch 37/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9836 - mae: 1.9738\n",
            "Epoch 00037: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9839 - mae: 1.9743 - val_loss: 2.3610 - val_mae: 2.2334 - lr: 2.4414e-07\n",
            "Epoch 38/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9784 - mae: 1.9829\n",
            "Epoch 00038: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9782 - mae: 1.9828 - val_loss: 2.2469 - val_mae: 2.2270 - lr: 2.4414e-07\n",
            "Epoch 39/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9796 - mae: 2.0094\n",
            "Epoch 00039: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9803 - mae: 2.0101 - val_loss: 2.3207 - val_mae: 2.2374 - lr: 1.2207e-07\n",
            "Epoch 40/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 1.9661 - mae: 1.9789\n",
            "Epoch 00040: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9661 - mae: 1.9789 - val_loss: 2.2691 - val_mae: 2.2362 - lr: 1.2207e-07\n",
            "Epoch 41/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9771 - mae: 1.9994\n",
            "Epoch 00041: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9772 - mae: 1.9991 - val_loss: 2.3216 - val_mae: 2.2740 - lr: 6.1035e-08\n",
            "Epoch 42/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9716 - mae: 1.9967\n",
            "Epoch 00042: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9715 - mae: 1.9963 - val_loss: 2.3061 - val_mae: 2.2916 - lr: 6.1035e-08\n",
            "Epoch 43/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9829 - mae: 2.0003\n",
            "Epoch 00043: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9829 - mae: 2.0004 - val_loss: 2.2995 - val_mae: 2.3022 - lr: 3.0518e-08\n",
            "Epoch 44/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9821 - mae: 2.0085\n",
            "Epoch 00044: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9827 - mae: 2.0086 - val_loss: 2.2964 - val_mae: 2.2037 - lr: 3.0518e-08\n",
            "Epoch 45/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 1.9754 - mae: 2.0253\n",
            "Epoch 00045: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 85ms/step - loss: 1.9754 - mae: 2.0253 - val_loss: 2.3979 - val_mae: 2.2645 - lr: 1.5259e-08\n",
            "Epoch 46/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9809 - mae: 1.9724\n",
            "Epoch 00046: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9807 - mae: 1.9729 - val_loss: 2.2738 - val_mae: 2.2758 - lr: 1.5259e-08\n",
            "Epoch 47/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9782 - mae: 1.9730\n",
            "Epoch 00047: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9782 - mae: 1.9730 - val_loss: 2.3043 - val_mae: 2.2227 - lr: 7.6294e-09\n",
            "Epoch 48/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9731 - mae: 2.0024\n",
            "Epoch 00048: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00048: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9735 - mae: 2.0028 - val_loss: 2.3224 - val_mae: 2.3103 - lr: 7.6294e-09\n",
            "Epoch 49/200\n",
            "244/245 [============================>.] - ETA: 0s - loss: 1.9779 - mae: 2.0143\n",
            "Epoch 00049: val_loss did not improve from 2.23010\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9780 - mae: 2.0140 - val_loss: 2.2698 - val_mae: 2.2568 - lr: 3.8147e-09\n",
            "Epoch 50/200\n",
            "245/245 [==============================] - ETA: 0s - loss: 1.9787 - mae: 1.9948Restoring model weights from the end of the best epoch: 20.\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 2.23010\n",
            "\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
            "245/245 [==============================] - 21s 86ms/step - loss: 1.9787 - mae: 1.9948 - val_loss: 2.3217 - val_mae: 2.2506 - lr: 3.8147e-09\n",
            "Epoch 00050: early stopping\n",
            "10/10 [==============================] - 0s 34ms/step - loss: 5.1594 - mae: 5.7707\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2160x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval Mean 6.812225692442855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DaLia_Data_PPG, DaLia_Data_ACC, DaLia_Data_truth = preprocess_dataset(DaLia_Data)\n",
        "train_model_loso(DaLia_Data, DaLia_Data_PPG, DaLia_Data_ACC, DaLia_Data_truth, \"DaLia\")"
      ],
      "metadata": {
        "id": "gaPvD01SZk47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce1afee1-2ca3-4cfc-b52e-da3968763710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fit model on training data fold  0\n",
            "Epoch 1/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 4.7019 - mae: 9.9693\n",
            "Epoch 00001: val_loss improved from inf to 3.66282, saving model to /home/jupyter/DaLia/fold0.h5\n",
            "1640/1640 [==============================] - 161s 93ms/step - loss: 4.7019 - mae: 9.9693 - val_loss: 3.6628 - val_mae: 8.6621 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 3.0648 - mae: 6.1985\n",
            "Epoch 00002: val_loss improved from 3.66282 to 3.44190, saving model to /home/jupyter/DaLia/fold0.h5\n",
            "1640/1640 [==============================] - 145s 88ms/step - loss: 3.0648 - mae: 6.1985 - val_loss: 3.4419 - val_mae: 8.6669 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.7417 - mae: 4.9905\n",
            "Epoch 00003: val_loss improved from 3.44190 to 3.00865, saving model to /home/jupyter/DaLia/fold0.h5\n",
            "1640/1640 [==============================] - 155s 95ms/step - loss: 2.7417 - mae: 4.9905 - val_loss: 3.0087 - val_mae: 7.2043 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.5436 - mae: 4.2862\n",
            "Epoch 00004: val_loss improved from 3.00865 to 2.87543, saving model to /home/jupyter/DaLia/fold0.h5\n",
            "1640/1640 [==============================] - 157s 96ms/step - loss: 2.5436 - mae: 4.2862 - val_loss: 2.8754 - val_mae: 6.3480 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.4194 - mae: 3.8158\n",
            "Epoch 00005: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 154s 94ms/step - loss: 2.4194 - mae: 3.8158 - val_loss: 2.9086 - val_mae: 6.0805 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.3317 - mae: 3.4908\n",
            "Epoch 00006: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1640/1640 [==============================] - 151s 92ms/step - loss: 2.3317 - mae: 3.4908 - val_loss: 2.8771 - val_mae: 6.4091 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.1634 - mae: 2.9613\n",
            "Epoch 00007: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 152s 93ms/step - loss: 2.1634 - mae: 2.9613 - val_loss: 2.9523 - val_mae: 5.9352 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.0993 - mae: 2.7440\n",
            "Epoch 00008: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1640/1640 [==============================] - 145s 88ms/step - loss: 2.0993 - mae: 2.7440 - val_loss: 3.0041 - val_mae: 5.7052 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.9991 - mae: 2.4718\n",
            "Epoch 00009: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 151s 92ms/step - loss: 1.9991 - mae: 2.4718 - val_loss: 2.9851 - val_mae: 5.3932 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.9621 - mae: 2.3561\n",
            "Epoch 00010: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1640/1640 [==============================] - 149s 91ms/step - loss: 1.9621 - mae: 2.3561 - val_loss: 3.0682 - val_mae: 5.1970 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.9107 - mae: 2.2225\n",
            "Epoch 00011: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 144s 88ms/step - loss: 1.9107 - mae: 2.2225 - val_loss: 3.0883 - val_mae: 5.3384 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8798 - mae: 2.1507\n",
            "Epoch 00012: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1640/1640 [==============================] - 152s 93ms/step - loss: 1.8798 - mae: 2.1507 - val_loss: 3.1699 - val_mae: 5.3721 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8554 - mae: 2.1020\n",
            "Epoch 00013: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 153s 93ms/step - loss: 1.8554 - mae: 2.1020 - val_loss: 3.1511 - val_mae: 5.1914 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8434 - mae: 2.0592\n",
            "Epoch 00014: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1640/1640 [==============================] - 154s 94ms/step - loss: 1.8434 - mae: 2.0592 - val_loss: 3.1794 - val_mae: 5.2482 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8287 - mae: 2.0447\n",
            "Epoch 00015: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 153s 93ms/step - loss: 1.8287 - mae: 2.0447 - val_loss: 3.1576 - val_mae: 5.1033 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8205 - mae: 2.0255\n",
            "Epoch 00016: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1640/1640 [==============================] - 151s 92ms/step - loss: 1.8205 - mae: 2.0255 - val_loss: 3.1824 - val_mae: 5.2080 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8113 - mae: 2.0079\n",
            "Epoch 00017: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 148s 90ms/step - loss: 1.8113 - mae: 2.0079 - val_loss: 3.2237 - val_mae: 5.2155 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8089 - mae: 1.9938\n",
            "Epoch 00018: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1640/1640 [==============================] - 143s 87ms/step - loss: 1.8089 - mae: 1.9938 - val_loss: 3.2036 - val_mae: 5.2401 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8049 - mae: 1.9973\n",
            "Epoch 00019: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 151s 92ms/step - loss: 1.8049 - mae: 1.9973 - val_loss: 3.2369 - val_mae: 5.1555 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8031 - mae: 1.9661\n",
            "Epoch 00020: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1640/1640 [==============================] - 149s 91ms/step - loss: 1.8031 - mae: 1.9661 - val_loss: 3.2592 - val_mae: 5.1937 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8028 - mae: 1.9731\n",
            "Epoch 00021: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 151s 92ms/step - loss: 1.8028 - mae: 1.9731 - val_loss: 3.2634 - val_mae: 5.2494 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8011 - mae: 1.9831\n",
            "Epoch 00022: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1640/1640 [==============================] - 149s 91ms/step - loss: 1.8011 - mae: 1.9831 - val_loss: 3.2221 - val_mae: 5.1551 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8004 - mae: 1.9814\n",
            "Epoch 00023: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 150s 92ms/step - loss: 1.8004 - mae: 1.9814 - val_loss: 3.2154 - val_mae: 5.1523 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7959 - mae: 1.9681\n",
            "Epoch 00024: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1640/1640 [==============================] - 144s 88ms/step - loss: 1.7959 - mae: 1.9681 - val_loss: 3.2316 - val_mae: 5.2233 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7960 - mae: 1.9772\n",
            "Epoch 00025: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 151s 92ms/step - loss: 1.7960 - mae: 1.9772 - val_loss: 3.2407 - val_mae: 5.1175 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8007 - mae: 1.9792\n",
            "Epoch 00026: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1640/1640 [==============================] - 141s 86ms/step - loss: 1.8007 - mae: 1.9792 - val_loss: 3.2386 - val_mae: 5.1689 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7959 - mae: 1.9716\n",
            "Epoch 00027: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 142s 87ms/step - loss: 1.7959 - mae: 1.9716 - val_loss: 3.2327 - val_mae: 5.2300 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7966 - mae: 1.9654\n",
            "Epoch 00028: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1640/1640 [==============================] - 147s 90ms/step - loss: 1.7966 - mae: 1.9654 - val_loss: 3.2176 - val_mae: 5.1619 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7989 - mae: 1.9718\n",
            "Epoch 00029: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 139s 85ms/step - loss: 1.7989 - mae: 1.9718 - val_loss: 3.2054 - val_mae: 5.2029 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8018 - mae: 1.9632\n",
            "Epoch 00030: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1640/1640 [==============================] - 142s 87ms/step - loss: 1.8018 - mae: 1.9632 - val_loss: 3.2263 - val_mae: 5.1529 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7970 - mae: 1.9642\n",
            "Epoch 00031: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 147s 89ms/step - loss: 1.7970 - mae: 1.9642 - val_loss: 3.2226 - val_mae: 5.1936 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7978 - mae: 1.9759\n",
            "Epoch 00032: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1640/1640 [==============================] - 149s 91ms/step - loss: 1.7978 - mae: 1.9759 - val_loss: 3.2289 - val_mae: 5.1311 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8014 - mae: 1.9637\n",
            "Epoch 00033: val_loss did not improve from 2.87543\n",
            "1640/1640 [==============================] - 144s 88ms/step - loss: 1.8014 - mae: 1.9637 - val_loss: 3.2212 - val_mae: 5.2057 - lr: 6.1035e-08\n",
            "Epoch 34/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7981 - mae: 1.9685Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 2.87543\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1640/1640 [==============================] - 143s 87ms/step - loss: 1.7981 - mae: 1.9685 - val_loss: 3.2103 - val_mae: 5.1888 - lr: 6.1035e-08\n",
            "Epoch 00034: early stopping\n",
            "82/82 [==============================] - 3s 32ms/step - loss: 2.6815 - mae: 5.2936\n",
            "Fit model on training data fold  1\n",
            "Epoch 1/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 4.8276 - mae: 13.1773\n",
            "Epoch 00001: val_loss improved from inf to 3.20218, saving model to /home/jupyter/DaLia/fold1.h5\n",
            "1640/1640 [==============================] - 152s 87ms/step - loss: 4.8276 - mae: 13.1773 - val_loss: 3.2022 - val_mae: 7.4361 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 3.0184 - mae: 5.9386\n",
            "Epoch 00002: val_loss improved from 3.20218 to 2.78360, saving model to /home/jupyter/DaLia/fold1.h5\n",
            "1640/1640 [==============================] - 142s 87ms/step - loss: 3.0184 - mae: 5.9386 - val_loss: 2.7836 - val_mae: 5.6839 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.6830 - mae: 4.5709\n",
            "Epoch 00003: val_loss improved from 2.78360 to 2.56142, saving model to /home/jupyter/DaLia/fold1.h5\n",
            "1640/1640 [==============================] - 140s 85ms/step - loss: 2.6830 - mae: 4.5709 - val_loss: 2.5614 - val_mae: 4.7971 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.4960 - mae: 3.9190\n",
            "Epoch 00004: val_loss did not improve from 2.56142\n",
            "1640/1640 [==============================] - 143s 87ms/step - loss: 2.4960 - mae: 3.9190 - val_loss: 2.6848 - val_mae: 4.9489 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.3869 - mae: 3.5617\n",
            "Epoch 00005: val_loss improved from 2.56142 to 2.47148, saving model to /home/jupyter/DaLia/fold1.h5\n",
            "1640/1640 [==============================] - 149s 91ms/step - loss: 2.3869 - mae: 3.5617 - val_loss: 2.4715 - val_mae: 4.5383 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.2990 - mae: 3.2488\n",
            "Epoch 00006: val_loss improved from 2.47148 to 2.38950, saving model to /home/jupyter/DaLia/fold1.h5\n",
            "1640/1640 [==============================] - 144s 88ms/step - loss: 2.2990 - mae: 3.2488 - val_loss: 2.3895 - val_mae: 4.4002 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.2278 - mae: 3.0300\n",
            "Epoch 00007: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 142s 87ms/step - loss: 2.2278 - mae: 3.0300 - val_loss: 2.8552 - val_mae: 4.3037 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.1810 - mae: 2.8588\n",
            "Epoch 00008: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1640/1640 [==============================] - 145s 88ms/step - loss: 2.1810 - mae: 2.8588 - val_loss: 2.4032 - val_mae: 3.9723 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.0297 - mae: 2.4370\n",
            "Epoch 00009: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 150s 91ms/step - loss: 2.0297 - mae: 2.4370 - val_loss: 2.4031 - val_mae: 3.6702 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.9793 - mae: 2.3022\n",
            "Epoch 00010: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1640/1640 [==============================] - 146s 89ms/step - loss: 1.9793 - mae: 2.3022 - val_loss: 2.6015 - val_mae: 3.6527 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8941 - mae: 2.0967\n",
            "Epoch 00011: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 149s 91ms/step - loss: 1.8941 - mae: 2.0967 - val_loss: 2.4526 - val_mae: 3.6131 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8633 - mae: 2.0307\n",
            "Epoch 00012: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1640/1640 [==============================] - 146s 89ms/step - loss: 1.8633 - mae: 2.0307 - val_loss: 2.5566 - val_mae: 3.6798 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8166 - mae: 1.9164\n",
            "Epoch 00013: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 142s 87ms/step - loss: 1.8166 - mae: 1.9164 - val_loss: 2.5671 - val_mae: 3.4593 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7990 - mae: 1.8848\n",
            "Epoch 00014: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1640/1640 [==============================] - 149s 91ms/step - loss: 1.7990 - mae: 1.8848 - val_loss: 2.5382 - val_mae: 3.5016 - lr: 1.2500e-04\n",
            "Epoch 15/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7734 - mae: 1.8461\n",
            "Epoch 00015: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 148s 91ms/step - loss: 1.7734 - mae: 1.8461 - val_loss: 2.5409 - val_mae: 3.4843 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7611 - mae: 1.8105\n",
            "Epoch 00016: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1640/1640 [==============================] - 149s 91ms/step - loss: 1.7611 - mae: 1.8105 - val_loss: 2.5870 - val_mae: 3.3830 - lr: 6.2500e-05\n",
            "Epoch 17/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7429 - mae: 1.7803\n",
            "Epoch 00017: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 151s 92ms/step - loss: 1.7429 - mae: 1.7803 - val_loss: 2.6216 - val_mae: 3.3466 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7449 - mae: 1.7791\n",
            "Epoch 00018: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1640/1640 [==============================] - 151s 92ms/step - loss: 1.7449 - mae: 1.7791 - val_loss: 2.6425 - val_mae: 3.3977 - lr: 3.1250e-05\n",
            "Epoch 19/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7378 - mae: 1.7738\n",
            "Epoch 00019: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 151s 92ms/step - loss: 1.7378 - mae: 1.7738 - val_loss: 2.6387 - val_mae: 3.3643 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7320 - mae: 1.7721\n",
            "Epoch 00020: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1640/1640 [==============================] - 151s 92ms/step - loss: 1.7320 - mae: 1.7721 - val_loss: 2.5868 - val_mae: 3.2840 - lr: 1.5625e-05\n",
            "Epoch 21/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7263 - mae: 1.7545\n",
            "Epoch 00021: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 146s 89ms/step - loss: 1.7263 - mae: 1.7545 - val_loss: 2.6186 - val_mae: 3.3666 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7273 - mae: 1.7512\n",
            "Epoch 00022: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1640/1640 [==============================] - 145s 88ms/step - loss: 1.7273 - mae: 1.7512 - val_loss: 2.6256 - val_mae: 3.2693 - lr: 7.8125e-06\n",
            "Epoch 23/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7223 - mae: 1.7451\n",
            "Epoch 00023: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 147s 90ms/step - loss: 1.7223 - mae: 1.7451 - val_loss: 2.6185 - val_mae: 3.3272 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7291 - mae: 1.7489\n",
            "Epoch 00024: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1640/1640 [==============================] - 150s 91ms/step - loss: 1.7291 - mae: 1.7489 - val_loss: 2.6067 - val_mae: 3.3278 - lr: 3.9063e-06\n",
            "Epoch 25/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7235 - mae: 1.7435\n",
            "Epoch 00025: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 145s 88ms/step - loss: 1.7235 - mae: 1.7435 - val_loss: 2.6191 - val_mae: 3.3592 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7211 - mae: 1.7415\n",
            "Epoch 00026: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1640/1640 [==============================] - 140s 85ms/step - loss: 1.7211 - mae: 1.7415 - val_loss: 2.6291 - val_mae: 3.2853 - lr: 1.9531e-06\n",
            "Epoch 27/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7190 - mae: 1.7491\n",
            "Epoch 00027: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 143s 87ms/step - loss: 1.7190 - mae: 1.7491 - val_loss: 2.6759 - val_mae: 3.4797 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7234 - mae: 1.7539\n",
            "Epoch 00028: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1640/1640 [==============================] - 147s 90ms/step - loss: 1.7234 - mae: 1.7539 - val_loss: 2.7004 - val_mae: 3.4299 - lr: 9.7656e-07\n",
            "Epoch 29/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7189 - mae: 1.7339\n",
            "Epoch 00029: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 143s 87ms/step - loss: 1.7189 - mae: 1.7339 - val_loss: 2.6502 - val_mae: 3.3242 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7200 - mae: 1.7512\n",
            "Epoch 00030: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1640/1640 [==============================] - 152s 92ms/step - loss: 1.7200 - mae: 1.7512 - val_loss: 2.6675 - val_mae: 3.2928 - lr: 4.8828e-07\n",
            "Epoch 31/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7241 - mae: 1.7494\n",
            "Epoch 00031: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 147s 90ms/step - loss: 1.7241 - mae: 1.7494 - val_loss: 2.6447 - val_mae: 3.3831 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7224 - mae: 1.7505\n",
            "Epoch 00032: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1640/1640 [==============================] - 144s 88ms/step - loss: 1.7224 - mae: 1.7505 - val_loss: 2.6116 - val_mae: 3.2798 - lr: 2.4414e-07\n",
            "Epoch 33/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7251 - mae: 1.7562\n",
            "Epoch 00033: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 147s 90ms/step - loss: 1.7251 - mae: 1.7562 - val_loss: 2.6221 - val_mae: 3.3448 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7217 - mae: 1.7401\n",
            "Epoch 00034: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.7217 - mae: 1.7401 - val_loss: 2.6301 - val_mae: 3.3118 - lr: 1.2207e-07\n",
            "Epoch 35/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7187 - mae: 1.7387\n",
            "Epoch 00035: val_loss did not improve from 2.38950\n",
            "1640/1640 [==============================] - 143s 87ms/step - loss: 1.7187 - mae: 1.7387 - val_loss: 2.6582 - val_mae: 3.3559 - lr: 6.1035e-08\n",
            "Epoch 36/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.7221 - mae: 1.7455Restoring model weights from the end of the best epoch: 6.\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 2.38950\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1640/1640 [==============================] - 149s 91ms/step - loss: 1.7221 - mae: 1.7455 - val_loss: 2.6732 - val_mae: 3.3914 - lr: 6.1035e-08\n",
            "Epoch 00036: early stopping\n",
            "134/134 [==============================] - 4s 32ms/step - loss: 3.7197 - mae: 9.2764\n",
            "Fit model on training data fold  2\n",
            "Epoch 1/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 5.2342 - mae: 11.6368\n",
            "Epoch 00001: val_loss improved from inf to 3.84928, saving model to /home/jupyter/DaLia/fold2.h5\n",
            "1640/1640 [==============================] - 160s 92ms/step - loss: 5.2342 - mae: 11.6368 - val_loss: 3.8493 - val_mae: 9.4837 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 3.1325 - mae: 6.3956\n",
            "Epoch 00002: val_loss improved from 3.84928 to 3.72453, saving model to /home/jupyter/DaLia/fold2.h5\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 3.1325 - mae: 6.3956 - val_loss: 3.7245 - val_mae: 8.1459 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.8168 - mae: 4.8743\n",
            "Epoch 00003: val_loss improved from 3.72453 to 3.49044, saving model to /home/jupyter/DaLia/fold2.h5\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 2.8168 - mae: 4.8743 - val_loss: 3.4904 - val_mae: 7.7298 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.5521 - mae: 4.0812\n",
            "Epoch 00004: val_loss improved from 3.49044 to 3.18329, saving model to /home/jupyter/DaLia/fold2.h5\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 2.5521 - mae: 4.0812 - val_loss: 3.1833 - val_mae: 6.9258 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.4179 - mae: 3.6753\n",
            "Epoch 00005: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 2.4179 - mae: 3.6753 - val_loss: 3.3485 - val_mae: 7.2177 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.3312 - mae: 3.4065\n",
            "Epoch 00006: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1640/1640 [==============================] - 137s 84ms/step - loss: 2.3312 - mae: 3.4065 - val_loss: 3.3590 - val_mae: 7.3139 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.1593 - mae: 2.8665\n",
            "Epoch 00007: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 137s 84ms/step - loss: 2.1593 - mae: 2.8665 - val_loss: 3.4171 - val_mae: 6.8607 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 2.1033 - mae: 2.6854\n",
            "Epoch 00008: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 2.1033 - mae: 2.6854 - val_loss: 3.2959 - val_mae: 6.6237 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.9975 - mae: 2.3995\n",
            "Epoch 00009: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.9975 - mae: 2.3995 - val_loss: 3.4522 - val_mae: 6.6021 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.9639 - mae: 2.2931\n",
            "Epoch 00010: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.9639 - mae: 2.2931 - val_loss: 3.4149 - val_mae: 6.3771 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.9133 - mae: 2.1756\n",
            "Epoch 00011: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.9133 - mae: 2.1756 - val_loss: 3.5836 - val_mae: 6.2397 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8938 - mae: 2.1175\n",
            "Epoch 00012: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.8938 - mae: 2.1175 - val_loss: 3.5454 - val_mae: 6.4238 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8606 - mae: 2.0528\n",
            "Epoch 00013: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.8606 - mae: 2.0528 - val_loss: 3.6882 - val_mae: 6.2586 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8528 - mae: 2.0406\n",
            "Epoch 00014: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.8528 - mae: 2.0406 - val_loss: 3.6824 - val_mae: 6.2164 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8368 - mae: 1.9969\n",
            "Epoch 00015: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 137s 84ms/step - loss: 1.8368 - mae: 1.9969 - val_loss: 3.7260 - val_mae: 6.1004 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8315 - mae: 1.9875\n",
            "Epoch 00016: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1640/1640 [==============================] - 137s 84ms/step - loss: 1.8315 - mae: 1.9875 - val_loss: 3.7675 - val_mae: 6.1944 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8279 - mae: 1.9952\n",
            "Epoch 00017: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 137s 84ms/step - loss: 1.8279 - mae: 1.9952 - val_loss: 3.7593 - val_mae: 6.2475 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8245 - mae: 1.9727\n",
            "Epoch 00018: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1640/1640 [==============================] - 137s 84ms/step - loss: 1.8245 - mae: 1.9727 - val_loss: 3.7998 - val_mae: 6.2436 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8152 - mae: 1.9741\n",
            "Epoch 00019: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.8152 - mae: 1.9741 - val_loss: 3.8129 - val_mae: 6.2854 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8107 - mae: 1.9601\n",
            "Epoch 00020: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.8107 - mae: 1.9601 - val_loss: 3.7406 - val_mae: 6.1347 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8118 - mae: 1.9513\n",
            "Epoch 00021: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 139s 85ms/step - loss: 1.8118 - mae: 1.9513 - val_loss: 3.7967 - val_mae: 6.2073 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8138 - mae: 1.9493\n",
            "Epoch 00022: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.8138 - mae: 1.9493 - val_loss: 3.7595 - val_mae: 6.3658 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8097 - mae: 1.9504\n",
            "Epoch 00023: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 139s 85ms/step - loss: 1.8097 - mae: 1.9504 - val_loss: 3.8162 - val_mae: 6.3563 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8110 - mae: 1.9350\n",
            "Epoch 00024: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1640/1640 [==============================] - 139s 85ms/step - loss: 1.8110 - mae: 1.9350 - val_loss: 3.7987 - val_mae: 6.2509 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8098 - mae: 1.9583\n",
            "Epoch 00025: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.8098 - mae: 1.9583 - val_loss: 3.7824 - val_mae: 6.2450 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8085 - mae: 1.9627\n",
            "Epoch 00026: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1640/1640 [==============================] - 139s 85ms/step - loss: 1.8085 - mae: 1.9627 - val_loss: 3.8059 - val_mae: 6.3053 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8094 - mae: 1.9435\n",
            "Epoch 00027: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.8094 - mae: 1.9435 - val_loss: 3.8220 - val_mae: 6.3695 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8056 - mae: 1.9478\n",
            "Epoch 00028: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.8056 - mae: 1.9478 - val_loss: 3.7231 - val_mae: 6.1766 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8056 - mae: 1.9485\n",
            "Epoch 00029: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.8056 - mae: 1.9485 - val_loss: 3.8446 - val_mae: 6.2993 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8058 - mae: 1.9289\n",
            "Epoch 00030: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.8058 - mae: 1.9289 - val_loss: 3.8236 - val_mae: 6.2839 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8087 - mae: 1.9434\n",
            "Epoch 00031: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.8087 - mae: 1.9434 - val_loss: 3.7249 - val_mae: 6.1527 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8041 - mae: 1.9392\n",
            "Epoch 00032: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.8041 - mae: 1.9392 - val_loss: 3.8138 - val_mae: 6.2805 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8075 - mae: 1.9351\n",
            "Epoch 00033: val_loss did not improve from 3.18329\n",
            "1640/1640 [==============================] - 138s 84ms/step - loss: 1.8075 - mae: 1.9351 - val_loss: 3.7880 - val_mae: 6.3534 - lr: 6.1035e-08\n",
            "Epoch 34/200\n",
            "1640/1640 [==============================] - ETA: 0s - loss: 1.8108 - mae: 1.9486Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 3.18329\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1640/1640 [==============================] - 139s 85ms/step - loss: 1.8108 - mae: 1.9486 - val_loss: 3.8106 - val_mae: 6.2724 - lr: 6.1035e-08\n",
            "Epoch 00034: early stopping\n",
            "167/167 [==============================] - 4s 24ms/step - loss: 2.4696 - mae: 4.4117\n",
            "Fit model on training data fold  3\n",
            "Epoch 1/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 6.8871 - mae: 11.3772\n",
            "Epoch 00001: val_loss improved from inf to 3.04144, saving model to /home/jupyter/DaLia/fold3.h5\n",
            "1597/1597 [==============================] - 161s 95ms/step - loss: 6.8871 - mae: 11.3772 - val_loss: 3.0414 - val_mae: 6.4224 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 3.0522 - mae: 6.7346\n",
            "Epoch 00002: val_loss did not improve from 3.04144\n",
            "1597/1597 [==============================] - 146s 91ms/step - loss: 3.0522 - mae: 6.7346 - val_loss: 3.4041 - val_mae: 6.6063 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.7450 - mae: 5.3874\n",
            "Epoch 00003: val_loss improved from 3.04144 to 2.69526, saving model to /home/jupyter/DaLia/fold3.h5\n",
            "1597/1597 [==============================] - 137s 86ms/step - loss: 2.7450 - mae: 5.3874 - val_loss: 2.6953 - val_mae: 4.4994 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.5697 - mae: 4.6868\n",
            "Epoch 00004: val_loss improved from 2.69526 to 2.62275, saving model to /home/jupyter/DaLia/fold3.h5\n",
            "1597/1597 [==============================] - 140s 88ms/step - loss: 2.5697 - mae: 4.6868 - val_loss: 2.6227 - val_mae: 4.2442 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.4981 - mae: 4.4784\n",
            "Epoch 00005: val_loss did not improve from 2.62275\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 2.4981 - mae: 4.4784 - val_loss: 2.7403 - val_mae: 4.8208 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.3873 - mae: 3.9694\n",
            "Epoch 00006: val_loss did not improve from 2.62275\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1597/1597 [==============================] - 148s 93ms/step - loss: 2.3873 - mae: 3.9694 - val_loss: 2.7374 - val_mae: 3.7803 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.2182 - mae: 3.3851\n",
            "Epoch 00007: val_loss improved from 2.62275 to 2.60442, saving model to /home/jupyter/DaLia/fold3.h5\n",
            "1597/1597 [==============================] - 145s 91ms/step - loss: 2.2182 - mae: 3.3851 - val_loss: 2.6044 - val_mae: 3.4899 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.1420 - mae: 3.0738\n",
            "Epoch 00008: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 140s 88ms/step - loss: 2.1420 - mae: 3.0738 - val_loss: 2.7167 - val_mae: 3.5504 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.1065 - mae: 2.9387\n",
            "Epoch 00009: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 2.1065 - mae: 2.9387 - val_loss: 2.6514 - val_mae: 3.5660 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.0032 - mae: 2.6360\n",
            "Epoch 00010: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 148s 93ms/step - loss: 2.0032 - mae: 2.6360 - val_loss: 2.7374 - val_mae: 3.3876 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.9656 - mae: 2.5084\n",
            "Epoch 00011: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1597/1597 [==============================] - 148s 93ms/step - loss: 1.9656 - mae: 2.5084 - val_loss: 2.6855 - val_mae: 3.5293 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.9127 - mae: 2.3712\n",
            "Epoch 00012: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 148s 92ms/step - loss: 1.9127 - mae: 2.3712 - val_loss: 2.8432 - val_mae: 3.3131 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8934 - mae: 2.2941\n",
            "Epoch 00013: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1597/1597 [==============================] - 148s 92ms/step - loss: 1.8934 - mae: 2.2941 - val_loss: 2.8566 - val_mae: 3.4041 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8570 - mae: 2.2329\n",
            "Epoch 00014: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 1.8570 - mae: 2.2329 - val_loss: 2.8110 - val_mae: 3.2877 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8447 - mae: 2.1709\n",
            "Epoch 00015: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1597/1597 [==============================] - 147s 92ms/step - loss: 1.8447 - mae: 2.1709 - val_loss: 2.8974 - val_mae: 3.2760 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8326 - mae: 2.1516\n",
            "Epoch 00016: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 1.8326 - mae: 2.1516 - val_loss: 2.8019 - val_mae: 3.2959 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8238 - mae: 2.1416\n",
            "Epoch 00017: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1597/1597 [==============================] - 148s 93ms/step - loss: 1.8238 - mae: 2.1416 - val_loss: 2.9280 - val_mae: 3.2592 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8187 - mae: 2.1372\n",
            "Epoch 00018: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 149s 94ms/step - loss: 1.8187 - mae: 2.1372 - val_loss: 2.8898 - val_mae: 3.3096 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8151 - mae: 2.1083\n",
            "Epoch 00019: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1597/1597 [==============================] - 150s 94ms/step - loss: 1.8151 - mae: 2.1083 - val_loss: 2.9367 - val_mae: 3.2846 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8106 - mae: 2.1034\n",
            "Epoch 00020: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 150s 94ms/step - loss: 1.8106 - mae: 2.1034 - val_loss: 2.9965 - val_mae: 3.2809 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8040 - mae: 2.0913\n",
            "Epoch 00021: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1597/1597 [==============================] - 151s 95ms/step - loss: 1.8040 - mae: 2.0913 - val_loss: 2.9663 - val_mae: 3.2486 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8016 - mae: 2.0772\n",
            "Epoch 00022: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 152s 95ms/step - loss: 1.8016 - mae: 2.0772 - val_loss: 2.9108 - val_mae: 3.2419 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8038 - mae: 2.0878\n",
            "Epoch 00023: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1597/1597 [==============================] - 148s 93ms/step - loss: 1.8038 - mae: 2.0878 - val_loss: 2.9316 - val_mae: 3.2911 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7970 - mae: 2.0811\n",
            "Epoch 00024: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 1.7970 - mae: 2.0811 - val_loss: 2.9590 - val_mae: 3.2790 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8030 - mae: 2.0932\n",
            "Epoch 00025: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 1.8030 - mae: 2.0932 - val_loss: 2.9573 - val_mae: 3.3106 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8030 - mae: 2.0817\n",
            "Epoch 00026: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 1.8030 - mae: 2.0817 - val_loss: 2.9466 - val_mae: 3.2815 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7978 - mae: 2.0745\n",
            "Epoch 00027: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1597/1597 [==============================] - 150s 94ms/step - loss: 1.7978 - mae: 2.0745 - val_loss: 2.9279 - val_mae: 3.2579 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8012 - mae: 2.0947\n",
            "Epoch 00028: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 151s 95ms/step - loss: 1.8012 - mae: 2.0947 - val_loss: 2.9606 - val_mae: 3.2957 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7994 - mae: 2.0755\n",
            "Epoch 00029: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1597/1597 [==============================] - 151s 95ms/step - loss: 1.7994 - mae: 2.0755 - val_loss: 2.9305 - val_mae: 3.2411 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8016 - mae: 2.0969\n",
            "Epoch 00030: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 150s 94ms/step - loss: 1.8016 - mae: 2.0969 - val_loss: 2.9419 - val_mae: 3.2956 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8018 - mae: 2.0897\n",
            "Epoch 00031: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1597/1597 [==============================] - 153s 96ms/step - loss: 1.8018 - mae: 2.0897 - val_loss: 2.9180 - val_mae: 3.3203 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8002 - mae: 2.0866\n",
            "Epoch 00032: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 148s 93ms/step - loss: 1.8002 - mae: 2.0866 - val_loss: 2.9199 - val_mae: 3.2171 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7968 - mae: 2.0718\n",
            "Epoch 00033: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1597/1597 [==============================] - 141s 88ms/step - loss: 1.7968 - mae: 2.0718 - val_loss: 2.9407 - val_mae: 3.2973 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7976 - mae: 2.0700\n",
            "Epoch 00034: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 150s 94ms/step - loss: 1.7976 - mae: 2.0700 - val_loss: 2.9308 - val_mae: 3.2922 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7987 - mae: 2.0780\n",
            "Epoch 00035: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1597/1597 [==============================] - 150s 94ms/step - loss: 1.7987 - mae: 2.0780 - val_loss: 2.9198 - val_mae: 3.2519 - lr: 6.1035e-08\n",
            "Epoch 36/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8012 - mae: 2.0852\n",
            "Epoch 00036: val_loss did not improve from 2.60442\n",
            "1597/1597 [==============================] - 155s 97ms/step - loss: 1.8012 - mae: 2.0852 - val_loss: 2.9486 - val_mae: 3.2844 - lr: 3.0518e-08\n",
            "Epoch 37/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8006 - mae: 2.0726Restoring model weights from the end of the best epoch: 7.\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 2.60442\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "1597/1597 [==============================] - 155s 97ms/step - loss: 1.8006 - mae: 2.0726 - val_loss: 2.9290 - val_mae: 3.3047 - lr: 3.0518e-08\n",
            "Epoch 00037: early stopping\n",
            "142/142 [==============================] - 5s 34ms/step - loss: 2.8461 - mae: 4.6695\n",
            "Fit model on training data fold  4\n",
            "Epoch 1/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 5.2413 - mae: 12.2252\n",
            "Epoch 00001: val_loss improved from inf to 3.51486, saving model to /home/jupyter/DaLia/fold4.h5\n",
            "1597/1597 [==============================] - 157s 92ms/step - loss: 5.2413 - mae: 12.2252 - val_loss: 3.5149 - val_mae: 8.8075 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 3.2759 - mae: 7.3228\n",
            "Epoch 00002: val_loss did not improve from 3.51486\n",
            "1597/1597 [==============================] - 140s 88ms/step - loss: 3.2759 - mae: 7.3228 - val_loss: 3.5210 - val_mae: 7.5516 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 3.0417 - mae: 5.9221\n",
            "Epoch 00003: val_loss improved from 3.51486 to 3.26277, saving model to /home/jupyter/DaLia/fold4.h5\n",
            "1597/1597 [==============================] - 145s 91ms/step - loss: 3.0417 - mae: 5.9221 - val_loss: 3.2628 - val_mae: 6.2745 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.7005 - mae: 4.7703\n",
            "Epoch 00004: val_loss improved from 3.26277 to 3.10403, saving model to /home/jupyter/DaLia/fold4.h5\n",
            "1597/1597 [==============================] - 148s 92ms/step - loss: 2.7005 - mae: 4.7703 - val_loss: 3.1040 - val_mae: 5.7657 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.5184 - mae: 4.2099\n",
            "Epoch 00005: val_loss improved from 3.10403 to 2.77781, saving model to /home/jupyter/DaLia/fold4.h5\n",
            "1597/1597 [==============================] - 147s 92ms/step - loss: 2.5184 - mae: 4.2099 - val_loss: 2.7778 - val_mae: 5.4014 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.3927 - mae: 3.7626\n",
            "Epoch 00006: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 2.3927 - mae: 3.7626 - val_loss: 2.8701 - val_mae: 5.0709 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.3199 - mae: 3.4925\n",
            "Epoch 00007: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1597/1597 [==============================] - 148s 93ms/step - loss: 2.3199 - mae: 3.4925 - val_loss: 2.8815 - val_mae: 4.8234 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.1436 - mae: 2.9679\n",
            "Epoch 00008: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 145s 91ms/step - loss: 2.1436 - mae: 2.9679 - val_loss: 2.8546 - val_mae: 4.6381 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.0885 - mae: 2.7679\n",
            "Epoch 00009: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1597/1597 [==============================] - 148s 93ms/step - loss: 2.0885 - mae: 2.7679 - val_loss: 2.8924 - val_mae: 4.3422 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.9843 - mae: 2.4732\n",
            "Epoch 00010: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 144s 90ms/step - loss: 1.9843 - mae: 2.4732 - val_loss: 2.9099 - val_mae: 4.4196 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.9460 - mae: 2.3498\n",
            "Epoch 00011: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1597/1597 [==============================] - 140s 88ms/step - loss: 1.9460 - mae: 2.3498 - val_loss: 3.0771 - val_mae: 4.2637 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8937 - mae: 2.2307\n",
            "Epoch 00012: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 148s 93ms/step - loss: 1.8937 - mae: 2.2307 - val_loss: 3.1455 - val_mae: 4.3077 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8720 - mae: 2.1826\n",
            "Epoch 00013: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1597/1597 [==============================] - 148s 92ms/step - loss: 1.8720 - mae: 2.1826 - val_loss: 3.1000 - val_mae: 4.2193 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8423 - mae: 2.1170\n",
            "Epoch 00014: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 144s 90ms/step - loss: 1.8423 - mae: 2.1170 - val_loss: 3.1562 - val_mae: 4.2497 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8295 - mae: 2.0939\n",
            "Epoch 00015: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1597/1597 [==============================] - 139s 87ms/step - loss: 1.8295 - mae: 2.0939 - val_loss: 3.2119 - val_mae: 4.2664 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8110 - mae: 2.0510\n",
            "Epoch 00016: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 147s 92ms/step - loss: 1.8110 - mae: 2.0510 - val_loss: 3.2525 - val_mae: 4.3616 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8111 - mae: 2.0489\n",
            "Epoch 00017: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1597/1597 [==============================] - 143s 90ms/step - loss: 1.8111 - mae: 2.0489 - val_loss: 3.2460 - val_mae: 4.3153 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8049 - mae: 2.0242\n",
            "Epoch 00018: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 148s 93ms/step - loss: 1.8049 - mae: 2.0242 - val_loss: 3.1831 - val_mae: 4.2029 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8003 - mae: 2.0072\n",
            "Epoch 00019: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1597/1597 [==============================] - 143s 90ms/step - loss: 1.8003 - mae: 2.0072 - val_loss: 3.2418 - val_mae: 4.2369 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7947 - mae: 2.0156\n",
            "Epoch 00020: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 1.7947 - mae: 2.0156 - val_loss: 3.2326 - val_mae: 4.2662 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7890 - mae: 1.9950\n",
            "Epoch 00021: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1597/1597 [==============================] - 148s 92ms/step - loss: 1.7890 - mae: 1.9950 - val_loss: 3.2522 - val_mae: 4.2433 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7908 - mae: 1.9913\n",
            "Epoch 00022: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 147s 92ms/step - loss: 1.7908 - mae: 1.9913 - val_loss: 3.2880 - val_mae: 4.2839 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7915 - mae: 2.0084\n",
            "Epoch 00023: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1597/1597 [==============================] - 145s 91ms/step - loss: 1.7915 - mae: 2.0084 - val_loss: 3.2611 - val_mae: 4.2875 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7901 - mae: 2.0186\n",
            "Epoch 00024: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 144s 90ms/step - loss: 1.7901 - mae: 2.0186 - val_loss: 3.2412 - val_mae: 4.2322 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7866 - mae: 1.9964\n",
            "Epoch 00025: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1597/1597 [==============================] - 139s 87ms/step - loss: 1.7866 - mae: 1.9964 - val_loss: 3.2509 - val_mae: 4.2064 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7915 - mae: 2.0023\n",
            "Epoch 00026: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 147s 92ms/step - loss: 1.7915 - mae: 2.0023 - val_loss: 3.2445 - val_mae: 4.2308 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7855 - mae: 1.9879\n",
            "Epoch 00027: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1597/1597 [==============================] - 147s 92ms/step - loss: 1.7855 - mae: 1.9879 - val_loss: 3.3137 - val_mae: 4.3345 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7874 - mae: 1.9863\n",
            "Epoch 00028: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 147s 92ms/step - loss: 1.7874 - mae: 1.9863 - val_loss: 3.2669 - val_mae: 4.3100 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7900 - mae: 1.9959\n",
            "Epoch 00029: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1597/1597 [==============================] - 141s 89ms/step - loss: 1.7900 - mae: 1.9959 - val_loss: 3.2508 - val_mae: 4.2453 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7894 - mae: 1.9921\n",
            "Epoch 00030: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 139s 87ms/step - loss: 1.7894 - mae: 1.9921 - val_loss: 3.2703 - val_mae: 4.2528 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7874 - mae: 1.9949\n",
            "Epoch 00031: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1597/1597 [==============================] - 141s 88ms/step - loss: 1.7874 - mae: 1.9949 - val_loss: 3.2856 - val_mae: 4.2448 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7846 - mae: 1.9851\n",
            "Epoch 00032: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 144s 90ms/step - loss: 1.7846 - mae: 1.9851 - val_loss: 3.2713 - val_mae: 4.2523 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7889 - mae: 2.0052\n",
            "Epoch 00033: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1597/1597 [==============================] - 141s 88ms/step - loss: 1.7889 - mae: 2.0052 - val_loss: 3.2567 - val_mae: 4.2603 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7861 - mae: 2.0019\n",
            "Epoch 00034: val_loss did not improve from 2.77781\n",
            "1597/1597 [==============================] - 139s 87ms/step - loss: 1.7861 - mae: 2.0019 - val_loss: 3.2810 - val_mae: 4.2069 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7875 - mae: 2.0039Restoring model weights from the end of the best epoch: 5.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 2.77781\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1597/1597 [==============================] - 147s 92ms/step - loss: 1.7875 - mae: 2.0039 - val_loss: 3.2716 - val_mae: 4.3223 - lr: 6.1035e-08\n",
            "Epoch 00035: early stopping\n",
            "140/140 [==============================] - 5s 35ms/step - loss: 2.4304 - mae: 3.6374\n",
            "Fit model on training data fold  5\n",
            "Epoch 1/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 4.9873 - mae: 12.9620\n",
            "Epoch 00001: val_loss improved from inf to 3.31496, saving model to /home/jupyter/DaLia/fold5.h5\n",
            "1597/1597 [==============================] - 160s 94ms/step - loss: 4.9873 - mae: 12.9620 - val_loss: 3.3150 - val_mae: 7.8190 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 3.2135 - mae: 6.8857\n",
            "Epoch 00002: val_loss improved from 3.31496 to 3.16570, saving model to /home/jupyter/DaLia/fold5.h5\n",
            "1597/1597 [==============================] - 145s 90ms/step - loss: 3.2135 - mae: 6.8857 - val_loss: 3.1657 - val_mae: 6.4274 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.9914 - mae: 5.6166\n",
            "Epoch 00003: val_loss improved from 3.16570 to 3.12986, saving model to /home/jupyter/DaLia/fold5.h5\n",
            "1597/1597 [==============================] - 140s 88ms/step - loss: 2.9914 - mae: 5.6166 - val_loss: 3.1299 - val_mae: 6.0324 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.6842 - mae: 4.5757\n",
            "Epoch 00004: val_loss improved from 3.12986 to 2.67360, saving model to /home/jupyter/DaLia/fold5.h5\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 2.6842 - mae: 4.5757 - val_loss: 2.6736 - val_mae: 4.1170 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.5021 - mae: 4.1135\n",
            "Epoch 00005: val_loss did not improve from 2.67360\n",
            "1597/1597 [==============================] - 145s 91ms/step - loss: 2.5021 - mae: 4.1135 - val_loss: 2.6864 - val_mae: 4.6940 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.4138 - mae: 3.8712\n",
            "Epoch 00006: val_loss improved from 2.67360 to 2.66618, saving model to /home/jupyter/DaLia/fold5.h5\n",
            "1597/1597 [==============================] - 136s 85ms/step - loss: 2.4138 - mae: 3.8712 - val_loss: 2.6662 - val_mae: 4.4853 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.3267 - mae: 3.5421\n",
            "Epoch 00007: val_loss improved from 2.66618 to 2.57880, saving model to /home/jupyter/DaLia/fold5.h5\n",
            "1597/1597 [==============================] - 141s 88ms/step - loss: 2.3267 - mae: 3.5421 - val_loss: 2.5788 - val_mae: 4.2477 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.2517 - mae: 3.2789\n",
            "Epoch 00008: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 148s 93ms/step - loss: 2.2517 - mae: 3.2789 - val_loss: 2.5878 - val_mae: 4.0874 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.2241 - mae: 3.2206\n",
            "Epoch 00009: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 2.2241 - mae: 3.2206 - val_loss: 2.6159 - val_mae: 3.8837 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.0717 - mae: 2.7514\n",
            "Epoch 00010: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 147s 92ms/step - loss: 2.0717 - mae: 2.7514 - val_loss: 2.8206 - val_mae: 4.0684 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 2.0190 - mae: 2.5688\n",
            "Epoch 00011: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1597/1597 [==============================] - 141s 88ms/step - loss: 2.0190 - mae: 2.5688 - val_loss: 2.7511 - val_mae: 3.7170 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.9314 - mae: 2.3498\n",
            "Epoch 00012: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 152s 95ms/step - loss: 1.9314 - mae: 2.3498 - val_loss: 2.8149 - val_mae: 3.4926 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8962 - mae: 2.2675\n",
            "Epoch 00013: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1597/1597 [==============================] - 149s 94ms/step - loss: 1.8962 - mae: 2.2675 - val_loss: 2.8540 - val_mae: 3.4342 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8479 - mae: 2.1417\n",
            "Epoch 00014: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 148s 93ms/step - loss: 1.8479 - mae: 2.1417 - val_loss: 2.9428 - val_mae: 3.4986 - lr: 1.2500e-04\n",
            "Epoch 15/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8307 - mae: 2.0901\n",
            "Epoch 00015: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 1.8307 - mae: 2.0901 - val_loss: 2.9368 - val_mae: 3.4948 - lr: 1.2500e-04\n",
            "Epoch 16/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.8035 - mae: 2.0481\n",
            "Epoch 00016: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 1.8035 - mae: 2.0481 - val_loss: 3.0151 - val_mae: 3.4074 - lr: 6.2500e-05\n",
            "Epoch 17/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7925 - mae: 2.0005\n",
            "Epoch 00017: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1597/1597 [==============================] - 144s 90ms/step - loss: 1.7925 - mae: 2.0005 - val_loss: 2.9828 - val_mae: 3.4265 - lr: 6.2500e-05\n",
            "Epoch 18/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7813 - mae: 1.9827\n",
            "Epoch 00018: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 140s 87ms/step - loss: 1.7813 - mae: 1.9827 - val_loss: 3.0742 - val_mae: 3.3999 - lr: 3.1250e-05\n",
            "Epoch 19/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7768 - mae: 1.9835\n",
            "Epoch 00019: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1597/1597 [==============================] - 148s 93ms/step - loss: 1.7768 - mae: 1.9835 - val_loss: 3.0626 - val_mae: 3.5377 - lr: 3.1250e-05\n",
            "Epoch 20/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7605 - mae: 1.9688\n",
            "Epoch 00020: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 145s 91ms/step - loss: 1.7605 - mae: 1.9688 - val_loss: 3.0949 - val_mae: 3.4417 - lr: 1.5625e-05\n",
            "Epoch 21/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7618 - mae: 1.9308\n",
            "Epoch 00021: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 1.7618 - mae: 1.9308 - val_loss: 3.0706 - val_mae: 3.3882 - lr: 1.5625e-05\n",
            "Epoch 22/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7608 - mae: 1.9430\n",
            "Epoch 00022: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 1.7608 - mae: 1.9430 - val_loss: 3.0743 - val_mae: 3.3909 - lr: 7.8125e-06\n",
            "Epoch 23/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7601 - mae: 1.9400\n",
            "Epoch 00023: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1597/1597 [==============================] - 147s 92ms/step - loss: 1.7601 - mae: 1.9400 - val_loss: 3.0593 - val_mae: 3.3984 - lr: 7.8125e-06\n",
            "Epoch 24/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7534 - mae: 1.9224\n",
            "Epoch 00024: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 148s 93ms/step - loss: 1.7534 - mae: 1.9224 - val_loss: 3.0837 - val_mae: 3.3648 - lr: 3.9063e-06\n",
            "Epoch 25/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7532 - mae: 1.9271\n",
            "Epoch 00025: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 1.7532 - mae: 1.9271 - val_loss: 3.0640 - val_mae: 3.3224 - lr: 3.9063e-06\n",
            "Epoch 26/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7537 - mae: 1.9253\n",
            "Epoch 00026: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 1.7537 - mae: 1.9253 - val_loss: 3.1324 - val_mae: 3.4349 - lr: 1.9531e-06\n",
            "Epoch 27/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7519 - mae: 1.9261\n",
            "Epoch 00027: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1597/1597 [==============================] - 146s 91ms/step - loss: 1.7519 - mae: 1.9261 - val_loss: 3.0995 - val_mae: 3.4451 - lr: 1.9531e-06\n",
            "Epoch 28/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7503 - mae: 1.9158\n",
            "Epoch 00028: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 147s 92ms/step - loss: 1.7503 - mae: 1.9158 - val_loss: 3.1148 - val_mae: 3.3742 - lr: 9.7656e-07\n",
            "Epoch 29/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7488 - mae: 1.9256\n",
            "Epoch 00029: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1597/1597 [==============================] - 136s 85ms/step - loss: 1.7488 - mae: 1.9256 - val_loss: 3.1451 - val_mae: 3.4580 - lr: 9.7656e-07\n",
            "Epoch 30/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7538 - mae: 1.9167\n",
            "Epoch 00030: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 139s 87ms/step - loss: 1.7538 - mae: 1.9167 - val_loss: 3.1061 - val_mae: 3.3431 - lr: 4.8828e-07\n",
            "Epoch 31/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7515 - mae: 1.9230\n",
            "Epoch 00031: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1597/1597 [==============================] - 150s 94ms/step - loss: 1.7515 - mae: 1.9230 - val_loss: 3.1059 - val_mae: 3.3953 - lr: 4.8828e-07\n",
            "Epoch 32/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7520 - mae: 1.9281\n",
            "Epoch 00032: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 149s 93ms/step - loss: 1.7520 - mae: 1.9281 - val_loss: 3.1152 - val_mae: 3.4280 - lr: 2.4414e-07\n",
            "Epoch 33/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7525 - mae: 1.9227\n",
            "Epoch 00033: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1597/1597 [==============================] - 150s 94ms/step - loss: 1.7525 - mae: 1.9227 - val_loss: 3.0798 - val_mae: 3.4223 - lr: 2.4414e-07\n",
            "Epoch 34/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7547 - mae: 1.9320\n",
            "Epoch 00034: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 153s 96ms/step - loss: 1.7547 - mae: 1.9320 - val_loss: 3.1060 - val_mae: 3.4368 - lr: 1.2207e-07\n",
            "Epoch 35/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7559 - mae: 1.9347\n",
            "Epoch 00035: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1597/1597 [==============================] - 150s 94ms/step - loss: 1.7559 - mae: 1.9347 - val_loss: 3.0764 - val_mae: 3.4022 - lr: 1.2207e-07\n",
            "Epoch 36/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7529 - mae: 1.9301\n",
            "Epoch 00036: val_loss did not improve from 2.57880\n",
            "1597/1597 [==============================] - 149s 94ms/step - loss: 1.7529 - mae: 1.9301 - val_loss: 3.0875 - val_mae: 3.4382 - lr: 6.1035e-08\n",
            "Epoch 37/200\n",
            "1597/1597 [==============================] - ETA: 0s - loss: 1.7517 - mae: 1.9264Restoring model weights from the end of the best epoch: 7.\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 2.57880\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1597/1597 [==============================] - 140s 88ms/step - loss: 1.7517 - mae: 1.9264 - val_loss: 3.1139 - val_mae: 3.4050 - lr: 6.1035e-08\n",
            "Epoch 00037: early stopping\n",
            "144/144 [==============================] - 5s 34ms/step - loss: 2.7996 - mae: 4.5551\n",
            "Fit model on training data fold  6\n",
            "Epoch 1/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 5.0038 - mae: 12.3165\n",
            "Epoch 00001: val_loss improved from inf to 3.75548, saving model to /home/jupyter/DaLia/fold6.h5\n",
            "1609/1609 [==============================] - 161s 94ms/step - loss: 5.0038 - mae: 12.3165 - val_loss: 3.7555 - val_mae: 10.0145 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 3.2210 - mae: 7.0078\n",
            "Epoch 00002: val_loss improved from 3.75548 to 2.86911, saving model to /home/jupyter/DaLia/fold6.h5\n",
            "1609/1609 [==============================] - 148s 92ms/step - loss: 3.2212 - mae: 7.0078 - val_loss: 2.8691 - val_mae: 5.4870 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 2.8741 - mae: 5.5076\n",
            "Epoch 00003: val_loss did not improve from 2.86911\n",
            "1609/1609 [==============================] - 150s 93ms/step - loss: 2.8742 - mae: 5.5076 - val_loss: 3.2598 - val_mae: 5.6299 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 2.6041 - mae: 4.5431\n",
            "Epoch 00004: val_loss improved from 2.86911 to 2.50518, saving model to /home/jupyter/DaLia/fold6.h5\n",
            "1609/1609 [==============================] - 147s 91ms/step - loss: 2.6041 - mae: 4.5431 - val_loss: 2.5052 - val_mae: 4.3782 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 2.4779 - mae: 4.1099\n",
            "Epoch 00005: val_loss improved from 2.50518 to 2.35244, saving model to /home/jupyter/DaLia/fold6.h5\n",
            "1609/1609 [==============================] - 142s 89ms/step - loss: 2.4779 - mae: 4.1099 - val_loss: 2.3524 - val_mae: 3.6298 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 2.3925 - mae: 3.8256\n",
            "Epoch 00006: val_loss did not improve from 2.35244\n",
            "1609/1609 [==============================] - 152s 95ms/step - loss: 2.3925 - mae: 3.8254 - val_loss: 2.5520 - val_mae: 3.9636 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 2.3057 - mae: 3.4895\n",
            "Epoch 00007: val_loss improved from 2.35244 to 2.30122, saving model to /home/jupyter/DaLia/fold6.h5\n",
            "1609/1609 [==============================] - 151s 94ms/step - loss: 2.3057 - mae: 3.4895 - val_loss: 2.3012 - val_mae: 3.6055 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 2.2445 - mae: 3.2419\n",
            "Epoch 00008: val_loss did not improve from 2.30122\n",
            "1609/1609 [==============================] - 152s 94ms/step - loss: 2.2445 - mae: 3.2419 - val_loss: 2.3458 - val_mae: 3.4760 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 2.1992 - mae: 3.0830\n",
            "Epoch 00009: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1609/1609 [==============================] - 148s 92ms/step - loss: 2.1993 - mae: 3.0836 - val_loss: 2.8474 - val_mae: 3.7512 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 2.0374 - mae: 2.6063\n",
            "Epoch 00010: val_loss improved from 2.30122 to 2.25873, saving model to /home/jupyter/DaLia/fold6.h5\n",
            "1609/1609 [==============================] - 152s 94ms/step - loss: 2.0374 - mae: 2.6063 - val_loss: 2.2587 - val_mae: 3.2342 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.9936 - mae: 2.4858\n",
            "Epoch 00011: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 153s 95ms/step - loss: 1.9936 - mae: 2.4858 - val_loss: 2.3874 - val_mae: 3.0393 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.9551 - mae: 2.3899\n",
            "Epoch 00012: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1609/1609 [==============================] - 152s 95ms/step - loss: 1.9552 - mae: 2.3901 - val_loss: 2.3288 - val_mae: 3.1372 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.8732 - mae: 2.1748\n",
            "Epoch 00013: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 147s 92ms/step - loss: 1.8732 - mae: 2.1748 - val_loss: 2.5656 - val_mae: 2.9554 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.8438 - mae: 2.1139\n",
            "Epoch 00014: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1609/1609 [==============================] - 154s 96ms/step - loss: 1.8438 - mae: 2.1139 - val_loss: 2.5785 - val_mae: 3.0223 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7997 - mae: 2.0165\n",
            "Epoch 00015: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 153s 95ms/step - loss: 1.7997 - mae: 2.0165 - val_loss: 2.3915 - val_mae: 2.8905 - lr: 1.2500e-04\n",
            "Epoch 16/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7761 - mae: 1.9607\n",
            "Epoch 00016: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1609/1609 [==============================] - 152s 94ms/step - loss: 1.7761 - mae: 1.9607 - val_loss: 2.3646 - val_mae: 2.8240 - lr: 1.2500e-04\n",
            "Epoch 17/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7511 - mae: 1.9042\n",
            "Epoch 00017: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 143s 89ms/step - loss: 1.7511 - mae: 1.9043 - val_loss: 2.4652 - val_mae: 2.8370 - lr: 6.2500e-05\n",
            "Epoch 18/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7411 - mae: 1.8937\n",
            "Epoch 00018: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1609/1609 [==============================] - 155s 96ms/step - loss: 1.7411 - mae: 1.8937 - val_loss: 2.3864 - val_mae: 2.8086 - lr: 6.2500e-05\n",
            "Epoch 19/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7319 - mae: 1.8769\n",
            "Epoch 00019: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 157s 98ms/step - loss: 1.7319 - mae: 1.8769 - val_loss: 2.4127 - val_mae: 2.8049 - lr: 3.1250e-05\n",
            "Epoch 20/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7164 - mae: 1.8418\n",
            "Epoch 00020: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1609/1609 [==============================] - 157s 98ms/step - loss: 1.7164 - mae: 1.8418 - val_loss: 2.4531 - val_mae: 2.7276 - lr: 3.1250e-05\n",
            "Epoch 21/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7161 - mae: 1.8439\n",
            "Epoch 00021: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 164s 102ms/step - loss: 1.7161 - mae: 1.8439 - val_loss: 2.4339 - val_mae: 2.7652 - lr: 1.5625e-05\n",
            "Epoch 22/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7192 - mae: 1.8454\n",
            "Epoch 00022: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1609/1609 [==============================] - 163s 101ms/step - loss: 1.7192 - mae: 1.8454 - val_loss: 2.4415 - val_mae: 2.8075 - lr: 1.5625e-05\n",
            "Epoch 23/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7057 - mae: 1.8267\n",
            "Epoch 00023: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 158s 98ms/step - loss: 1.7057 - mae: 1.8267 - val_loss: 2.4386 - val_mae: 2.8165 - lr: 7.8125e-06\n",
            "Epoch 24/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7078 - mae: 1.8142\n",
            "Epoch 00024: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1609/1609 [==============================] - 161s 100ms/step - loss: 1.7078 - mae: 1.8142 - val_loss: 2.4672 - val_mae: 2.7950 - lr: 7.8125e-06\n",
            "Epoch 25/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7024 - mae: 1.8102\n",
            "Epoch 00025: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 155s 96ms/step - loss: 1.7024 - mae: 1.8102 - val_loss: 2.4402 - val_mae: 2.7859 - lr: 3.9063e-06\n",
            "Epoch 26/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7032 - mae: 1.8129\n",
            "Epoch 00026: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1609/1609 [==============================] - 150s 93ms/step - loss: 1.7032 - mae: 1.8129 - val_loss: 2.4693 - val_mae: 2.7622 - lr: 3.9063e-06\n",
            "Epoch 27/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7064 - mae: 1.8211\n",
            "Epoch 00027: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 141s 88ms/step - loss: 1.7065 - mae: 1.8210 - val_loss: 2.4825 - val_mae: 2.8456 - lr: 1.9531e-06\n",
            "Epoch 28/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7023 - mae: 1.8143\n",
            "Epoch 00028: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1609/1609 [==============================] - 150s 93ms/step - loss: 1.7023 - mae: 1.8145 - val_loss: 2.4070 - val_mae: 2.7995 - lr: 1.9531e-06\n",
            "Epoch 29/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7027 - mae: 1.8223\n",
            "Epoch 00029: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 150s 93ms/step - loss: 1.7027 - mae: 1.8223 - val_loss: 2.4418 - val_mae: 2.7631 - lr: 9.7656e-07\n",
            "Epoch 30/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7054 - mae: 1.8239\n",
            "Epoch 00030: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1609/1609 [==============================] - 150s 93ms/step - loss: 1.7054 - mae: 1.8239 - val_loss: 2.4703 - val_mae: 2.7984 - lr: 9.7656e-07\n",
            "Epoch 31/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7047 - mae: 1.8186\n",
            "Epoch 00031: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 144s 89ms/step - loss: 1.7048 - mae: 1.8187 - val_loss: 2.4435 - val_mae: 2.7552 - lr: 4.8828e-07\n",
            "Epoch 32/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7026 - mae: 1.8212\n",
            "Epoch 00032: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1609/1609 [==============================] - 149s 93ms/step - loss: 1.7026 - mae: 1.8212 - val_loss: 2.4268 - val_mae: 2.7478 - lr: 4.8828e-07\n",
            "Epoch 33/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7034 - mae: 1.8208\n",
            "Epoch 00033: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 145s 90ms/step - loss: 1.7035 - mae: 1.8210 - val_loss: 2.4449 - val_mae: 2.7056 - lr: 2.4414e-07\n",
            "Epoch 34/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.6978 - mae: 1.8198\n",
            "Epoch 00034: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1609/1609 [==============================] - 141s 88ms/step - loss: 1.6978 - mae: 1.8198 - val_loss: 2.4529 - val_mae: 2.8208 - lr: 2.4414e-07\n",
            "Epoch 35/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7034 - mae: 1.8162\n",
            "Epoch 00035: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 151s 94ms/step - loss: 1.7034 - mae: 1.8162 - val_loss: 2.4424 - val_mae: 2.7496 - lr: 1.2207e-07\n",
            "Epoch 36/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7019 - mae: 1.8072\n",
            "Epoch 00036: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1609/1609 [==============================] - 150s 93ms/step - loss: 1.7019 - mae: 1.8072 - val_loss: 2.4495 - val_mae: 2.7698 - lr: 1.2207e-07\n",
            "Epoch 37/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.6986 - mae: 1.8197\n",
            "Epoch 00037: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 149s 93ms/step - loss: 1.6986 - mae: 1.8197 - val_loss: 2.4721 - val_mae: 2.7903 - lr: 6.1035e-08\n",
            "Epoch 38/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7037 - mae: 1.8231\n",
            "Epoch 00038: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1609/1609 [==============================] - 150s 93ms/step - loss: 1.7037 - mae: 1.8231 - val_loss: 2.4069 - val_mae: 2.7812 - lr: 6.1035e-08\n",
            "Epoch 39/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7001 - mae: 1.8203\n",
            "Epoch 00039: val_loss did not improve from 2.25873\n",
            "1609/1609 [==============================] - 152s 94ms/step - loss: 1.7001 - mae: 1.8203 - val_loss: 2.4306 - val_mae: 2.7642 - lr: 3.0518e-08\n",
            "Epoch 40/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7007 - mae: 1.8057Restoring model weights from the end of the best epoch: 10.\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 2.25873\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "1609/1609 [==============================] - 149s 93ms/step - loss: 1.7007 - mae: 1.8057 - val_loss: 2.4589 - val_mae: 2.7676 - lr: 3.0518e-08\n",
            "Epoch 00040: early stopping\n",
            "143/143 [==============================] - 5s 32ms/step - loss: 3.1509 - mae: 6.1185\n",
            "Fit model on training data fold  7\n",
            "Epoch 1/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 4.5276 - mae: 11.5530\n",
            "Epoch 00001: val_loss improved from inf to 3.70509, saving model to /home/jupyter/DaLia/fold7.h5\n",
            "1609/1609 [==============================] - 162s 95ms/step - loss: 4.5275 - mae: 11.5527 - val_loss: 3.7051 - val_mae: 8.4861 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 3.1421 - mae: 6.5817\n",
            "Epoch 00002: val_loss improved from 3.70509 to 3.09017, saving model to /home/jupyter/DaLia/fold7.h5\n",
            "1609/1609 [==============================] - 149s 93ms/step - loss: 3.1421 - mae: 6.5817 - val_loss: 3.0902 - val_mae: 6.0987 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 2.7757 - mae: 5.1059\n",
            "Epoch 00003: val_loss improved from 3.09017 to 2.84298, saving model to /home/jupyter/DaLia/fold7.h5\n",
            "1609/1609 [==============================] - 150s 93ms/step - loss: 2.7757 - mae: 5.1059 - val_loss: 2.8430 - val_mae: 5.1962 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 2.5723 - mae: 4.4937\n",
            "Epoch 00004: val_loss improved from 2.84298 to 2.68627, saving model to /home/jupyter/DaLia/fold7.h5\n",
            "1609/1609 [==============================] - 149s 93ms/step - loss: 2.5724 - mae: 4.4947 - val_loss: 2.6863 - val_mae: 4.7183 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 2.4513 - mae: 3.9958\n",
            "Epoch 00005: val_loss did not improve from 2.68627\n",
            "1609/1609 [==============================] - 153s 95ms/step - loss: 2.4513 - mae: 3.9958 - val_loss: 2.7260 - val_mae: 4.8166 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 2.3572 - mae: 3.6308\n",
            "Epoch 00006: val_loss did not improve from 2.68627\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1609/1609 [==============================] - 151s 94ms/step - loss: 2.3573 - mae: 3.6307 - val_loss: 2.7489 - val_mae: 5.4698 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 2.1752 - mae: 3.0739\n",
            "Epoch 00007: val_loss improved from 2.68627 to 2.63513, saving model to /home/jupyter/DaLia/fold7.h5\n",
            "1609/1609 [==============================] - 154s 95ms/step - loss: 2.1752 - mae: 3.0739 - val_loss: 2.6351 - val_mae: 4.5564 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 2.1050 - mae: 2.8038\n",
            "Epoch 00008: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 152s 94ms/step - loss: 2.1050 - mae: 2.8038 - val_loss: 2.7354 - val_mae: 4.2450 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 2.0598 - mae: 2.6953\n",
            "Epoch 00009: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1609/1609 [==============================] - 150s 94ms/step - loss: 2.0598 - mae: 2.6953 - val_loss: 2.8515 - val_mae: 4.4072 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.9655 - mae: 2.4111\n",
            "Epoch 00010: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 147s 91ms/step - loss: 1.9655 - mae: 2.4111 - val_loss: 2.7714 - val_mae: 3.9695 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.9226 - mae: 2.3110\n",
            "Epoch 00011: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1609/1609 [==============================] - 138s 86ms/step - loss: 1.9226 - mae: 2.3113 - val_loss: 2.7972 - val_mae: 4.1426 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.8671 - mae: 2.1843\n",
            "Epoch 00012: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 142s 88ms/step - loss: 1.8671 - mae: 2.1843 - val_loss: 2.8150 - val_mae: 3.9568 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.8503 - mae: 2.1334\n",
            "Epoch 00013: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1609/1609 [==============================] - 148s 92ms/step - loss: 1.8503 - mae: 2.1334 - val_loss: 2.8621 - val_mae: 3.8971 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.8228 - mae: 2.0644\n",
            "Epoch 00014: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 144s 89ms/step - loss: 1.8228 - mae: 2.0643 - val_loss: 2.8424 - val_mae: 3.8814 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.8109 - mae: 2.0573\n",
            "Epoch 00015: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1609/1609 [==============================] - 140s 87ms/step - loss: 1.8110 - mae: 2.0573 - val_loss: 2.8916 - val_mae: 3.7498 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7937 - mae: 1.9950\n",
            "Epoch 00016: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 148s 92ms/step - loss: 1.7937 - mae: 1.9950 - val_loss: 2.8552 - val_mae: 3.8662 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7906 - mae: 2.0149\n",
            "Epoch 00017: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1609/1609 [==============================] - 148s 92ms/step - loss: 1.7906 - mae: 2.0149 - val_loss: 2.8839 - val_mae: 3.8598 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7803 - mae: 1.9876\n",
            "Epoch 00018: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 145s 90ms/step - loss: 1.7803 - mae: 1.9876 - val_loss: 2.8633 - val_mae: 3.8544 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7788 - mae: 1.9866\n",
            "Epoch 00019: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1609/1609 [==============================] - 141s 88ms/step - loss: 1.7787 - mae: 1.9865 - val_loss: 2.9118 - val_mae: 3.9297 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7720 - mae: 1.9673\n",
            "Epoch 00020: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 150s 93ms/step - loss: 1.7720 - mae: 1.9673 - val_loss: 2.8985 - val_mae: 3.7940 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7734 - mae: 1.9698\n",
            "Epoch 00021: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1609/1609 [==============================] - 147s 91ms/step - loss: 1.7734 - mae: 1.9698 - val_loss: 2.9088 - val_mae: 3.8306 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7677 - mae: 1.9605\n",
            "Epoch 00022: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 141s 88ms/step - loss: 1.7677 - mae: 1.9610 - val_loss: 2.9481 - val_mae: 3.9097 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7671 - mae: 1.9560\n",
            "Epoch 00023: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1609/1609 [==============================] - 148s 92ms/step - loss: 1.7671 - mae: 1.9560 - val_loss: 2.9425 - val_mae: 3.8354 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7664 - mae: 1.9608\n",
            "Epoch 00024: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 142s 89ms/step - loss: 1.7664 - mae: 1.9609 - val_loss: 2.9298 - val_mae: 3.8389 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7631 - mae: 1.9545\n",
            "Epoch 00025: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1609/1609 [==============================] - 153s 95ms/step - loss: 1.7631 - mae: 1.9545 - val_loss: 2.9165 - val_mae: 3.8689 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7648 - mae: 1.9464\n",
            "Epoch 00026: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 150s 93ms/step - loss: 1.7648 - mae: 1.9464 - val_loss: 2.8896 - val_mae: 3.8188 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7654 - mae: 1.9594\n",
            "Epoch 00027: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1609/1609 [==============================] - 150s 93ms/step - loss: 1.7654 - mae: 1.9594 - val_loss: 2.9571 - val_mae: 3.9030 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7681 - mae: 1.9761\n",
            "Epoch 00028: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 150s 93ms/step - loss: 1.7681 - mae: 1.9761 - val_loss: 2.9373 - val_mae: 3.8363 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7675 - mae: 1.9630\n",
            "Epoch 00029: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1609/1609 [==============================] - 151s 94ms/step - loss: 1.7675 - mae: 1.9630 - val_loss: 2.8912 - val_mae: 3.8481 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7647 - mae: 1.9515\n",
            "Epoch 00030: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 151s 94ms/step - loss: 1.7647 - mae: 1.9515 - val_loss: 2.9440 - val_mae: 3.8517 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7607 - mae: 1.9509\n",
            "Epoch 00031: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1609/1609 [==============================] - 152s 94ms/step - loss: 1.7607 - mae: 1.9509 - val_loss: 2.9487 - val_mae: 3.8741 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7628 - mae: 1.9527\n",
            "Epoch 00032: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 151s 94ms/step - loss: 1.7628 - mae: 1.9526 - val_loss: 2.9444 - val_mae: 3.9184 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7631 - mae: 1.9562\n",
            "Epoch 00033: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1609/1609 [==============================] - 150s 93ms/step - loss: 1.7631 - mae: 1.9561 - val_loss: 2.9575 - val_mae: 3.8935 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7646 - mae: 1.9507\n",
            "Epoch 00034: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 146s 91ms/step - loss: 1.7646 - mae: 1.9508 - val_loss: 2.9321 - val_mae: 3.8780 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7597 - mae: 1.9379\n",
            "Epoch 00035: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1609/1609 [==============================] - 141s 88ms/step - loss: 1.7597 - mae: 1.9379 - val_loss: 2.9308 - val_mae: 3.8508 - lr: 6.1035e-08\n",
            "Epoch 36/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7642 - mae: 1.9658\n",
            "Epoch 00036: val_loss did not improve from 2.63513\n",
            "1609/1609 [==============================] - 149s 92ms/step - loss: 1.7646 - mae: 1.9665 - val_loss: 2.9131 - val_mae: 3.8875 - lr: 3.0518e-08\n",
            "Epoch 37/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7629 - mae: 1.9653Restoring model weights from the end of the best epoch: 7.\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 2.63513\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "1609/1609 [==============================] - 146s 91ms/step - loss: 1.7629 - mae: 1.9652 - val_loss: 2.9046 - val_mae: 3.8115 - lr: 3.0518e-08\n",
            "Epoch 00037: early stopping\n",
            "129/129 [==============================] - 4s 33ms/step - loss: 2.4638 - mae: 4.4050\n",
            "Fit model on training data fold  8\n",
            "Epoch 1/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 4.8563 - mae: 11.3223\n",
            "Epoch 00001: val_loss improved from inf to 3.78114, saving model to /home/jupyter/DaLia/fold8.h5\n",
            "1609/1609 [==============================] - 147s 86ms/step - loss: 4.8562 - mae: 11.3219 - val_loss: 3.7811 - val_mae: 9.6305 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 3.2489 - mae: 7.0639\n",
            "Epoch 00002: val_loss improved from 3.78114 to 3.29134, saving model to /home/jupyter/DaLia/fold8.h5\n",
            "1609/1609 [==============================] - 140s 87ms/step - loss: 3.2489 - mae: 7.0637 - val_loss: 3.2913 - val_mae: 6.7321 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 3.0590 - mae: 5.8754\n",
            "Epoch 00003: val_loss did not improve from 3.29134\n",
            "1609/1609 [==============================] - 143s 89ms/step - loss: 3.0590 - mae: 5.8754 - val_loss: 3.4051 - val_mae: 6.8818 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 2.7151 - mae: 4.6430\n",
            "Epoch 00004: val_loss improved from 3.29134 to 3.01277, saving model to /home/jupyter/DaLia/fold8.h5\n",
            "1609/1609 [==============================] - 142s 88ms/step - loss: 2.7152 - mae: 4.6431 - val_loss: 3.0128 - val_mae: 6.0922 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 2.5052 - mae: 4.0677\n",
            "Epoch 00005: val_loss improved from 3.01277 to 2.96806, saving model to /home/jupyter/DaLia/fold8.h5\n",
            "1609/1609 [==============================] - 141s 88ms/step - loss: 2.5053 - mae: 4.0679 - val_loss: 2.9681 - val_mae: 6.1855 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 2.3833 - mae: 3.6354\n",
            "Epoch 00006: val_loss did not improve from 2.96806\n",
            "1609/1609 [==============================] - 141s 88ms/step - loss: 2.3834 - mae: 3.6354 - val_loss: 3.1377 - val_mae: 6.3932 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 2.3127 - mae: 3.4257\n",
            "Epoch 00007: val_loss improved from 2.96806 to 2.88823, saving model to /home/jupyter/DaLia/fold8.h5\n",
            "1609/1609 [==============================] - 147s 91ms/step - loss: 2.3128 - mae: 3.4263 - val_loss: 2.8882 - val_mae: 5.6449 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 2.2523 - mae: 3.2445\n",
            "Epoch 00008: val_loss improved from 2.88823 to 2.84743, saving model to /home/jupyter/DaLia/fold8.h5\n",
            "1609/1609 [==============================] - 146s 91ms/step - loss: 2.2523 - mae: 3.2445 - val_loss: 2.8474 - val_mae: 5.4530 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 2.1902 - mae: 3.0213\n",
            "Epoch 00009: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 148s 92ms/step - loss: 2.1903 - mae: 3.0215 - val_loss: 2.8893 - val_mae: 5.3506 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 2.1492 - mae: 2.9066\n",
            "Epoch 00010: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1609/1609 [==============================] - 142s 88ms/step - loss: 2.1492 - mae: 2.9066 - val_loss: 2.8731 - val_mae: 5.2812 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 2.0157 - mae: 2.5423\n",
            "Epoch 00011: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 141s 87ms/step - loss: 2.0157 - mae: 2.5423 - val_loss: 3.0010 - val_mae: 4.8617 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.9566 - mae: 2.3621\n",
            "Epoch 00012: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1609/1609 [==============================] - 148s 92ms/step - loss: 1.9565 - mae: 2.3621 - val_loss: 2.9428 - val_mae: 4.7648 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.8709 - mae: 2.1365\n",
            "Epoch 00013: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 144s 90ms/step - loss: 1.8710 - mae: 2.1366 - val_loss: 3.0797 - val_mae: 4.7400 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.8431 - mae: 2.0700\n",
            "Epoch 00014: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1609/1609 [==============================] - 143s 89ms/step - loss: 1.8431 - mae: 2.0700 - val_loss: 3.2409 - val_mae: 4.7321 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7900 - mae: 1.9491\n",
            "Epoch 00015: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 142s 88ms/step - loss: 1.7900 - mae: 1.9491 - val_loss: 3.2462 - val_mae: 4.8626 - lr: 1.2500e-04\n",
            "Epoch 16/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7764 - mae: 1.9497\n",
            "Epoch 00016: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1609/1609 [==============================] - 147s 91ms/step - loss: 1.7764 - mae: 1.9497 - val_loss: 3.1691 - val_mae: 4.5913 - lr: 1.2500e-04\n",
            "Epoch 17/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7520 - mae: 1.8956\n",
            "Epoch 00017: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 146s 91ms/step - loss: 1.7520 - mae: 1.8956 - val_loss: 3.2329 - val_mae: 4.6948 - lr: 6.2500e-05\n",
            "Epoch 18/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7446 - mae: 1.8618\n",
            "Epoch 00018: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1609/1609 [==============================] - 145s 90ms/step - loss: 1.7447 - mae: 1.8618 - val_loss: 3.1937 - val_mae: 4.6417 - lr: 6.2500e-05\n",
            "Epoch 19/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7248 - mae: 1.8393\n",
            "Epoch 00019: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 144s 90ms/step - loss: 1.7248 - mae: 1.8393 - val_loss: 3.2155 - val_mae: 4.6353 - lr: 3.1250e-05\n",
            "Epoch 20/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7262 - mae: 1.8400\n",
            "Epoch 00020: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1609/1609 [==============================] - 142s 88ms/step - loss: 1.7262 - mae: 1.8400 - val_loss: 3.2325 - val_mae: 4.6204 - lr: 3.1250e-05\n",
            "Epoch 21/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7130 - mae: 1.8111\n",
            "Epoch 00021: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 144s 89ms/step - loss: 1.7131 - mae: 1.8116 - val_loss: 3.2521 - val_mae: 4.5220 - lr: 1.5625e-05\n",
            "Epoch 22/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7129 - mae: 1.8172\n",
            "Epoch 00022: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1609/1609 [==============================] - 144s 89ms/step - loss: 1.7130 - mae: 1.8172 - val_loss: 3.2505 - val_mae: 4.6422 - lr: 1.5625e-05\n",
            "Epoch 23/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7113 - mae: 1.7920\n",
            "Epoch 00023: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 146s 90ms/step - loss: 1.7113 - mae: 1.7920 - val_loss: 3.2319 - val_mae: 4.5506 - lr: 7.8125e-06\n",
            "Epoch 24/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7111 - mae: 1.8166\n",
            "Epoch 00024: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1609/1609 [==============================] - 142s 88ms/step - loss: 1.7111 - mae: 1.8166 - val_loss: 3.2439 - val_mae: 4.6116 - lr: 7.8125e-06\n",
            "Epoch 25/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7068 - mae: 1.8011\n",
            "Epoch 00025: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 144s 90ms/step - loss: 1.7068 - mae: 1.8011 - val_loss: 3.2812 - val_mae: 4.7216 - lr: 3.9063e-06\n",
            "Epoch 26/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7054 - mae: 1.7924\n",
            "Epoch 00026: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1609/1609 [==============================] - 147s 91ms/step - loss: 1.7054 - mae: 1.7924 - val_loss: 3.3733 - val_mae: 4.6970 - lr: 3.9063e-06\n",
            "Epoch 27/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7036 - mae: 1.8078\n",
            "Epoch 00027: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 147s 91ms/step - loss: 1.7035 - mae: 1.8079 - val_loss: 3.3067 - val_mae: 4.6550 - lr: 1.9531e-06\n",
            "Epoch 28/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7031 - mae: 1.7941\n",
            "Epoch 00028: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1609/1609 [==============================] - 145s 90ms/step - loss: 1.7032 - mae: 1.7942 - val_loss: 3.3431 - val_mae: 4.6908 - lr: 1.9531e-06\n",
            "Epoch 29/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7050 - mae: 1.7874\n",
            "Epoch 00029: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 147s 91ms/step - loss: 1.7049 - mae: 1.7873 - val_loss: 3.2704 - val_mae: 4.5886 - lr: 9.7656e-07\n",
            "Epoch 30/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7059 - mae: 1.8044\n",
            "Epoch 00030: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1609/1609 [==============================] - 149s 93ms/step - loss: 1.7059 - mae: 1.8044 - val_loss: 3.3122 - val_mae: 4.6003 - lr: 9.7656e-07\n",
            "Epoch 31/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7035 - mae: 1.7934\n",
            "Epoch 00031: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 147s 91ms/step - loss: 1.7035 - mae: 1.7934 - val_loss: 3.2795 - val_mae: 4.5529 - lr: 4.8828e-07\n",
            "Epoch 32/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7044 - mae: 1.8059\n",
            "Epoch 00032: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1609/1609 [==============================] - 146s 91ms/step - loss: 1.7044 - mae: 1.8059 - val_loss: 3.2560 - val_mae: 4.5071 - lr: 4.8828e-07\n",
            "Epoch 33/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7066 - mae: 1.7921\n",
            "Epoch 00033: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 145s 90ms/step - loss: 1.7066 - mae: 1.7921 - val_loss: 3.2850 - val_mae: 4.5491 - lr: 2.4414e-07\n",
            "Epoch 34/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7012 - mae: 1.7966\n",
            "Epoch 00034: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1609/1609 [==============================] - 140s 87ms/step - loss: 1.7012 - mae: 1.7966 - val_loss: 3.2599 - val_mae: 4.6573 - lr: 2.4414e-07\n",
            "Epoch 35/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7017 - mae: 1.7962\n",
            "Epoch 00035: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 141s 88ms/step - loss: 1.7018 - mae: 1.7963 - val_loss: 3.2765 - val_mae: 4.6805 - lr: 1.2207e-07\n",
            "Epoch 36/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7009 - mae: 1.7920\n",
            "Epoch 00036: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1609/1609 [==============================] - 143s 89ms/step - loss: 1.7010 - mae: 1.7921 - val_loss: 3.3102 - val_mae: 4.7282 - lr: 1.2207e-07\n",
            "Epoch 37/200\n",
            "1608/1609 [============================>.] - ETA: 0s - loss: 1.7070 - mae: 1.8018\n",
            "Epoch 00037: val_loss did not improve from 2.84743\n",
            "1609/1609 [==============================] - 142s 88ms/step - loss: 1.7070 - mae: 1.8018 - val_loss: 3.2499 - val_mae: 4.5600 - lr: 6.1035e-08\n",
            "Epoch 38/200\n",
            "1609/1609 [==============================] - ETA: 0s - loss: 1.7007 - mae: 1.8016Restoring model weights from the end of the best epoch: 8.\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 2.84743\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1609/1609 [==============================] - 145s 90ms/step - loss: 1.7007 - mae: 1.8016 - val_loss: 3.3017 - val_mae: 4.6158 - lr: 6.1035e-08\n",
            "Epoch 00038: early stopping\n",
            "143/143 [==============================] - 5s 32ms/step - loss: 2.0906 - mae: 2.4287\n",
            "Fit model on training data fold  9\n",
            "Epoch 1/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 4.5731 - mae: 10.2391\n",
            "Epoch 00001: val_loss improved from inf to 3.66583, saving model to /home/jupyter/DaLia/fold9.h5\n",
            "1627/1627 [==============================] - 163s 94ms/step - loss: 4.5731 - mae: 10.2391 - val_loss: 3.6658 - val_mae: 9.3772 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 3.0605 - mae: 6.0781\n",
            "Epoch 00002: val_loss did not improve from 3.66583\n",
            "1627/1627 [==============================] - 150s 92ms/step - loss: 3.0605 - mae: 6.0781 - val_loss: 4.4533 - val_mae: 11.2995 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.7264 - mae: 4.8009\n",
            "Epoch 00003: val_loss improved from 3.66583 to 3.24412, saving model to /home/jupyter/DaLia/fold9.h5\n",
            "1627/1627 [==============================] - 151s 93ms/step - loss: 2.7264 - mae: 4.8009 - val_loss: 3.2441 - val_mae: 7.9709 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.5379 - mae: 4.2361\n",
            "Epoch 00004: val_loss did not improve from 3.24412\n",
            "1627/1627 [==============================] - 152s 93ms/step - loss: 2.5379 - mae: 4.2361 - val_loss: 3.3788 - val_mae: 7.8860 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.4175 - mae: 3.7695\n",
            "Epoch 00005: val_loss improved from 3.24412 to 2.91827, saving model to /home/jupyter/DaLia/fold9.h5\n",
            "1627/1627 [==============================] - 152s 93ms/step - loss: 2.4175 - mae: 3.7695 - val_loss: 2.9183 - val_mae: 6.1668 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.3309 - mae: 3.4718\n",
            "Epoch 00006: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 147s 91ms/step - loss: 2.3309 - mae: 3.4718 - val_loss: 3.5971 - val_mae: 8.1553 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.2579 - mae: 3.2249\n",
            "Epoch 00007: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1627/1627 [==============================] - 142s 87ms/step - loss: 2.2579 - mae: 3.2249 - val_loss: 3.1831 - val_mae: 6.7684 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.0985 - mae: 2.7186\n",
            "Epoch 00008: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 152s 93ms/step - loss: 2.0985 - mae: 2.7186 - val_loss: 3.3975 - val_mae: 5.9666 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.0401 - mae: 2.5289\n",
            "Epoch 00009: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1627/1627 [==============================] - 146s 90ms/step - loss: 2.0401 - mae: 2.5289 - val_loss: 3.5695 - val_mae: 6.3716 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.9491 - mae: 2.3140\n",
            "Epoch 00010: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 142s 87ms/step - loss: 1.9491 - mae: 2.3140 - val_loss: 3.4788 - val_mae: 5.6139 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.9142 - mae: 2.1967\n",
            "Epoch 00011: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1627/1627 [==============================] - 144s 89ms/step - loss: 1.9142 - mae: 2.1967 - val_loss: 3.6672 - val_mae: 5.7740 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8614 - mae: 2.0893\n",
            "Epoch 00012: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 143s 88ms/step - loss: 1.8614 - mae: 2.0893 - val_loss: 3.8524 - val_mae: 6.1786 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8399 - mae: 2.0205\n",
            "Epoch 00013: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1627/1627 [==============================] - 143s 88ms/step - loss: 1.8399 - mae: 2.0205 - val_loss: 3.9244 - val_mae: 5.7996 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8107 - mae: 1.9698\n",
            "Epoch 00014: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 144s 89ms/step - loss: 1.8107 - mae: 1.9698 - val_loss: 3.9086 - val_mae: 5.8852 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7977 - mae: 1.9299\n",
            "Epoch 00015: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1627/1627 [==============================] - 146s 89ms/step - loss: 1.7977 - mae: 1.9299 - val_loss: 3.8654 - val_mae: 6.0762 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7856 - mae: 1.9244\n",
            "Epoch 00016: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 149s 91ms/step - loss: 1.7856 - mae: 1.9244 - val_loss: 3.9825 - val_mae: 5.8923 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7804 - mae: 1.9064\n",
            "Epoch 00017: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1627/1627 [==============================] - 139s 86ms/step - loss: 1.7804 - mae: 1.9064 - val_loss: 3.9604 - val_mae: 5.9819 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7756 - mae: 1.8951\n",
            "Epoch 00018: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 141s 87ms/step - loss: 1.7756 - mae: 1.8951 - val_loss: 4.0414 - val_mae: 5.8062 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7692 - mae: 1.8804\n",
            "Epoch 00019: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1627/1627 [==============================] - 149s 91ms/step - loss: 1.7692 - mae: 1.8804 - val_loss: 4.0237 - val_mae: 5.9407 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7680 - mae: 1.8938\n",
            "Epoch 00020: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 146s 90ms/step - loss: 1.7680 - mae: 1.8938 - val_loss: 4.0314 - val_mae: 5.9592 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7674 - mae: 1.8812\n",
            "Epoch 00021: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1627/1627 [==============================] - 148s 91ms/step - loss: 1.7674 - mae: 1.8812 - val_loss: 4.0939 - val_mae: 5.9421 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7631 - mae: 1.8588\n",
            "Epoch 00022: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 145s 89ms/step - loss: 1.7631 - mae: 1.8588 - val_loss: 4.0348 - val_mae: 5.8004 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7620 - mae: 1.8674\n",
            "Epoch 00023: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1627/1627 [==============================] - 142s 87ms/step - loss: 1.7620 - mae: 1.8674 - val_loss: 4.0279 - val_mae: 5.8084 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7567 - mae: 1.8719\n",
            "Epoch 00024: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 143s 88ms/step - loss: 1.7567 - mae: 1.8719 - val_loss: 4.1057 - val_mae: 5.9781 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7674 - mae: 1.8805\n",
            "Epoch 00025: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1627/1627 [==============================] - 138s 85ms/step - loss: 1.7674 - mae: 1.8805 - val_loss: 4.0184 - val_mae: 5.7878 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7625 - mae: 1.8749\n",
            "Epoch 00026: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 141s 87ms/step - loss: 1.7625 - mae: 1.8749 - val_loss: 4.0802 - val_mae: 5.8683 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7567 - mae: 1.8592\n",
            "Epoch 00027: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1627/1627 [==============================] - 144s 88ms/step - loss: 1.7567 - mae: 1.8592 - val_loss: 4.0693 - val_mae: 5.8824 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7570 - mae: 1.8522\n",
            "Epoch 00028: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 147s 90ms/step - loss: 1.7570 - mae: 1.8522 - val_loss: 3.9947 - val_mae: 5.9352 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7611 - mae: 1.8707\n",
            "Epoch 00029: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1627/1627 [==============================] - 150s 92ms/step - loss: 1.7611 - mae: 1.8707 - val_loss: 4.0109 - val_mae: 5.8130 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7610 - mae: 1.8528\n",
            "Epoch 00030: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 145s 89ms/step - loss: 1.7610 - mae: 1.8528 - val_loss: 3.9529 - val_mae: 5.7071 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7586 - mae: 1.8435\n",
            "Epoch 00031: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1627/1627 [==============================] - 138s 85ms/step - loss: 1.7586 - mae: 1.8435 - val_loss: 4.0490 - val_mae: 5.8718 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7556 - mae: 1.8452\n",
            "Epoch 00032: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 141s 87ms/step - loss: 1.7556 - mae: 1.8452 - val_loss: 4.0026 - val_mae: 5.8403 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7562 - mae: 1.8600\n",
            "Epoch 00033: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1627/1627 [==============================] - 145s 89ms/step - loss: 1.7562 - mae: 1.8600 - val_loss: 3.9490 - val_mae: 5.7413 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7560 - mae: 1.8572\n",
            "Epoch 00034: val_loss did not improve from 2.91827\n",
            "1627/1627 [==============================] - 146s 90ms/step - loss: 1.7560 - mae: 1.8572 - val_loss: 4.0293 - val_mae: 5.9470 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7595 - mae: 1.8728Restoring model weights from the end of the best epoch: 5.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 2.91827\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1627/1627 [==============================] - 138s 85ms/step - loss: 1.7595 - mae: 1.8728 - val_loss: 4.0484 - val_mae: 5.8265 - lr: 6.1035e-08\n",
            "Epoch 00035: early stopping\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 4.4356 - mae: 11.3155\n",
            "Fit model on training data fold  10\n",
            "Epoch 1/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 4.6581 - mae: 10.0062\n",
            "Epoch 00001: val_loss improved from inf to 4.54575, saving model to /home/jupyter/DaLia/fold10.h5\n",
            "1627/1627 [==============================] - 150s 87ms/step - loss: 4.6581 - mae: 10.0062 - val_loss: 4.5458 - val_mae: 12.2419 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 3.0595 - mae: 5.9173\n",
            "Epoch 00002: val_loss did not improve from 4.54575\n",
            "1627/1627 [==============================] - 138s 85ms/step - loss: 3.0595 - mae: 5.9173 - val_loss: 4.6845 - val_mae: 12.2994 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.7716 - mae: 4.8559\n",
            "Epoch 00003: val_loss improved from 4.54575 to 4.12479, saving model to /home/jupyter/DaLia/fold10.h5\n",
            "1627/1627 [==============================] - 142s 87ms/step - loss: 2.7716 - mae: 4.8559 - val_loss: 4.1248 - val_mae: 9.7968 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.5557 - mae: 4.2094\n",
            "Epoch 00004: val_loss did not improve from 4.12479\n",
            "1627/1627 [==============================] - 150s 92ms/step - loss: 2.5557 - mae: 4.2094 - val_loss: 4.4782 - val_mae: 10.4699 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.4577 - mae: 3.9143\n",
            "Epoch 00005: val_loss improved from 4.12479 to 4.03379, saving model to /home/jupyter/DaLia/fold10.h5\n",
            "1627/1627 [==============================] - 152s 93ms/step - loss: 2.4577 - mae: 3.9143 - val_loss: 4.0338 - val_mae: 9.7520 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.3694 - mae: 3.6168\n",
            "Epoch 00006: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 153s 94ms/step - loss: 2.3694 - mae: 3.6168 - val_loss: 4.4257 - val_mae: 9.2762 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.2841 - mae: 3.3050\n",
            "Epoch 00007: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1627/1627 [==============================] - 148s 91ms/step - loss: 2.2841 - mae: 3.3050 - val_loss: 4.7007 - val_mae: 9.5885 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.1147 - mae: 2.7832\n",
            "Epoch 00008: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 143s 88ms/step - loss: 2.1147 - mae: 2.7832 - val_loss: 5.0387 - val_mae: 9.0805 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.0633 - mae: 2.6098\n",
            "Epoch 00009: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1627/1627 [==============================] - 142s 87ms/step - loss: 2.0633 - mae: 2.6098 - val_loss: 4.7190 - val_mae: 9.2867 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.9612 - mae: 2.3274\n",
            "Epoch 00010: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 147s 90ms/step - loss: 1.9612 - mae: 2.3274 - val_loss: 5.3955 - val_mae: 8.8898 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.9255 - mae: 2.2367\n",
            "Epoch 00011: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1627/1627 [==============================] - 142s 87ms/step - loss: 1.9255 - mae: 2.2367 - val_loss: 5.6275 - val_mae: 9.0135 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8702 - mae: 2.1019\n",
            "Epoch 00012: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 150s 92ms/step - loss: 1.8702 - mae: 2.1019 - val_loss: 5.4720 - val_mae: 8.7214 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8494 - mae: 2.0499\n",
            "Epoch 00013: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1627/1627 [==============================] - 151s 93ms/step - loss: 1.8494 - mae: 2.0499 - val_loss: 5.5760 - val_mae: 8.8157 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8158 - mae: 1.9775\n",
            "Epoch 00014: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 151s 93ms/step - loss: 1.8158 - mae: 1.9775 - val_loss: 5.9728 - val_mae: 8.9208 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8087 - mae: 1.9488\n",
            "Epoch 00015: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1627/1627 [==============================] - 150s 92ms/step - loss: 1.8087 - mae: 1.9488 - val_loss: 6.0451 - val_mae: 8.9819 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7953 - mae: 1.9329\n",
            "Epoch 00016: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 149s 92ms/step - loss: 1.7953 - mae: 1.9329 - val_loss: 6.0552 - val_mae: 8.9428 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7926 - mae: 1.9293\n",
            "Epoch 00017: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1627/1627 [==============================] - 145s 89ms/step - loss: 1.7926 - mae: 1.9293 - val_loss: 6.1124 - val_mae: 9.0513 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7782 - mae: 1.9052\n",
            "Epoch 00018: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 137s 84ms/step - loss: 1.7782 - mae: 1.9052 - val_loss: 6.1289 - val_mae: 8.8326 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7765 - mae: 1.8909\n",
            "Epoch 00019: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1627/1627 [==============================] - 142s 87ms/step - loss: 1.7765 - mae: 1.8909 - val_loss: 6.1226 - val_mae: 8.8892 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7697 - mae: 1.8925\n",
            "Epoch 00020: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 144s 89ms/step - loss: 1.7697 - mae: 1.8925 - val_loss: 6.2081 - val_mae: 8.8191 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7692 - mae: 1.8834\n",
            "Epoch 00021: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1627/1627 [==============================] - 147s 90ms/step - loss: 1.7692 - mae: 1.8834 - val_loss: 6.2549 - val_mae: 8.8985 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7654 - mae: 1.8807\n",
            "Epoch 00022: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 142s 87ms/step - loss: 1.7654 - mae: 1.8807 - val_loss: 6.2106 - val_mae: 9.0412 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7654 - mae: 1.8798\n",
            "Epoch 00023: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1627/1627 [==============================] - 141s 87ms/step - loss: 1.7654 - mae: 1.8798 - val_loss: 6.1546 - val_mae: 8.7511 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7624 - mae: 1.8729\n",
            "Epoch 00024: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 144s 89ms/step - loss: 1.7624 - mae: 1.8729 - val_loss: 6.2283 - val_mae: 8.9110 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7694 - mae: 1.8816\n",
            "Epoch 00025: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1627/1627 [==============================] - 142s 87ms/step - loss: 1.7694 - mae: 1.8816 - val_loss: 6.1728 - val_mae: 8.9308 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7629 - mae: 1.8666\n",
            "Epoch 00026: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 145s 89ms/step - loss: 1.7629 - mae: 1.8666 - val_loss: 6.1904 - val_mae: 8.8533 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7629 - mae: 1.8600\n",
            "Epoch 00027: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1627/1627 [==============================] - 143s 88ms/step - loss: 1.7629 - mae: 1.8600 - val_loss: 6.2098 - val_mae: 8.8881 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7653 - mae: 1.8872\n",
            "Epoch 00028: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 145s 89ms/step - loss: 1.7653 - mae: 1.8872 - val_loss: 6.2396 - val_mae: 8.9666 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7697 - mae: 1.8854\n",
            "Epoch 00029: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1627/1627 [==============================] - 141s 87ms/step - loss: 1.7697 - mae: 1.8854 - val_loss: 6.2822 - val_mae: 8.8641 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7650 - mae: 1.8735\n",
            "Epoch 00030: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 147s 91ms/step - loss: 1.7650 - mae: 1.8735 - val_loss: 6.2608 - val_mae: 8.9588 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7594 - mae: 1.8662\n",
            "Epoch 00031: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1627/1627 [==============================] - 149s 92ms/step - loss: 1.7594 - mae: 1.8662 - val_loss: 6.2810 - val_mae: 8.9566 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7656 - mae: 1.8879\n",
            "Epoch 00032: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 146s 90ms/step - loss: 1.7656 - mae: 1.8879 - val_loss: 6.2313 - val_mae: 8.8318 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7641 - mae: 1.8715\n",
            "Epoch 00033: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1627/1627 [==============================] - 140s 86ms/step - loss: 1.7641 - mae: 1.8715 - val_loss: 6.1564 - val_mae: 8.8563 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7651 - mae: 1.8814\n",
            "Epoch 00034: val_loss did not improve from 4.03379\n",
            "1627/1627 [==============================] - 141s 87ms/step - loss: 1.7651 - mae: 1.8814 - val_loss: 6.2719 - val_mae: 8.8077 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.7647 - mae: 1.8877Restoring model weights from the end of the best epoch: 5.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 4.03379\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1627/1627 [==============================] - 150s 92ms/step - loss: 1.7647 - mae: 1.8877 - val_loss: 6.2307 - val_mae: 8.9326 - lr: 6.1035e-08\n",
            "Epoch 00035: early stopping\n",
            "124/124 [==============================] - 4s 32ms/step - loss: 2.7343 - mae: 4.3539\n",
            "Fit model on training data fold  11\n",
            "Epoch 1/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 5.0021 - mae: 10.9031\n",
            "Epoch 00001: val_loss improved from inf to 3.91111, saving model to /home/jupyter/DaLia/fold11.h5\n",
            "1627/1627 [==============================] - 157s 91ms/step - loss: 5.0021 - mae: 10.9031 - val_loss: 3.9111 - val_mae: 9.7721 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 3.0681 - mae: 6.2301\n",
            "Epoch 00002: val_loss improved from 3.91111 to 3.68077, saving model to /home/jupyter/DaLia/fold11.h5\n",
            "1627/1627 [==============================] - 142s 87ms/step - loss: 3.0681 - mae: 6.2301 - val_loss: 3.6808 - val_mae: 9.0829 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.7510 - mae: 5.0376\n",
            "Epoch 00003: val_loss improved from 3.68077 to 3.25065, saving model to /home/jupyter/DaLia/fold11.h5\n",
            "1627/1627 [==============================] - 146s 90ms/step - loss: 2.7510 - mae: 5.0376 - val_loss: 3.2507 - val_mae: 7.9933 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.5563 - mae: 4.3996\n",
            "Epoch 00004: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 137s 84ms/step - loss: 2.5563 - mae: 4.3996 - val_loss: 3.5289 - val_mae: 8.5951 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.4362 - mae: 3.9233\n",
            "Epoch 00005: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1627/1627 [==============================] - 138s 85ms/step - loss: 2.4362 - mae: 3.9233 - val_loss: 3.5544 - val_mae: 8.5999 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.2604 - mae: 3.3703\n",
            "Epoch 00006: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 142s 87ms/step - loss: 2.2604 - mae: 3.3703 - val_loss: 3.4945 - val_mae: 6.9228 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.1820 - mae: 3.0602\n",
            "Epoch 00007: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1627/1627 [==============================] - 147s 90ms/step - loss: 2.1820 - mae: 3.0602 - val_loss: 3.3543 - val_mae: 7.2138 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.0826 - mae: 2.7707\n",
            "Epoch 00008: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 149s 92ms/step - loss: 2.0826 - mae: 2.7707 - val_loss: 3.5872 - val_mae: 6.8856 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 2.0405 - mae: 2.5982\n",
            "Epoch 00009: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1627/1627 [==============================] - 147s 90ms/step - loss: 2.0405 - mae: 2.5982 - val_loss: 4.0006 - val_mae: 7.5619 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.9756 - mae: 2.4412\n",
            "Epoch 00010: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 146s 90ms/step - loss: 1.9756 - mae: 2.4412 - val_loss: 4.0249 - val_mae: 6.9775 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.9548 - mae: 2.3689\n",
            "Epoch 00011: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1627/1627 [==============================] - 146s 90ms/step - loss: 1.9548 - mae: 2.3689 - val_loss: 4.1556 - val_mae: 7.3255 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.9199 - mae: 2.2752\n",
            "Epoch 00012: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 149s 92ms/step - loss: 1.9199 - mae: 2.2752 - val_loss: 4.1238 - val_mae: 6.7087 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.9142 - mae: 2.2882\n",
            "Epoch 00013: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1627/1627 [==============================] - 149s 92ms/step - loss: 1.9142 - mae: 2.2882 - val_loss: 4.2954 - val_mae: 7.0000 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8935 - mae: 2.2335\n",
            "Epoch 00014: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 149s 92ms/step - loss: 1.8935 - mae: 2.2335 - val_loss: 4.2875 - val_mae: 6.9331 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8877 - mae: 2.2081\n",
            "Epoch 00015: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1627/1627 [==============================] - 144s 89ms/step - loss: 1.8877 - mae: 2.2081 - val_loss: 4.1910 - val_mae: 6.9111 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8769 - mae: 2.1918\n",
            "Epoch 00016: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 138s 85ms/step - loss: 1.8769 - mae: 2.1918 - val_loss: 4.2520 - val_mae: 6.8117 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8760 - mae: 2.2016\n",
            "Epoch 00017: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1627/1627 [==============================] - 143s 88ms/step - loss: 1.8760 - mae: 2.2016 - val_loss: 4.2535 - val_mae: 6.7891 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8678 - mae: 2.1682\n",
            "Epoch 00018: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 152s 93ms/step - loss: 1.8678 - mae: 2.1682 - val_loss: 4.2786 - val_mae: 6.8437 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8705 - mae: 2.1608\n",
            "Epoch 00019: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1627/1627 [==============================] - 147s 90ms/step - loss: 1.8705 - mae: 2.1608 - val_loss: 4.2412 - val_mae: 6.8353 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8692 - mae: 2.1667\n",
            "Epoch 00020: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 143s 88ms/step - loss: 1.8692 - mae: 2.1667 - val_loss: 4.2739 - val_mae: 6.7562 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8671 - mae: 2.1585\n",
            "Epoch 00021: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1627/1627 [==============================] - 151s 93ms/step - loss: 1.8671 - mae: 2.1585 - val_loss: 4.3409 - val_mae: 6.9673 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8634 - mae: 2.1481\n",
            "Epoch 00022: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 145s 89ms/step - loss: 1.8634 - mae: 2.1481 - val_loss: 4.2823 - val_mae: 6.7875 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8614 - mae: 2.1497\n",
            "Epoch 00023: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1627/1627 [==============================] - 142s 87ms/step - loss: 1.8614 - mae: 2.1497 - val_loss: 4.4957 - val_mae: 6.9973 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8625 - mae: 2.1401\n",
            "Epoch 00024: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 152s 93ms/step - loss: 1.8625 - mae: 2.1401 - val_loss: 4.2830 - val_mae: 6.8869 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8627 - mae: 2.1437\n",
            "Epoch 00025: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1627/1627 [==============================] - 148s 91ms/step - loss: 1.8627 - mae: 2.1437 - val_loss: 4.2358 - val_mae: 6.5686 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8587 - mae: 2.1421\n",
            "Epoch 00026: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 138s 85ms/step - loss: 1.8587 - mae: 2.1421 - val_loss: 4.2587 - val_mae: 6.8304 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8638 - mae: 2.1295\n",
            "Epoch 00027: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1627/1627 [==============================] - 138s 85ms/step - loss: 1.8638 - mae: 2.1295 - val_loss: 4.2606 - val_mae: 6.7055 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8602 - mae: 2.1255\n",
            "Epoch 00028: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 142s 88ms/step - loss: 1.8602 - mae: 2.1255 - val_loss: 4.3340 - val_mae: 6.9656 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8625 - mae: 2.1450\n",
            "Epoch 00029: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1627/1627 [==============================] - 151s 93ms/step - loss: 1.8625 - mae: 2.1450 - val_loss: 4.2831 - val_mae: 6.8276 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8624 - mae: 2.1405\n",
            "Epoch 00030: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 151s 93ms/step - loss: 1.8624 - mae: 2.1405 - val_loss: 4.2595 - val_mae: 6.6658 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8646 - mae: 2.1420\n",
            "Epoch 00031: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1627/1627 [==============================] - 147s 91ms/step - loss: 1.8646 - mae: 2.1420 - val_loss: 4.2566 - val_mae: 6.8343 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8599 - mae: 2.1584\n",
            "Epoch 00032: val_loss did not improve from 3.25065\n",
            "1627/1627 [==============================] - 142s 88ms/step - loss: 1.8599 - mae: 2.1584 - val_loss: 4.2211 - val_mae: 6.6003 - lr: 6.1035e-08\n",
            "Epoch 33/200\n",
            "1627/1627 [==============================] - ETA: 0s - loss: 1.8605 - mae: 2.1620Restoring model weights from the end of the best epoch: 3.\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 3.25065\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1627/1627 [==============================] - 143s 88ms/step - loss: 1.8605 - mae: 2.1620 - val_loss: 4.2619 - val_mae: 6.8269 - lr: 6.1035e-08\n",
            "Epoch 00033: early stopping\n",
            "146/146 [==============================] - 4s 24ms/step - loss: 3.6633 - mae: 10.5466\n",
            "Fit model on training data fold  12\n",
            "Epoch 1/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 5.2292 - mae: 11.4342\n",
            "Epoch 00001: val_loss improved from inf to 2.64040, saving model to /home/jupyter/DaLia/fold12.h5\n",
            "1616/1616 [==============================] - 163s 95ms/step - loss: 5.2292 - mae: 11.4342 - val_loss: 2.6404 - val_mae: 4.7396 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.9034 - mae: 6.0625\n",
            "Epoch 00002: val_loss did not improve from 2.64040\n",
            "1616/1616 [==============================] - 154s 95ms/step - loss: 2.9034 - mae: 6.0625 - val_loss: 2.6454 - val_mae: 3.8666 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.6904 - mae: 5.1633\n",
            "Epoch 00003: val_loss improved from 2.64040 to 2.40703, saving model to /home/jupyter/DaLia/fold12.h5\n",
            "1616/1616 [==============================] - 152s 94ms/step - loss: 2.6904 - mae: 5.1633 - val_loss: 2.4070 - val_mae: 3.3471 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.5392 - mae: 4.4851\n",
            "Epoch 00004: val_loss improved from 2.40703 to 2.35804, saving model to /home/jupyter/DaLia/fold12.h5\n",
            "1616/1616 [==============================] - 146s 91ms/step - loss: 2.5392 - mae: 4.4851 - val_loss: 2.3580 - val_mae: 2.9453 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.4408 - mae: 4.0899\n",
            "Epoch 00005: val_loss did not improve from 2.35804\n",
            "1616/1616 [==============================] - 142s 88ms/step - loss: 2.4408 - mae: 4.0899 - val_loss: 2.5392 - val_mae: 3.2953 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.3744 - mae: 3.8165\n",
            "Epoch 00006: val_loss did not improve from 2.35804\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1616/1616 [==============================] - 150s 93ms/step - loss: 2.3744 - mae: 3.8165 - val_loss: 2.5473 - val_mae: 3.2453 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.2289 - mae: 3.3562\n",
            "Epoch 00007: val_loss improved from 2.35804 to 2.15351, saving model to /home/jupyter/DaLia/fold12.h5\n",
            "1616/1616 [==============================] - 150s 93ms/step - loss: 2.2289 - mae: 3.3562 - val_loss: 2.1535 - val_mae: 2.6880 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.1442 - mae: 3.0382\n",
            "Epoch 00008: val_loss improved from 2.15351 to 2.13513, saving model to /home/jupyter/DaLia/fold12.h5\n",
            "1616/1616 [==============================] - 151s 93ms/step - loss: 2.1442 - mae: 3.0382 - val_loss: 2.1351 - val_mae: 2.5984 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.0964 - mae: 2.8521\n",
            "Epoch 00009: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 151s 93ms/step - loss: 2.0964 - mae: 2.8521 - val_loss: 2.1944 - val_mae: 2.4946 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.0673 - mae: 2.7454\n",
            "Epoch 00010: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1616/1616 [==============================] - 153s 94ms/step - loss: 2.0673 - mae: 2.7454 - val_loss: 2.1937 - val_mae: 2.5177 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.9764 - mae: 2.4899\n",
            "Epoch 00011: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 150s 93ms/step - loss: 1.9764 - mae: 2.4899 - val_loss: 2.2164 - val_mae: 2.5184 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.9378 - mae: 2.3786\n",
            "Epoch 00012: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1616/1616 [==============================] - 150s 93ms/step - loss: 1.9378 - mae: 2.3786 - val_loss: 2.2712 - val_mae: 2.4028 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8869 - mae: 2.2297\n",
            "Epoch 00013: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 147s 91ms/step - loss: 1.8869 - mae: 2.2297 - val_loss: 2.2361 - val_mae: 2.5000 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8676 - mae: 2.1935\n",
            "Epoch 00014: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1616/1616 [==============================] - 142s 88ms/step - loss: 1.8676 - mae: 2.1935 - val_loss: 2.2840 - val_mae: 2.3735 - lr: 1.2500e-04\n",
            "Epoch 15/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8337 - mae: 2.1130\n",
            "Epoch 00015: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 148s 92ms/step - loss: 1.8337 - mae: 2.1130 - val_loss: 2.2743 - val_mae: 2.3284 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8212 - mae: 2.0828\n",
            "Epoch 00016: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1616/1616 [==============================] - 143s 89ms/step - loss: 1.8212 - mae: 2.0828 - val_loss: 2.2807 - val_mae: 2.3833 - lr: 6.2500e-05\n",
            "Epoch 17/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8073 - mae: 2.0640\n",
            "Epoch 00017: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 151s 93ms/step - loss: 1.8073 - mae: 2.0640 - val_loss: 2.2870 - val_mae: 2.3237 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8014 - mae: 2.0523\n",
            "Epoch 00018: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1616/1616 [==============================] - 148s 91ms/step - loss: 1.8014 - mae: 2.0523 - val_loss: 2.2785 - val_mae: 2.3148 - lr: 3.1250e-05\n",
            "Epoch 19/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7889 - mae: 2.0166\n",
            "Epoch 00019: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 149s 92ms/step - loss: 1.7889 - mae: 2.0166 - val_loss: 2.2751 - val_mae: 2.3450 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7921 - mae: 2.0334\n",
            "Epoch 00020: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1616/1616 [==============================] - 148s 92ms/step - loss: 1.7921 - mae: 2.0334 - val_loss: 2.2722 - val_mae: 2.3371 - lr: 1.5625e-05\n",
            "Epoch 21/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7879 - mae: 2.0325\n",
            "Epoch 00021: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 150s 93ms/step - loss: 1.7879 - mae: 2.0325 - val_loss: 2.2861 - val_mae: 2.3313 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7851 - mae: 2.0109\n",
            "Epoch 00022: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1616/1616 [==============================] - 146s 91ms/step - loss: 1.7851 - mae: 2.0109 - val_loss: 2.3035 - val_mae: 2.3340 - lr: 7.8125e-06\n",
            "Epoch 23/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7786 - mae: 2.0045\n",
            "Epoch 00023: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 142s 88ms/step - loss: 1.7786 - mae: 2.0045 - val_loss: 2.2869 - val_mae: 2.2998 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7786 - mae: 2.0109\n",
            "Epoch 00024: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1616/1616 [==============================] - 152s 94ms/step - loss: 1.7786 - mae: 2.0109 - val_loss: 2.2830 - val_mae: 2.3249 - lr: 3.9063e-06\n",
            "Epoch 25/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7771 - mae: 1.9855\n",
            "Epoch 00025: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 153s 95ms/step - loss: 1.7771 - mae: 1.9855 - val_loss: 2.2785 - val_mae: 2.3143 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7753 - mae: 1.9638\n",
            "Epoch 00026: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1616/1616 [==============================] - 152s 94ms/step - loss: 1.7753 - mae: 1.9638 - val_loss: 2.2734 - val_mae: 2.2864 - lr: 1.9531e-06\n",
            "Epoch 27/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7791 - mae: 2.0005\n",
            "Epoch 00027: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 153s 94ms/step - loss: 1.7791 - mae: 2.0005 - val_loss: 2.2850 - val_mae: 2.3047 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7765 - mae: 1.9952\n",
            "Epoch 00028: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1616/1616 [==============================] - 151s 94ms/step - loss: 1.7765 - mae: 1.9952 - val_loss: 2.2700 - val_mae: 2.2958 - lr: 9.7656e-07\n",
            "Epoch 29/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7813 - mae: 2.0004\n",
            "Epoch 00029: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 155s 96ms/step - loss: 1.7813 - mae: 2.0004 - val_loss: 2.3007 - val_mae: 2.3385 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7801 - mae: 2.0102\n",
            "Epoch 00030: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1616/1616 [==============================] - 152s 94ms/step - loss: 1.7801 - mae: 2.0102 - val_loss: 2.2903 - val_mae: 2.3602 - lr: 4.8828e-07\n",
            "Epoch 31/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7759 - mae: 2.0023\n",
            "Epoch 00031: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 149s 92ms/step - loss: 1.7759 - mae: 2.0023 - val_loss: 2.2868 - val_mae: 2.3010 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7803 - mae: 2.0017\n",
            "Epoch 00032: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1616/1616 [==============================] - 143s 88ms/step - loss: 1.7803 - mae: 2.0017 - val_loss: 2.2924 - val_mae: 2.3522 - lr: 2.4414e-07\n",
            "Epoch 33/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7765 - mae: 1.9800\n",
            "Epoch 00033: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 149s 92ms/step - loss: 1.7765 - mae: 1.9800 - val_loss: 2.2769 - val_mae: 2.3137 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7767 - mae: 1.9936\n",
            "Epoch 00034: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1616/1616 [==============================] - 143s 88ms/step - loss: 1.7767 - mae: 1.9936 - val_loss: 2.2856 - val_mae: 2.3496 - lr: 1.2207e-07\n",
            "Epoch 35/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7784 - mae: 1.9920\n",
            "Epoch 00035: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 153s 95ms/step - loss: 1.7784 - mae: 1.9920 - val_loss: 2.2858 - val_mae: 2.3500 - lr: 6.1035e-08\n",
            "Epoch 36/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7783 - mae: 2.0103\n",
            "Epoch 00036: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1616/1616 [==============================] - 155s 96ms/step - loss: 1.7783 - mae: 2.0103 - val_loss: 2.2845 - val_mae: 2.3464 - lr: 6.1035e-08\n",
            "Epoch 37/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7760 - mae: 2.0150\n",
            "Epoch 00037: val_loss did not improve from 2.13513\n",
            "1616/1616 [==============================] - 148s 92ms/step - loss: 1.7760 - mae: 2.0150 - val_loss: 2.2805 - val_mae: 2.3470 - lr: 3.0518e-08\n",
            "Epoch 38/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7742 - mae: 1.9824Restoring model weights from the end of the best epoch: 8.\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 2.13513\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "1616/1616 [==============================] - 143s 88ms/step - loss: 1.7742 - mae: 1.9824 - val_loss: 2.2764 - val_mae: 2.3122 - lr: 3.0518e-08\n",
            "Epoch 00038: early stopping\n",
            "124/124 [==============================] - 4s 34ms/step - loss: 3.1543 - mae: 7.4731\n",
            "Fit model on training data fold  13\n",
            "Epoch 1/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 5.7407 - mae: 10.5684\n",
            "Epoch 00001: val_loss improved from inf to 3.52635, saving model to /home/jupyter/DaLia/fold13.h5\n",
            "1616/1616 [==============================] - 166s 97ms/step - loss: 5.7407 - mae: 10.5684 - val_loss: 3.5263 - val_mae: 8.3814 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 3.0610 - mae: 6.3880\n",
            "Epoch 00002: val_loss improved from 3.52635 to 3.31883, saving model to /home/jupyter/DaLia/fold13.h5\n",
            "1616/1616 [==============================] - 155s 96ms/step - loss: 3.0610 - mae: 6.3880 - val_loss: 3.3188 - val_mae: 8.6302 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.7281 - mae: 5.1335\n",
            "Epoch 00003: val_loss improved from 3.31883 to 2.95803, saving model to /home/jupyter/DaLia/fold13.h5\n",
            "1616/1616 [==============================] - 157s 97ms/step - loss: 2.7281 - mae: 5.1335 - val_loss: 2.9580 - val_mae: 6.2537 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.5639 - mae: 4.5133\n",
            "Epoch 00004: val_loss did not improve from 2.95803\n",
            "1616/1616 [==============================] - 155s 96ms/step - loss: 2.5639 - mae: 4.5133 - val_loss: 2.9795 - val_mae: 6.8341 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.4967 - mae: 4.3069\n",
            "Epoch 00005: val_loss improved from 2.95803 to 2.73603, saving model to /home/jupyter/DaLia/fold13.h5\n",
            "1616/1616 [==============================] - 153s 95ms/step - loss: 2.4967 - mae: 4.3069 - val_loss: 2.7360 - val_mae: 5.5909 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.3870 - mae: 3.8278\n",
            "Epoch 00006: val_loss did not improve from 2.73603\n",
            "1616/1616 [==============================] - 148s 91ms/step - loss: 2.3870 - mae: 3.8278 - val_loss: 2.8145 - val_mae: 5.7335 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.3059 - mae: 3.5216\n",
            "Epoch 00007: val_loss improved from 2.73603 to 2.67090, saving model to /home/jupyter/DaLia/fold13.h5\n",
            "1616/1616 [==============================] - 138s 85ms/step - loss: 2.3059 - mae: 3.5216 - val_loss: 2.6709 - val_mae: 5.4630 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.2470 - mae: 3.2904\n",
            "Epoch 00008: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 141s 88ms/step - loss: 2.2470 - mae: 3.2904 - val_loss: 2.7634 - val_mae: 5.2135 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.1847 - mae: 3.0810\n",
            "Epoch 00009: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1616/1616 [==============================] - 150s 93ms/step - loss: 2.1847 - mae: 3.0810 - val_loss: 2.7601 - val_mae: 5.1537 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.0412 - mae: 2.6394\n",
            "Epoch 00010: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 145s 90ms/step - loss: 2.0412 - mae: 2.6394 - val_loss: 2.8368 - val_mae: 5.4488 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.9843 - mae: 2.4358\n",
            "Epoch 00011: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1616/1616 [==============================] - 138s 85ms/step - loss: 1.9843 - mae: 2.4358 - val_loss: 2.7418 - val_mae: 4.5182 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8965 - mae: 2.2314\n",
            "Epoch 00012: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 141s 87ms/step - loss: 1.8965 - mae: 2.2314 - val_loss: 2.8840 - val_mae: 4.7380 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8604 - mae: 2.1271\n",
            "Epoch 00013: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1616/1616 [==============================] - 150s 93ms/step - loss: 1.8604 - mae: 2.1271 - val_loss: 3.0341 - val_mae: 4.8327 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8059 - mae: 2.0241\n",
            "Epoch 00014: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 149s 92ms/step - loss: 1.8059 - mae: 2.0241 - val_loss: 3.0959 - val_mae: 4.7950 - lr: 1.2500e-04\n",
            "Epoch 15/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7902 - mae: 1.9657\n",
            "Epoch 00015: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1616/1616 [==============================] - 146s 90ms/step - loss: 1.7902 - mae: 1.9657 - val_loss: 3.1393 - val_mae: 4.7185 - lr: 1.2500e-04\n",
            "Epoch 16/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7623 - mae: 1.9160\n",
            "Epoch 00016: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 146s 91ms/step - loss: 1.7623 - mae: 1.9160 - val_loss: 3.1226 - val_mae: 4.7157 - lr: 6.2500e-05\n",
            "Epoch 17/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7491 - mae: 1.8691\n",
            "Epoch 00017: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1616/1616 [==============================] - 142s 88ms/step - loss: 1.7491 - mae: 1.8691 - val_loss: 3.1685 - val_mae: 4.7357 - lr: 6.2500e-05\n",
            "Epoch 18/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7338 - mae: 1.8555\n",
            "Epoch 00018: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 141s 87ms/step - loss: 1.7338 - mae: 1.8555 - val_loss: 3.0823 - val_mae: 4.4893 - lr: 3.1250e-05\n",
            "Epoch 19/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7411 - mae: 1.8667\n",
            "Epoch 00019: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1616/1616 [==============================] - 149s 92ms/step - loss: 1.7411 - mae: 1.8667 - val_loss: 3.1653 - val_mae: 4.6518 - lr: 3.1250e-05\n",
            "Epoch 20/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7219 - mae: 1.8369\n",
            "Epoch 00020: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 148s 92ms/step - loss: 1.7219 - mae: 1.8369 - val_loss: 3.0995 - val_mae: 4.6226 - lr: 1.5625e-05\n",
            "Epoch 21/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7193 - mae: 1.8382\n",
            "Epoch 00021: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1616/1616 [==============================] - 143s 89ms/step - loss: 1.7193 - mae: 1.8382 - val_loss: 3.0954 - val_mae: 4.5283 - lr: 1.5625e-05\n",
            "Epoch 22/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7171 - mae: 1.8320\n",
            "Epoch 00022: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 144s 89ms/step - loss: 1.7171 - mae: 1.8320 - val_loss: 3.1202 - val_mae: 4.6194 - lr: 7.8125e-06\n",
            "Epoch 23/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7169 - mae: 1.8244\n",
            "Epoch 00023: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1616/1616 [==============================] - 151s 93ms/step - loss: 1.7169 - mae: 1.8244 - val_loss: 3.1144 - val_mae: 4.5698 - lr: 7.8125e-06\n",
            "Epoch 24/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7191 - mae: 1.8190\n",
            "Epoch 00024: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 148s 91ms/step - loss: 1.7191 - mae: 1.8190 - val_loss: 3.0756 - val_mae: 4.4346 - lr: 3.9063e-06\n",
            "Epoch 25/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7100 - mae: 1.8132\n",
            "Epoch 00025: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1616/1616 [==============================] - 142s 88ms/step - loss: 1.7100 - mae: 1.8132 - val_loss: 3.1329 - val_mae: 4.5694 - lr: 3.9063e-06\n",
            "Epoch 26/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7137 - mae: 1.8254\n",
            "Epoch 00026: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 151s 94ms/step - loss: 1.7137 - mae: 1.8254 - val_loss: 3.1510 - val_mae: 4.5674 - lr: 1.9531e-06\n",
            "Epoch 27/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7127 - mae: 1.8022\n",
            "Epoch 00027: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1616/1616 [==============================] - 147s 91ms/step - loss: 1.7127 - mae: 1.8022 - val_loss: 3.1566 - val_mae: 4.6108 - lr: 1.9531e-06\n",
            "Epoch 28/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7108 - mae: 1.8125\n",
            "Epoch 00028: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 138s 85ms/step - loss: 1.7108 - mae: 1.8125 - val_loss: 3.1257 - val_mae: 4.6047 - lr: 9.7656e-07\n",
            "Epoch 29/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7136 - mae: 1.8057\n",
            "Epoch 00029: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1616/1616 [==============================] - 142s 88ms/step - loss: 1.7136 - mae: 1.8057 - val_loss: 3.1609 - val_mae: 4.6730 - lr: 9.7656e-07\n",
            "Epoch 30/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7093 - mae: 1.8027\n",
            "Epoch 00030: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 150s 93ms/step - loss: 1.7093 - mae: 1.8027 - val_loss: 3.1758 - val_mae: 4.7218 - lr: 4.8828e-07\n",
            "Epoch 31/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7110 - mae: 1.8145\n",
            "Epoch 00031: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1616/1616 [==============================] - 150s 93ms/step - loss: 1.7110 - mae: 1.8145 - val_loss: 3.1720 - val_mae: 4.6111 - lr: 4.8828e-07\n",
            "Epoch 32/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7123 - mae: 1.8104\n",
            "Epoch 00032: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 146s 90ms/step - loss: 1.7123 - mae: 1.8104 - val_loss: 3.1481 - val_mae: 4.5656 - lr: 2.4414e-07\n",
            "Epoch 33/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7085 - mae: 1.8109\n",
            "Epoch 00033: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1616/1616 [==============================] - 142s 88ms/step - loss: 1.7085 - mae: 1.8109 - val_loss: 3.1364 - val_mae: 4.5845 - lr: 2.4414e-07\n",
            "Epoch 34/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7094 - mae: 1.8178\n",
            "Epoch 00034: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 152s 94ms/step - loss: 1.7094 - mae: 1.8178 - val_loss: 3.1486 - val_mae: 4.6002 - lr: 1.2207e-07\n",
            "Epoch 35/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7065 - mae: 1.7993\n",
            "Epoch 00035: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1616/1616 [==============================] - 153s 95ms/step - loss: 1.7065 - mae: 1.7993 - val_loss: 3.1553 - val_mae: 4.5774 - lr: 1.2207e-07\n",
            "Epoch 36/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7079 - mae: 1.7978\n",
            "Epoch 00036: val_loss did not improve from 2.67090\n",
            "1616/1616 [==============================] - 150s 93ms/step - loss: 1.7079 - mae: 1.7978 - val_loss: 3.0991 - val_mae: 4.4927 - lr: 6.1035e-08\n",
            "Epoch 37/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7145 - mae: 1.8172Restoring model weights from the end of the best epoch: 7.\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 2.67090\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1616/1616 [==============================] - 151s 94ms/step - loss: 1.7145 - mae: 1.8172 - val_loss: 3.2070 - val_mae: 4.6647 - lr: 6.1035e-08\n",
            "Epoch 00037: early stopping\n",
            "146/146 [==============================] - 5s 32ms/step - loss: 2.2058 - mae: 2.5329\n",
            "Fit model on training data fold  14\n",
            "Epoch 1/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 4.9931 - mae: 10.4232\n",
            "Epoch 00001: val_loss improved from inf to 3.99382, saving model to /home/jupyter/DaLia/fold14.h5\n",
            "1616/1616 [==============================] - 157s 91ms/step - loss: 4.9931 - mae: 10.4232 - val_loss: 3.9938 - val_mae: 10.9779 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 3.2010 - mae: 6.8129\n",
            "Epoch 00002: val_loss improved from 3.99382 to 3.55245, saving model to /home/jupyter/DaLia/fold14.h5\n",
            "1616/1616 [==============================] - 142s 88ms/step - loss: 3.2010 - mae: 6.8129 - val_loss: 3.5525 - val_mae: 7.7985 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.8882 - mae: 5.3272\n",
            "Epoch 00003: val_loss improved from 3.55245 to 3.16262, saving model to /home/jupyter/DaLia/fold14.h5\n",
            "1616/1616 [==============================] - 150s 93ms/step - loss: 2.8882 - mae: 5.3272 - val_loss: 3.1626 - val_mae: 6.3319 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.6055 - mae: 4.4388\n",
            "Epoch 00004: val_loss improved from 3.16262 to 2.70127, saving model to /home/jupyter/DaLia/fold14.h5\n",
            "1616/1616 [==============================] - 151s 93ms/step - loss: 2.6055 - mae: 4.4388 - val_loss: 2.7013 - val_mae: 4.7102 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.4682 - mae: 4.0392\n",
            "Epoch 00005: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 150s 93ms/step - loss: 2.4682 - mae: 4.0392 - val_loss: 2.7320 - val_mae: 5.1904 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.3833 - mae: 3.7351\n",
            "Epoch 00006: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1616/1616 [==============================] - 149s 92ms/step - loss: 2.3833 - mae: 3.7351 - val_loss: 2.8010 - val_mae: 4.8068 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.1934 - mae: 3.1241\n",
            "Epoch 00007: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 145s 90ms/step - loss: 2.1934 - mae: 3.1241 - val_loss: 2.7797 - val_mae: 4.8298 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.1281 - mae: 2.8545\n",
            "Epoch 00008: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1616/1616 [==============================] - 137s 85ms/step - loss: 2.1281 - mae: 2.8545 - val_loss: 2.7963 - val_mae: 4.6454 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 2.0193 - mae: 2.5653\n",
            "Epoch 00009: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 141s 88ms/step - loss: 2.0193 - mae: 2.5653 - val_loss: 2.9579 - val_mae: 4.9579 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.9776 - mae: 2.4171\n",
            "Epoch 00010: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1616/1616 [==============================] - 149s 92ms/step - loss: 1.9776 - mae: 2.4171 - val_loss: 3.1653 - val_mae: 5.2547 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.9145 - mae: 2.2600\n",
            "Epoch 00011: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 141s 87ms/step - loss: 1.9145 - mae: 2.2600 - val_loss: 3.1164 - val_mae: 4.8018 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8938 - mae: 2.2206\n",
            "Epoch 00012: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1616/1616 [==============================] - 146s 90ms/step - loss: 1.8938 - mae: 2.2206 - val_loss: 3.0431 - val_mae: 4.4784 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8557 - mae: 2.1313\n",
            "Epoch 00013: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 143s 88ms/step - loss: 1.8557 - mae: 2.1313 - val_loss: 3.1161 - val_mae: 4.5550 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8480 - mae: 2.1146\n",
            "Epoch 00014: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "1616/1616 [==============================] - 151s 93ms/step - loss: 1.8480 - mae: 2.1146 - val_loss: 3.1548 - val_mae: 4.7854 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8273 - mae: 2.0665\n",
            "Epoch 00015: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 148s 91ms/step - loss: 1.8273 - mae: 2.0665 - val_loss: 3.2104 - val_mae: 4.8324 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8208 - mae: 2.0544\n",
            "Epoch 00016: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1616/1616 [==============================] - 146s 90ms/step - loss: 1.8208 - mae: 2.0544 - val_loss: 3.1770 - val_mae: 4.6956 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8128 - mae: 2.0237\n",
            "Epoch 00017: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 143s 88ms/step - loss: 1.8128 - mae: 2.0237 - val_loss: 3.3002 - val_mae: 4.7628 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8093 - mae: 2.0161\n",
            "Epoch 00018: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "1616/1616 [==============================] - 151s 93ms/step - loss: 1.8093 - mae: 2.0161 - val_loss: 3.2230 - val_mae: 4.6451 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8082 - mae: 2.0253\n",
            "Epoch 00019: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 148s 91ms/step - loss: 1.8082 - mae: 2.0253 - val_loss: 3.1106 - val_mae: 4.4041 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8049 - mae: 2.0116\n",
            "Epoch 00020: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1616/1616 [==============================] - 142s 88ms/step - loss: 1.8049 - mae: 2.0116 - val_loss: 3.2896 - val_mae: 4.9498 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7994 - mae: 2.0197\n",
            "Epoch 00021: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 151s 93ms/step - loss: 1.7994 - mae: 2.0197 - val_loss: 3.2126 - val_mae: 4.5618 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7970 - mae: 1.9978\n",
            "Epoch 00022: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "1616/1616 [==============================] - 150s 93ms/step - loss: 1.7970 - mae: 1.9978 - val_loss: 3.1556 - val_mae: 4.4827 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7998 - mae: 2.0010\n",
            "Epoch 00023: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 147s 91ms/step - loss: 1.7998 - mae: 2.0010 - val_loss: 3.1595 - val_mae: 4.6061 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7982 - mae: 1.9929\n",
            "Epoch 00024: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1616/1616 [==============================] - 143s 88ms/step - loss: 1.7982 - mae: 1.9929 - val_loss: 3.2490 - val_mae: 4.7007 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.8029 - mae: 2.0018\n",
            "Epoch 00025: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 151s 94ms/step - loss: 1.8029 - mae: 2.0018 - val_loss: 3.2164 - val_mae: 4.6263 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7944 - mae: 1.9997\n",
            "Epoch 00026: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "1616/1616 [==============================] - 154s 95ms/step - loss: 1.7944 - mae: 1.9997 - val_loss: 3.1174 - val_mae: 4.4277 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7971 - mae: 2.0065\n",
            "Epoch 00027: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 153s 95ms/step - loss: 1.7971 - mae: 2.0065 - val_loss: 3.2153 - val_mae: 4.6653 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7993 - mae: 2.0114\n",
            "Epoch 00028: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1616/1616 [==============================] - 149s 92ms/step - loss: 1.7993 - mae: 2.0114 - val_loss: 3.1573 - val_mae: 4.4787 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7972 - mae: 2.0093\n",
            "Epoch 00029: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 143s 89ms/step - loss: 1.7972 - mae: 2.0093 - val_loss: 3.2983 - val_mae: 4.7964 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7998 - mae: 1.9983\n",
            "Epoch 00030: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "1616/1616 [==============================] - 149s 92ms/step - loss: 1.7998 - mae: 1.9983 - val_loss: 3.3157 - val_mae: 4.7757 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7986 - mae: 2.0002\n",
            "Epoch 00031: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 143s 88ms/step - loss: 1.7986 - mae: 2.0002 - val_loss: 3.2157 - val_mae: 4.5810 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7982 - mae: 1.9816\n",
            "Epoch 00032: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1616/1616 [==============================] - 148s 92ms/step - loss: 1.7982 - mae: 1.9816 - val_loss: 3.2006 - val_mae: 4.5506 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7949 - mae: 2.0040\n",
            "Epoch 00033: val_loss did not improve from 2.70127\n",
            "1616/1616 [==============================] - 143s 89ms/step - loss: 1.7949 - mae: 2.0040 - val_loss: 3.1765 - val_mae: 4.6147 - lr: 6.1035e-08\n",
            "Epoch 34/200\n",
            "1616/1616 [==============================] - ETA: 0s - loss: 1.7975 - mae: 2.0105Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 2.70127\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "1616/1616 [==============================] - 150s 93ms/step - loss: 1.7975 - mae: 2.0105 - val_loss: 3.2845 - val_mae: 4.6492 - lr: 6.1035e-08\n",
            "Epoch 00034: early stopping\n",
            "137/137 [==============================] - 3s 24ms/step - loss: 2.2541 - mae: 3.1716\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2160x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval Mean 5.58250825714182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IEEE_Train_Data_PPG, IEEE_Train_Data_ACC, IEEE_Train_Data_truth = preprocess_dataset(IEEE_Train_Data)\n",
        "train_model_loso(IEEE_Train_Data, IEEE_Train_Data_PPG, IEEE_Train_Data_ACC, IEEE_Train_Data_truth, \"IEEE_Train\")"
      ],
      "metadata": {
        "id": "Pg4QiyvyZloH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8dd708ee-d72b-4717-cd5b-120b31c04aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fit model on training data fold  0\n",
            "Epoch 1/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 67.5560 - mae: 77.5107\n",
            "Epoch 00001: val_loss improved from inf to 5.89698, saving model to /home/jupyter/IEEE_Train/fold0.h5\n",
            "42/42 [==============================] - 17s 161ms/step - loss: 67.5560 - mae: 77.5107 - val_loss: 5.8970 - val_mae: 19.9611 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.5549 - mae: 13.6913\n",
            "Epoch 00002: val_loss did not improve from 5.89698\n",
            "42/42 [==============================] - 4s 106ms/step - loss: 4.5549 - mae: 13.6913 - val_loss: 6.2750 - val_mae: 21.3418 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.7936 - mae: 10.8376\n",
            "Epoch 00003: val_loss did not improve from 5.89698\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "42/42 [==============================] - 4s 103ms/step - loss: 3.7936 - mae: 10.8376 - val_loss: 6.8679 - val_mae: 23.9147 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.5223 - mae: 8.9562\n",
            "Epoch 00004: val_loss did not improve from 5.89698\n",
            "42/42 [==============================] - 4s 103ms/step - loss: 3.5223 - mae: 8.9562 - val_loss: 6.6659 - val_mae: 22.7324 - lr: 5.0000e-04\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3282 - mae: 7.8917\n",
            "Epoch 00005: val_loss did not improve from 5.89698\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.3282 - mae: 7.8917 - val_loss: 7.0433 - val_mae: 23.1471 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2164 - mae: 7.8637\n",
            "Epoch 00006: val_loss improved from 5.89698 to 5.49006, saving model to /home/jupyter/IEEE_Train/fold0.h5\n",
            "42/42 [==============================] - 5s 108ms/step - loss: 3.2164 - mae: 7.8637 - val_loss: 5.4901 - val_mae: 17.1284 - lr: 2.5000e-04\n",
            "Epoch 7/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1511 - mae: 7.2193\n",
            "Epoch 00007: val_loss did not improve from 5.49006\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.1511 - mae: 7.2193 - val_loss: 6.5006 - val_mae: 20.5304 - lr: 2.5000e-04\n",
            "Epoch 8/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1560 - mae: 6.9950\n",
            "Epoch 00008: val_loss improved from 5.49006 to 5.29543, saving model to /home/jupyter/IEEE_Train/fold0.h5\n",
            "42/42 [==============================] - 4s 105ms/step - loss: 3.1560 - mae: 6.9950 - val_loss: 5.2954 - val_mae: 16.3675 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0806 - mae: 6.9953\n",
            "Epoch 00009: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 3.0806 - mae: 6.9953 - val_loss: 6.2848 - val_mae: 18.6348 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0359 - mae: 6.7407\n",
            "Epoch 00010: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 3.0359 - mae: 6.7407 - val_loss: 5.7856 - val_mae: 17.3924 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.9515 - mae: 6.1770\n",
            "Epoch 00011: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.9515 - mae: 6.1770 - val_loss: 6.9557 - val_mae: 20.7388 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8940 - mae: 5.8634\n",
            "Epoch 00012: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.8940 - mae: 5.8634 - val_loss: 5.9641 - val_mae: 16.5205 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8834 - mae: 6.0236\n",
            "Epoch 00013: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.8834 - mae: 6.0236 - val_loss: 6.0302 - val_mae: 17.0252 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8713 - mae: 5.9536\n",
            "Epoch 00014: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.8713 - mae: 5.9536 - val_loss: 6.8273 - val_mae: 19.5976 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8365 - mae: 5.7481\n",
            "Epoch 00015: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.8365 - mae: 5.7481 - val_loss: 6.8202 - val_mae: 19.0581 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8270 - mae: 5.6366\n",
            "Epoch 00016: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8270 - mae: 5.6366 - val_loss: 6.9043 - val_mae: 19.7583 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8236 - mae: 5.8350\n",
            "Epoch 00017: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.8236 - mae: 5.8350 - val_loss: 7.0189 - val_mae: 20.2564 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7982 - mae: 5.7334\n",
            "Epoch 00018: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.7982 - mae: 5.7334 - val_loss: 7.1289 - val_mae: 20.0243 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8094 - mae: 5.5023\n",
            "Epoch 00019: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.8094 - mae: 5.5023 - val_loss: 6.9360 - val_mae: 19.6070 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8151 - mae: 5.5168\n",
            "Epoch 00020: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.8151 - mae: 5.5168 - val_loss: 7.1435 - val_mae: 20.3017 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7976 - mae: 5.4456\n",
            "Epoch 00021: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.7976 - mae: 5.4456 - val_loss: 6.6489 - val_mae: 18.8015 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7875 - mae: 5.3997\n",
            "Epoch 00022: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.7875 - mae: 5.3997 - val_loss: 7.1453 - val_mae: 20.3447 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7996 - mae: 5.3779\n",
            "Epoch 00023: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.7996 - mae: 5.3779 - val_loss: 7.2979 - val_mae: 20.8618 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7898 - mae: 5.3973\n",
            "Epoch 00024: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.7898 - mae: 5.3973 - val_loss: 7.1581 - val_mae: 20.1543 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7908 - mae: 5.4718\n",
            "Epoch 00025: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.7908 - mae: 5.4718 - val_loss: 7.2024 - val_mae: 20.9934 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7853 - mae: 5.2979\n",
            "Epoch 00026: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.7853 - mae: 5.2979 - val_loss: 7.2417 - val_mae: 20.9113 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7908 - mae: 5.5660\n",
            "Epoch 00027: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.7908 - mae: 5.5660 - val_loss: 7.0434 - val_mae: 19.7554 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7905 - mae: 5.5584\n",
            "Epoch 00028: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.7905 - mae: 5.5584 - val_loss: 7.0744 - val_mae: 20.2286 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7884 - mae: 5.5138\n",
            "Epoch 00029: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.7884 - mae: 5.5138 - val_loss: 7.1995 - val_mae: 20.6207 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7882 - mae: 5.5711\n",
            "Epoch 00030: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.7882 - mae: 5.5711 - val_loss: 7.0062 - val_mae: 19.6433 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7863 - mae: 5.4063\n",
            "Epoch 00031: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.7863 - mae: 5.4063 - val_loss: 6.8490 - val_mae: 19.5902 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7948 - mae: 5.5863\n",
            "Epoch 00032: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.7948 - mae: 5.5863 - val_loss: 7.1817 - val_mae: 20.3689 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7926 - mae: 5.4298\n",
            "Epoch 00033: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.7926 - mae: 5.4298 - val_loss: 7.4972 - val_mae: 21.1706 - lr: 6.1035e-08\n",
            "Epoch 34/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7910 - mae: 5.6947\n",
            "Epoch 00034: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.7910 - mae: 5.6947 - val_loss: 7.2133 - val_mae: 20.7815 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7878 - mae: 5.2194\n",
            "Epoch 00035: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.7878 - mae: 5.2194 - val_loss: 6.7808 - val_mae: 19.6714 - lr: 3.0518e-08\n",
            "Epoch 36/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8022 - mae: 5.6823\n",
            "Epoch 00036: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.8022 - mae: 5.6823 - val_loss: 6.6494 - val_mae: 18.6913 - lr: 3.0518e-08\n",
            "Epoch 37/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7953 - mae: 5.3602\n",
            "Epoch 00037: val_loss did not improve from 5.29543\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.7953 - mae: 5.3602 - val_loss: 6.9563 - val_mae: 19.0227 - lr: 1.5259e-08\n",
            "Epoch 38/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7918 - mae: 5.6768Restoring model weights from the end of the best epoch: 8.\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 5.29543\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.7918 - mae: 5.6768 - val_loss: 7.3001 - val_mae: 20.4285 - lr: 1.5259e-08\n",
            "Epoch 00038: early stopping\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 7.4324 - mae: 20.6687\n",
            "Fit model on training data fold  1\n",
            "Epoch 1/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 106.7174 - mae: 108.9854\n",
            "Epoch 00001: val_loss improved from inf to 14.60500, saving model to /home/jupyter/IEEE_Train/fold1.h5\n",
            "42/42 [==============================] - 17s 172ms/step - loss: 106.7174 - mae: 108.9854 - val_loss: 14.6050 - val_mae: 53.5262 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.9513 - mae: 16.0228\n",
            "Epoch 00002: val_loss improved from 14.60500 to 5.42365, saving model to /home/jupyter/IEEE_Train/fold1.h5\n",
            "42/42 [==============================] - 5s 114ms/step - loss: 4.9513 - mae: 16.0228 - val_loss: 5.4237 - val_mae: 19.3886 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.7403 - mae: 10.5444\n",
            "Epoch 00003: val_loss improved from 5.42365 to 4.94081, saving model to /home/jupyter/IEEE_Train/fold1.h5\n",
            "42/42 [==============================] - 4s 106ms/step - loss: 3.7403 - mae: 10.5444 - val_loss: 4.9408 - val_mae: 15.8280 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.5276 - mae: 9.6387\n",
            "Epoch 00004: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 3.5276 - mae: 9.6387 - val_loss: 5.3055 - val_mae: 17.6499 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.4069 - mae: 8.9913\n",
            "Epoch 00005: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 3.4069 - mae: 8.9913 - val_loss: 5.5051 - val_mae: 17.8784 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2080 - mae: 7.8608\n",
            "Epoch 00006: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 3.2080 - mae: 7.8608 - val_loss: 6.6578 - val_mae: 24.0268 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1290 - mae: 7.7705\n",
            "Epoch 00007: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.1290 - mae: 7.7705 - val_loss: 7.5488 - val_mae: 25.3992 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0503 - mae: 7.1052\n",
            "Epoch 00008: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0503 - mae: 7.1052 - val_loss: 6.8695 - val_mae: 22.7610 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.9791 - mae: 6.2822\n",
            "Epoch 00009: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.9791 - mae: 6.2822 - val_loss: 7.5270 - val_mae: 25.1037 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.9582 - mae: 6.0586\n",
            "Epoch 00010: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.9582 - mae: 6.0586 - val_loss: 7.2883 - val_mae: 24.0391 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.9283 - mae: 6.2099\n",
            "Epoch 00011: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.9283 - mae: 6.2099 - val_loss: 7.6273 - val_mae: 23.7735 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.9045 - mae: 6.0945\n",
            "Epoch 00012: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.9045 - mae: 6.0945 - val_loss: 7.7571 - val_mae: 23.9699 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8713 - mae: 5.9959\n",
            "Epoch 00013: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.8713 - mae: 5.9959 - val_loss: 7.6016 - val_mae: 22.7473 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8455 - mae: 5.7850\n",
            "Epoch 00014: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.8455 - mae: 5.7850 - val_loss: 7.5867 - val_mae: 23.5277 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8351 - mae: 5.8876\n",
            "Epoch 00015: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.8351 - mae: 5.8876 - val_loss: 7.6624 - val_mae: 22.8228 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8201 - mae: 5.5019\n",
            "Epoch 00016: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8201 - mae: 5.5019 - val_loss: 7.4191 - val_mae: 21.1067 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8430 - mae: 5.6748\n",
            "Epoch 00017: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8430 - mae: 5.6748 - val_loss: 7.7670 - val_mae: 22.9728 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8197 - mae: 5.5221\n",
            "Epoch 00018: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8197 - mae: 5.5221 - val_loss: 7.3702 - val_mae: 21.5094 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8226 - mae: 5.5866\n",
            "Epoch 00019: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8226 - mae: 5.5866 - val_loss: 7.6137 - val_mae: 21.6371 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8046 - mae: 5.5691\n",
            "Epoch 00020: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8046 - mae: 5.5691 - val_loss: 7.2297 - val_mae: 21.6421 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8037 - mae: 5.4523\n",
            "Epoch 00021: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8037 - mae: 5.4523 - val_loss: 7.8072 - val_mae: 22.6891 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8052 - mae: 5.5389\n",
            "Epoch 00022: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8052 - mae: 5.5389 - val_loss: 7.3917 - val_mae: 21.1902 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8013 - mae: 5.4274\n",
            "Epoch 00023: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8013 - mae: 5.4274 - val_loss: 7.9281 - val_mae: 23.8106 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8165 - mae: 5.7629\n",
            "Epoch 00024: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8165 - mae: 5.7629 - val_loss: 7.6379 - val_mae: 22.3608 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8153 - mae: 5.5301\n",
            "Epoch 00025: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8153 - mae: 5.5301 - val_loss: 7.3967 - val_mae: 22.0100 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8019 - mae: 5.6067\n",
            "Epoch 00026: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8019 - mae: 5.6067 - val_loss: 7.5784 - val_mae: 22.6185 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8000 - mae: 5.4586\n",
            "Epoch 00027: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8000 - mae: 5.4586 - val_loss: 7.6266 - val_mae: 22.7883 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8230 - mae: 5.4653\n",
            "Epoch 00028: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8230 - mae: 5.4653 - val_loss: 7.5038 - val_mae: 22.1705 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8003 - mae: 5.5388\n",
            "Epoch 00029: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8003 - mae: 5.5388 - val_loss: 7.3186 - val_mae: 21.1846 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8068 - mae: 5.4010\n",
            "Epoch 00030: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8068 - mae: 5.4010 - val_loss: 7.8417 - val_mae: 22.7634 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8055 - mae: 5.6462\n",
            "Epoch 00031: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "42/42 [==============================] - 4s 93ms/step - loss: 2.8055 - mae: 5.6462 - val_loss: 7.8275 - val_mae: 23.4328 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8116 - mae: 5.5969\n",
            "Epoch 00032: val_loss did not improve from 4.94081\n",
            "42/42 [==============================] - 4s 93ms/step - loss: 2.8116 - mae: 5.5969 - val_loss: 7.4020 - val_mae: 21.8637 - lr: 6.1035e-08\n",
            "Epoch 33/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8022 - mae: 5.3539Restoring model weights from the end of the best epoch: 3.\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 4.94081\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8022 - mae: 5.3539 - val_loss: 8.0804 - val_mae: 24.0639 - lr: 6.1035e-08\n",
            "Epoch 00033: early stopping\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 4.6293 - mae: 14.8332\n",
            "Fit model on training data fold  2\n",
            "Epoch 1/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 72.6079 - mae: 78.5894\n",
            "Epoch 00001: val_loss improved from inf to 6.28351, saving model to /home/jupyter/IEEE_Train/fold2.h5\n",
            "42/42 [==============================] - 17s 173ms/step - loss: 72.6079 - mae: 78.5894 - val_loss: 6.2835 - val_mae: 19.7993 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.7389 - mae: 14.5750\n",
            "Epoch 00002: val_loss improved from 6.28351 to 5.63793, saving model to /home/jupyter/IEEE_Train/fold2.h5\n",
            "42/42 [==============================] - 4s 106ms/step - loss: 4.7389 - mae: 14.5750 - val_loss: 5.6379 - val_mae: 18.2734 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.8923 - mae: 10.9175\n",
            "Epoch 00003: val_loss improved from 5.63793 to 4.80079, saving model to /home/jupyter/IEEE_Train/fold2.h5\n",
            "42/42 [==============================] - 4s 104ms/step - loss: 3.8923 - mae: 10.9175 - val_loss: 4.8008 - val_mae: 15.4335 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.7163 - mae: 10.1761\n",
            "Epoch 00004: val_loss did not improve from 4.80079\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.7163 - mae: 10.1761 - val_loss: 5.6141 - val_mae: 17.9239 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3996 - mae: 8.5325\n",
            "Epoch 00005: val_loss did not improve from 4.80079\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.3996 - mae: 8.5325 - val_loss: 4.8885 - val_mae: 15.0950 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2290 - mae: 7.6965\n",
            "Epoch 00006: val_loss did not improve from 4.80079\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.2290 - mae: 7.6965 - val_loss: 4.9301 - val_mae: 14.2630 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1564 - mae: 7.3224\n",
            "Epoch 00007: val_loss did not improve from 4.80079\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.1564 - mae: 7.3224 - val_loss: 5.0575 - val_mae: 15.1005 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0749 - mae: 6.8788\n",
            "Epoch 00008: val_loss improved from 4.80079 to 4.63507, saving model to /home/jupyter/IEEE_Train/fold2.h5\n",
            "42/42 [==============================] - 4s 103ms/step - loss: 3.0749 - mae: 6.8788 - val_loss: 4.6351 - val_mae: 13.5654 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.9989 - mae: 6.6335\n",
            "Epoch 00009: val_loss did not improve from 4.63507\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.9989 - mae: 6.6335 - val_loss: 4.9491 - val_mae: 14.5485 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.9849 - mae: 6.7675\n",
            "Epoch 00010: val_loss did not improve from 4.63507\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.9849 - mae: 6.7675 - val_loss: 4.9241 - val_mae: 14.0019 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.9456 - mae: 6.3837\n",
            "Epoch 00011: val_loss did not improve from 4.63507\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.9456 - mae: 6.3837 - val_loss: 5.0841 - val_mae: 15.2657 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.9222 - mae: 6.4085\n",
            "Epoch 00012: val_loss did not improve from 4.63507\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.9222 - mae: 6.4085 - val_loss: 4.8467 - val_mae: 13.3497 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8739 - mae: 5.9115\n",
            "Epoch 00013: val_loss improved from 4.63507 to 4.46807, saving model to /home/jupyter/IEEE_Train/fold2.h5\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.8739 - mae: 5.9115 - val_loss: 4.4681 - val_mae: 11.8694 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8816 - mae: 6.0840\n",
            "Epoch 00014: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8816 - mae: 6.0840 - val_loss: 4.7084 - val_mae: 12.4949 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8645 - mae: 5.9556\n",
            "Epoch 00015: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.8645 - mae: 5.9556 - val_loss: 4.8142 - val_mae: 12.8991 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8535 - mae: 5.9427\n",
            "Epoch 00016: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8535 - mae: 5.9427 - val_loss: 4.8444 - val_mae: 13.3535 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8332 - mae: 5.4845\n",
            "Epoch 00017: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8332 - mae: 5.4845 - val_loss: 4.9589 - val_mae: 12.9831 - lr: 3.1250e-05\n",
            "Epoch 18/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8368 - mae: 5.6617\n",
            "Epoch 00018: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8368 - mae: 5.6617 - val_loss: 4.9903 - val_mae: 13.4748 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8249 - mae: 5.5732\n",
            "Epoch 00019: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8249 - mae: 5.5732 - val_loss: 4.7179 - val_mae: 13.1653 - lr: 1.5625e-05\n",
            "Epoch 20/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8201 - mae: 5.9283\n",
            "Epoch 00020: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8201 - mae: 5.9283 - val_loss: 4.8214 - val_mae: 12.8449 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8253 - mae: 5.8108\n",
            "Epoch 00021: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8253 - mae: 5.8108 - val_loss: 4.7062 - val_mae: 12.3818 - lr: 7.8125e-06\n",
            "Epoch 22/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8251 - mae: 5.7681\n",
            "Epoch 00022: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8251 - mae: 5.7681 - val_loss: 4.7153 - val_mae: 12.3164 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8261 - mae: 5.6917\n",
            "Epoch 00023: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8261 - mae: 5.6917 - val_loss: 4.7202 - val_mae: 13.0632 - lr: 3.9063e-06\n",
            "Epoch 24/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8186 - mae: 5.6632\n",
            "Epoch 00024: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8186 - mae: 5.6632 - val_loss: 4.7450 - val_mae: 12.3226 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8116 - mae: 5.6239\n",
            "Epoch 00025: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8116 - mae: 5.6239 - val_loss: 4.7195 - val_mae: 12.9484 - lr: 1.9531e-06\n",
            "Epoch 26/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8180 - mae: 5.4922\n",
            "Epoch 00026: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8180 - mae: 5.4922 - val_loss: 4.8438 - val_mae: 12.6503 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8155 - mae: 5.8029\n",
            "Epoch 00027: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8155 - mae: 5.8029 - val_loss: 4.8042 - val_mae: 13.0119 - lr: 9.7656e-07\n",
            "Epoch 28/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8186 - mae: 5.7977\n",
            "Epoch 00028: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8186 - mae: 5.7977 - val_loss: 4.8407 - val_mae: 12.9623 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8265 - mae: 5.4507\n",
            "Epoch 00029: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8265 - mae: 5.4507 - val_loss: 4.7756 - val_mae: 12.9231 - lr: 4.8828e-07\n",
            "Epoch 30/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8145 - mae: 5.6368\n",
            "Epoch 00030: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8145 - mae: 5.6368 - val_loss: 4.7284 - val_mae: 13.2083 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8337 - mae: 5.8059\n",
            "Epoch 00031: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8337 - mae: 5.8059 - val_loss: 4.6665 - val_mae: 11.9527 - lr: 2.4414e-07\n",
            "Epoch 32/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8174 - mae: 5.6780\n",
            "Epoch 00032: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8174 - mae: 5.6780 - val_loss: 4.6911 - val_mae: 12.5851 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8176 - mae: 5.5601\n",
            "Epoch 00033: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8176 - mae: 5.5601 - val_loss: 4.4919 - val_mae: 12.0448 - lr: 1.2207e-07\n",
            "Epoch 34/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8265 - mae: 5.8094\n",
            "Epoch 00034: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8265 - mae: 5.8094 - val_loss: 4.5659 - val_mae: 11.7954 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8170 - mae: 5.5161\n",
            "Epoch 00035: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8170 - mae: 5.5161 - val_loss: 4.7372 - val_mae: 12.7464 - lr: 6.1035e-08\n",
            "Epoch 36/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8369 - mae: 5.7299\n",
            "Epoch 00036: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8369 - mae: 5.7299 - val_loss: 4.8142 - val_mae: 12.9488 - lr: 3.0518e-08\n",
            "Epoch 37/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8184 - mae: 5.5990\n",
            "Epoch 00037: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8184 - mae: 5.5990 - val_loss: 4.6373 - val_mae: 12.1768 - lr: 3.0518e-08\n",
            "Epoch 38/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8145 - mae: 5.6801\n",
            "Epoch 00038: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8145 - mae: 5.6801 - val_loss: 4.7294 - val_mae: 12.3696 - lr: 1.5259e-08\n",
            "Epoch 39/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8159 - mae: 5.4946\n",
            "Epoch 00039: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8159 - mae: 5.4946 - val_loss: 5.0130 - val_mae: 13.0682 - lr: 1.5259e-08\n",
            "Epoch 40/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8269 - mae: 5.8752\n",
            "Epoch 00040: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8269 - mae: 5.8752 - val_loss: 4.8324 - val_mae: 13.2563 - lr: 7.6294e-09\n",
            "Epoch 41/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8231 - mae: 5.5235\n",
            "Epoch 00041: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8231 - mae: 5.5235 - val_loss: 4.7684 - val_mae: 12.5791 - lr: 7.6294e-09\n",
            "Epoch 42/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8181 - mae: 5.7066\n",
            "Epoch 00042: val_loss did not improve from 4.46807\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8181 - mae: 5.7066 - val_loss: 4.8296 - val_mae: 12.8480 - lr: 3.8147e-09\n",
            "Epoch 43/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8146 - mae: 5.5939Restoring model weights from the end of the best epoch: 13.\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 4.46807\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8146 - mae: 5.5939 - val_loss: 4.6399 - val_mae: 12.3593 - lr: 3.8147e-09\n",
            "Epoch 00043: early stopping\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 6.9969 - mae: 22.1774\n",
            "Fit model on training data fold  3\n",
            "Epoch 1/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 171.5224 - mae: 134.0157\n",
            "Epoch 00001: val_loss improved from inf to 57.59775, saving model to /home/jupyter/IEEE_Train/fold3.h5\n",
            "42/42 [==============================] - 17s 176ms/step - loss: 171.5224 - mae: 134.0157 - val_loss: 57.5977 - val_mae: 93.8753 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 10.9763 - mae: 27.2438\n",
            "Epoch 00002: val_loss improved from 57.59775 to 5.03530, saving model to /home/jupyter/IEEE_Train/fold3.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 10.9763 - mae: 27.2438 - val_loss: 5.0353 - val_mae: 17.3235 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.0510 - mae: 11.6309\n",
            "Epoch 00003: val_loss improved from 5.03530 to 3.84759, saving model to /home/jupyter/IEEE_Train/fold3.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 4.0510 - mae: 11.6309 - val_loss: 3.8476 - val_mae: 11.1271 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.7862 - mae: 10.7177\n",
            "Epoch 00004: val_loss improved from 3.84759 to 3.62524, saving model to /home/jupyter/IEEE_Train/fold3.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.7862 - mae: 10.7177 - val_loss: 3.6252 - val_mae: 8.2540 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.5672 - mae: 9.5992\n",
            "Epoch 00005: val_loss improved from 3.62524 to 3.51945, saving model to /home/jupyter/IEEE_Train/fold3.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.5672 - mae: 9.5992 - val_loss: 3.5195 - val_mae: 9.2211 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3893 - mae: 8.2352\n",
            "Epoch 00006: val_loss did not improve from 3.51945\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.3893 - mae: 8.2352 - val_loss: 3.5546 - val_mae: 9.6543 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3206 - mae: 8.1470\n",
            "Epoch 00007: val_loss improved from 3.51945 to 3.26861, saving model to /home/jupyter/IEEE_Train/fold3.h5\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.3206 - mae: 8.1470 - val_loss: 3.2686 - val_mae: 8.0779 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2498 - mae: 7.8452\n",
            "Epoch 00008: val_loss did not improve from 3.26861\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.2498 - mae: 7.8452 - val_loss: 3.5601 - val_mae: 8.0464 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1523 - mae: 7.3205\n",
            "Epoch 00009: val_loss improved from 3.26861 to 3.10680, saving model to /home/jupyter/IEEE_Train/fold3.h5\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.1523 - mae: 7.3205 - val_loss: 3.1068 - val_mae: 7.0454 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1419 - mae: 6.9998\n",
            "Epoch 00010: val_loss did not improve from 3.10680\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.1419 - mae: 6.9998 - val_loss: 3.2240 - val_mae: 6.9023 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0852 - mae: 6.5040\n",
            "Epoch 00011: val_loss improved from 3.10680 to 3.05033, saving model to /home/jupyter/IEEE_Train/fold3.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 3.0852 - mae: 6.5040 - val_loss: 3.0503 - val_mae: 6.0755 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0771 - mae: 6.0650\n",
            "Epoch 00012: val_loss did not improve from 3.05033\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0771 - mae: 6.0650 - val_loss: 4.7046 - val_mae: 10.9776 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0987 - mae: 6.0023\n",
            "Epoch 00013: val_loss did not improve from 3.05033\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0987 - mae: 6.0023 - val_loss: 3.1900 - val_mae: 6.4566 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7392 - mae: 5.0396\n",
            "Epoch 00014: val_loss improved from 3.05033 to 2.89240, saving model to /home/jupyter/IEEE_Train/fold3.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 2.7392 - mae: 5.0396 - val_loss: 2.8924 - val_mae: 5.5026 - lr: 5.0000e-04\n",
            "Epoch 15/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.6354 - mae: 4.1791\n",
            "Epoch 00015: val_loss did not improve from 2.89240\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.6354 - mae: 4.1791 - val_loss: 2.9620 - val_mae: 5.1447 - lr: 5.0000e-04\n",
            "Epoch 16/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.6634 - mae: 4.4555\n",
            "Epoch 00016: val_loss did not improve from 2.89240\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "42/42 [==============================] - 4s 93ms/step - loss: 2.6634 - mae: 4.4555 - val_loss: 3.8289 - val_mae: 6.7520 - lr: 5.0000e-04\n",
            "Epoch 17/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.4189 - mae: 3.5937\n",
            "Epoch 00017: val_loss did not improve from 2.89240\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.4189 - mae: 3.5937 - val_loss: 3.0388 - val_mae: 4.6039 - lr: 2.5000e-04\n",
            "Epoch 18/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.3582 - mae: 3.2597\n",
            "Epoch 00018: val_loss did not improve from 2.89240\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.3582 - mae: 3.2597 - val_loss: 3.2388 - val_mae: 5.0232 - lr: 2.5000e-04\n",
            "Epoch 19/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.3029 - mae: 3.2094\n",
            "Epoch 00019: val_loss improved from 2.89240 to 2.88286, saving model to /home/jupyter/IEEE_Train/fold3.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.3029 - mae: 3.2094 - val_loss: 2.8829 - val_mae: 3.9254 - lr: 1.2500e-04\n",
            "Epoch 20/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.2139 - mae: 2.9202\n",
            "Epoch 00020: val_loss improved from 2.88286 to 2.72810, saving model to /home/jupyter/IEEE_Train/fold3.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.2139 - mae: 2.9202 - val_loss: 2.7281 - val_mae: 3.4877 - lr: 1.2500e-04\n",
            "Epoch 21/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.1528 - mae: 2.7733\n",
            "Epoch 00021: val_loss improved from 2.72810 to 2.63932, saving model to /home/jupyter/IEEE_Train/fold3.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.1528 - mae: 2.7733 - val_loss: 2.6393 - val_mae: 3.1858 - lr: 1.2500e-04\n",
            "Epoch 22/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.1228 - mae: 2.5942\n",
            "Epoch 00022: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.1228 - mae: 2.5942 - val_loss: 3.2151 - val_mae: 4.1179 - lr: 1.2500e-04\n",
            "Epoch 23/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0925 - mae: 2.4591\n",
            "Epoch 00023: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.0925 - mae: 2.4591 - val_loss: 2.9302 - val_mae: 3.6403 - lr: 1.2500e-04\n",
            "Epoch 24/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0105 - mae: 2.3757\n",
            "Epoch 00024: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.0105 - mae: 2.3757 - val_loss: 2.7143 - val_mae: 3.1418 - lr: 6.2500e-05\n",
            "Epoch 25/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0015 - mae: 2.2655\n",
            "Epoch 00025: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.0015 - mae: 2.2655 - val_loss: 2.8789 - val_mae: 3.3931 - lr: 6.2500e-05\n",
            "Epoch 26/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9490 - mae: 2.1894\n",
            "Epoch 00026: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 1.9490 - mae: 2.1894 - val_loss: 3.0881 - val_mae: 3.4470 - lr: 3.1250e-05\n",
            "Epoch 27/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9287 - mae: 2.1602\n",
            "Epoch 00027: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.9287 - mae: 2.1602 - val_loss: 3.0801 - val_mae: 3.3851 - lr: 3.1250e-05\n",
            "Epoch 28/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8808 - mae: 2.1031\n",
            "Epoch 00028: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 1.8808 - mae: 2.1031 - val_loss: 3.1482 - val_mae: 3.4605 - lr: 1.5625e-05\n",
            "Epoch 29/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9431 - mae: 2.1769\n",
            "Epoch 00029: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 1.9431 - mae: 2.1769 - val_loss: 3.1527 - val_mae: 3.6121 - lr: 1.5625e-05\n",
            "Epoch 30/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8670 - mae: 2.0256\n",
            "Epoch 00030: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 1.8670 - mae: 2.0256 - val_loss: 3.0996 - val_mae: 3.5035 - lr: 7.8125e-06\n",
            "Epoch 31/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8815 - mae: 2.0766\n",
            "Epoch 00031: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 1.8815 - mae: 2.0766 - val_loss: 3.0243 - val_mae: 3.5121 - lr: 7.8125e-06\n",
            "Epoch 32/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8803 - mae: 2.0406\n",
            "Epoch 00032: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 1.8803 - mae: 2.0406 - val_loss: 2.9817 - val_mae: 3.1969 - lr: 3.9063e-06\n",
            "Epoch 33/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8477 - mae: 1.9918\n",
            "Epoch 00033: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "42/42 [==============================] - 4s 93ms/step - loss: 1.8477 - mae: 1.9918 - val_loss: 2.8802 - val_mae: 3.1893 - lr: 3.9063e-06\n",
            "Epoch 34/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8642 - mae: 2.0257\n",
            "Epoch 00034: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.8642 - mae: 2.0257 - val_loss: 2.8526 - val_mae: 3.1069 - lr: 1.9531e-06\n",
            "Epoch 35/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8589 - mae: 1.9939\n",
            "Epoch 00035: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "42/42 [==============================] - 4s 93ms/step - loss: 1.8589 - mae: 1.9939 - val_loss: 2.9022 - val_mae: 3.0543 - lr: 1.9531e-06\n",
            "Epoch 36/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8460 - mae: 2.0005\n",
            "Epoch 00036: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 1.8460 - mae: 2.0005 - val_loss: 3.0898 - val_mae: 3.4497 - lr: 9.7656e-07\n",
            "Epoch 37/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8584 - mae: 2.0214\n",
            "Epoch 00037: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.8584 - mae: 2.0214 - val_loss: 3.0896 - val_mae: 3.4504 - lr: 9.7656e-07\n",
            "Epoch 38/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8452 - mae: 1.9663\n",
            "Epoch 00038: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.8452 - mae: 1.9663 - val_loss: 3.0360 - val_mae: 3.3267 - lr: 4.8828e-07\n",
            "Epoch 39/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8299 - mae: 1.9248\n",
            "Epoch 00039: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.8299 - mae: 1.9248 - val_loss: 3.1242 - val_mae: 3.4319 - lr: 4.8828e-07\n",
            "Epoch 40/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8395 - mae: 1.9398\n",
            "Epoch 00040: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.8395 - mae: 1.9398 - val_loss: 2.9443 - val_mae: 3.1872 - lr: 2.4414e-07\n",
            "Epoch 41/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8501 - mae: 2.0009\n",
            "Epoch 00041: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 1.8501 - mae: 2.0009 - val_loss: 2.8537 - val_mae: 3.1066 - lr: 2.4414e-07\n",
            "Epoch 42/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8544 - mae: 1.9804\n",
            "Epoch 00042: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.8544 - mae: 1.9804 - val_loss: 2.9619 - val_mae: 3.1758 - lr: 1.2207e-07\n",
            "Epoch 43/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8362 - mae: 1.9541\n",
            "Epoch 00043: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.8362 - mae: 1.9541 - val_loss: 3.0220 - val_mae: 3.3242 - lr: 1.2207e-07\n",
            "Epoch 44/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8266 - mae: 1.9535\n",
            "Epoch 00044: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 1.8266 - mae: 1.9535 - val_loss: 2.9221 - val_mae: 3.2326 - lr: 6.1035e-08\n",
            "Epoch 45/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8961 - mae: 2.0886\n",
            "Epoch 00045: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.8961 - mae: 2.0886 - val_loss: 3.1207 - val_mae: 3.6145 - lr: 6.1035e-08\n",
            "Epoch 46/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8349 - mae: 2.0081\n",
            "Epoch 00046: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 1.8349 - mae: 2.0081 - val_loss: 2.9076 - val_mae: 3.2174 - lr: 3.0518e-08\n",
            "Epoch 47/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8200 - mae: 1.9632\n",
            "Epoch 00047: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 1.8200 - mae: 1.9632 - val_loss: 3.0830 - val_mae: 3.3215 - lr: 3.0518e-08\n",
            "Epoch 48/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8748 - mae: 2.0528\n",
            "Epoch 00048: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.8748 - mae: 2.0528 - val_loss: 2.9525 - val_mae: 3.2494 - lr: 1.5259e-08\n",
            "Epoch 49/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8574 - mae: 1.9766\n",
            "Epoch 00049: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.8574 - mae: 1.9766 - val_loss: 3.1138 - val_mae: 3.6583 - lr: 1.5259e-08\n",
            "Epoch 50/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8501 - mae: 1.9743\n",
            "Epoch 00050: val_loss did not improve from 2.63932\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.8501 - mae: 1.9743 - val_loss: 2.9570 - val_mae: 3.3143 - lr: 7.6294e-09\n",
            "Epoch 51/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8558 - mae: 2.0105Restoring model weights from the end of the best epoch: 21.\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 2.63932\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 1.8558 - mae: 2.0105 - val_loss: 2.9884 - val_mae: 3.4609 - lr: 7.6294e-09\n",
            "Epoch 00051: early stopping\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 4.4709 - mae: 6.3935\n",
            "Fit model on training data fold  4\n",
            "Epoch 1/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 57.1866 - mae: 96.5122\n",
            "Epoch 00001: val_loss improved from inf to 5.66403, saving model to /home/jupyter/IEEE_Train/fold4.h5\n",
            "42/42 [==============================] - 16s 162ms/step - loss: 57.1866 - mae: 96.5122 - val_loss: 5.6640 - val_mae: 27.5249 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.7628 - mae: 21.1830\n",
            "Epoch 00002: val_loss improved from 5.66403 to 4.79940, saving model to /home/jupyter/IEEE_Train/fold4.h5\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 4.7628 - mae: 21.1830 - val_loss: 4.7994 - val_mae: 21.1095 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.2880 - mae: 17.5926\n",
            "Epoch 00003: val_loss improved from 4.79940 to 4.69660, saving model to /home/jupyter/IEEE_Train/fold4.h5\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 4.2880 - mae: 17.5926 - val_loss: 4.6966 - val_mae: 19.9053 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.1504 - mae: 16.7559\n",
            "Epoch 00004: val_loss improved from 4.69660 to 4.10406, saving model to /home/jupyter/IEEE_Train/fold4.h5\n",
            "42/42 [==============================] - 5s 116ms/step - loss: 4.1504 - mae: 16.7559 - val_loss: 4.1041 - val_mae: 14.8829 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.9962 - mae: 14.8954\n",
            "Epoch 00005: val_loss did not improve from 4.10406\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.9962 - mae: 14.8954 - val_loss: 4.5413 - val_mae: 19.5916 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.8905 - mae: 13.8606\n",
            "Epoch 00006: val_loss did not improve from 4.10406\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.8905 - mae: 13.8606 - val_loss: 4.1133 - val_mae: 14.4491 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.7731 - mae: 12.9447\n",
            "Epoch 00007: val_loss improved from 4.10406 to 3.91087, saving model to /home/jupyter/IEEE_Train/fold4.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 3.7731 - mae: 12.9447 - val_loss: 3.9109 - val_mae: 14.1641 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.6823 - mae: 12.2590\n",
            "Epoch 00008: val_loss improved from 3.91087 to 3.86840, saving model to /home/jupyter/IEEE_Train/fold4.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 3.6823 - mae: 12.2590 - val_loss: 3.8684 - val_mae: 12.8429 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.5988 - mae: 11.6908\n",
            "Epoch 00009: val_loss improved from 3.86840 to 3.81509, saving model to /home/jupyter/IEEE_Train/fold4.h5\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 3.5988 - mae: 11.6908 - val_loss: 3.8151 - val_mae: 12.5058 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.5698 - mae: 11.0449\n",
            "Epoch 00010: val_loss did not improve from 3.81509\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.5698 - mae: 11.0449 - val_loss: 3.8409 - val_mae: 12.4737 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.5418 - mae: 10.9525\n",
            "Epoch 00011: val_loss did not improve from 3.81509\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.5418 - mae: 10.9525 - val_loss: 3.9655 - val_mae: 12.6567 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.4707 - mae: 10.2650\n",
            "Epoch 00012: val_loss improved from 3.81509 to 3.71149, saving model to /home/jupyter/IEEE_Train/fold4.h5\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.4707 - mae: 10.2650 - val_loss: 3.7115 - val_mae: 10.9674 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.4236 - mae: 9.9849 \n",
            "Epoch 00013: val_loss improved from 3.71149 to 3.67659, saving model to /home/jupyter/IEEE_Train/fold4.h5\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.4236 - mae: 9.9849 - val_loss: 3.6766 - val_mae: 10.8869 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3866 - mae: 10.2131\n",
            "Epoch 00014: val_loss did not improve from 3.67659\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.3866 - mae: 10.2131 - val_loss: 3.7657 - val_mae: 11.7127 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3798 - mae: 9.5998\n",
            "Epoch 00015: val_loss improved from 3.67659 to 3.67634, saving model to /home/jupyter/IEEE_Train/fold4.h5\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.3798 - mae: 9.5998 - val_loss: 3.6763 - val_mae: 11.0173 - lr: 2.5000e-04\n",
            "Epoch 16/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3716 - mae: 9.4318\n",
            "Epoch 00016: val_loss did not improve from 3.67634\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.3716 - mae: 9.4318 - val_loss: 3.8645 - val_mae: 11.3650 - lr: 2.5000e-04\n",
            "Epoch 17/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3225 - mae: 9.4347\n",
            "Epoch 00017: val_loss improved from 3.67634 to 3.61674, saving model to /home/jupyter/IEEE_Train/fold4.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 3.3225 - mae: 9.4347 - val_loss: 3.6167 - val_mae: 10.4785 - lr: 2.5000e-04\n",
            "Epoch 18/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3038 - mae: 8.9768\n",
            "Epoch 00018: val_loss did not improve from 3.61674\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.3038 - mae: 8.9768 - val_loss: 3.6372 - val_mae: 10.5648 - lr: 2.5000e-04\n",
            "Epoch 19/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3267 - mae: 8.7084\n",
            "Epoch 00019: val_loss did not improve from 3.61674\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.3267 - mae: 8.7084 - val_loss: 3.6744 - val_mae: 9.6104 - lr: 2.5000e-04\n",
            "Epoch 20/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2308 - mae: 8.3441\n",
            "Epoch 00020: val_loss did not improve from 3.61674\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.2308 - mae: 8.3441 - val_loss: 3.6715 - val_mae: 11.0119 - lr: 1.2500e-04\n",
            "Epoch 21/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2154 - mae: 7.9813\n",
            "Epoch 00021: val_loss improved from 3.61674 to 3.41482, saving model to /home/jupyter/IEEE_Train/fold4.h5\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.2154 - mae: 7.9813 - val_loss: 3.4148 - val_mae: 8.7455 - lr: 1.2500e-04\n",
            "Epoch 22/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2212 - mae: 8.2098\n",
            "Epoch 00022: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.2212 - mae: 8.2098 - val_loss: 3.6144 - val_mae: 9.8371 - lr: 1.2500e-04\n",
            "Epoch 23/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1707 - mae: 8.1176\n",
            "Epoch 00023: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.1707 - mae: 8.1176 - val_loss: 3.5754 - val_mae: 8.7945 - lr: 1.2500e-04\n",
            "Epoch 24/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1546 - mae: 7.6415\n",
            "Epoch 00024: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.1546 - mae: 7.6415 - val_loss: 3.4814 - val_mae: 9.0851 - lr: 6.2500e-05\n",
            "Epoch 25/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1264 - mae: 7.6745\n",
            "Epoch 00025: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.1264 - mae: 7.6745 - val_loss: 3.5385 - val_mae: 9.5476 - lr: 6.2500e-05\n",
            "Epoch 26/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1142 - mae: 7.6734\n",
            "Epoch 00026: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.1142 - mae: 7.6734 - val_loss: 3.4430 - val_mae: 8.4684 - lr: 3.1250e-05\n",
            "Epoch 27/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1126 - mae: 7.5482\n",
            "Epoch 00027: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.1126 - mae: 7.5482 - val_loss: 3.5093 - val_mae: 9.4698 - lr: 3.1250e-05\n",
            "Epoch 28/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0976 - mae: 7.7950\n",
            "Epoch 00028: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0976 - mae: 7.7950 - val_loss: 3.5043 - val_mae: 8.8957 - lr: 1.5625e-05\n",
            "Epoch 29/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0804 - mae: 7.6284\n",
            "Epoch 00029: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0804 - mae: 7.6284 - val_loss: 3.5043 - val_mae: 9.0680 - lr: 1.5625e-05\n",
            "Epoch 30/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0831 - mae: 7.4107\n",
            "Epoch 00030: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 93ms/step - loss: 3.0831 - mae: 7.4107 - val_loss: 3.5094 - val_mae: 9.3703 - lr: 7.8125e-06\n",
            "Epoch 31/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0864 - mae: 7.4266\n",
            "Epoch 00031: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0864 - mae: 7.4266 - val_loss: 3.4949 - val_mae: 9.1734 - lr: 7.8125e-06\n",
            "Epoch 32/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0683 - mae: 7.2597\n",
            "Epoch 00032: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0683 - mae: 7.2597 - val_loss: 3.5122 - val_mae: 9.0012 - lr: 3.9063e-06\n",
            "Epoch 33/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0950 - mae: 7.4820\n",
            "Epoch 00033: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.0950 - mae: 7.4820 - val_loss: 3.5092 - val_mae: 9.1690 - lr: 3.9063e-06\n",
            "Epoch 34/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0787 - mae: 7.6717\n",
            "Epoch 00034: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 93ms/step - loss: 3.0787 - mae: 7.6717 - val_loss: 3.5262 - val_mae: 9.6243 - lr: 1.9531e-06\n",
            "Epoch 35/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0828 - mae: 7.6263\n",
            "Epoch 00035: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.0828 - mae: 7.6263 - val_loss: 3.5566 - val_mae: 9.0847 - lr: 1.9531e-06\n",
            "Epoch 36/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0698 - mae: 7.1236\n",
            "Epoch 00036: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.0698 - mae: 7.1236 - val_loss: 3.5087 - val_mae: 8.8316 - lr: 9.7656e-07\n",
            "Epoch 37/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0900 - mae: 7.6018\n",
            "Epoch 00037: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.0900 - mae: 7.6018 - val_loss: 3.5381 - val_mae: 8.9597 - lr: 9.7656e-07\n",
            "Epoch 38/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0697 - mae: 7.6573\n",
            "Epoch 00038: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0697 - mae: 7.6573 - val_loss: 3.5140 - val_mae: 8.5767 - lr: 4.8828e-07\n",
            "Epoch 39/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0799 - mae: 7.5446\n",
            "Epoch 00039: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0799 - mae: 7.5446 - val_loss: 3.5452 - val_mae: 8.9401 - lr: 4.8828e-07\n",
            "Epoch 40/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0706 - mae: 7.1704\n",
            "Epoch 00040: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0706 - mae: 7.1704 - val_loss: 3.5077 - val_mae: 9.1043 - lr: 2.4414e-07\n",
            "Epoch 41/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0746 - mae: 7.5556\n",
            "Epoch 00041: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0746 - mae: 7.5556 - val_loss: 3.4898 - val_mae: 9.2581 - lr: 2.4414e-07\n",
            "Epoch 42/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0742 - mae: 7.7421\n",
            "Epoch 00042: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0742 - mae: 7.7421 - val_loss: 3.5154 - val_mae: 9.0691 - lr: 1.2207e-07\n",
            "Epoch 43/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0744 - mae: 7.7372\n",
            "Epoch 00043: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0744 - mae: 7.7372 - val_loss: 3.5072 - val_mae: 9.0344 - lr: 1.2207e-07\n",
            "Epoch 44/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0801 - mae: 7.4873\n",
            "Epoch 00044: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0801 - mae: 7.4873 - val_loss: 3.5254 - val_mae: 9.0724 - lr: 6.1035e-08\n",
            "Epoch 45/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0827 - mae: 7.5878\n",
            "Epoch 00045: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0827 - mae: 7.5878 - val_loss: 3.5017 - val_mae: 9.2425 - lr: 6.1035e-08\n",
            "Epoch 46/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0804 - mae: 7.3581\n",
            "Epoch 00046: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0804 - mae: 7.3581 - val_loss: 3.5279 - val_mae: 8.7090 - lr: 3.0518e-08\n",
            "Epoch 47/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0711 - mae: 7.6924\n",
            "Epoch 00047: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.0711 - mae: 7.6924 - val_loss: 3.5161 - val_mae: 9.4674 - lr: 3.0518e-08\n",
            "Epoch 48/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0802 - mae: 7.3116\n",
            "Epoch 00048: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.0802 - mae: 7.3116 - val_loss: 3.5028 - val_mae: 9.2184 - lr: 1.5259e-08\n",
            "Epoch 49/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0741 - mae: 7.5089\n",
            "Epoch 00049: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.0741 - mae: 7.5089 - val_loss: 3.5111 - val_mae: 9.3986 - lr: 1.5259e-08\n",
            "Epoch 50/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0703 - mae: 7.6593\n",
            "Epoch 00050: val_loss did not improve from 3.41482\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0703 - mae: 7.6593 - val_loss: 3.5226 - val_mae: 9.2552 - lr: 7.6294e-09\n",
            "Epoch 51/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0782 - mae: 7.0990Restoring model weights from the end of the best epoch: 21.\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 3.41482\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0782 - mae: 7.0990 - val_loss: 3.4898 - val_mae: 9.3824 - lr: 7.6294e-09\n",
            "Epoch 00051: early stopping\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 3.3404 - mae: 8.8611\n",
            "Fit model on training data fold  5\n",
            "Epoch 1/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 107.2632 - mae: 135.6938\n",
            "Epoch 00001: val_loss improved from inf to 18.81046, saving model to /home/jupyter/IEEE_Train/fold5.h5\n",
            "42/42 [==============================] - 16s 168ms/step - loss: 107.2632 - mae: 135.6938 - val_loss: 18.8105 - val_mae: 89.0740 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 8.8048 - mae: 44.8180\n",
            "Epoch 00002: val_loss improved from 18.81046 to 12.34460, saving model to /home/jupyter/IEEE_Train/fold5.h5\n",
            "42/42 [==============================] - 5s 114ms/step - loss: 8.8048 - mae: 44.8180 - val_loss: 12.3446 - val_mae: 42.2805 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.3177 - mae: 18.4140\n",
            "Epoch 00003: val_loss improved from 12.34460 to 4.59335, saving model to /home/jupyter/IEEE_Train/fold5.h5\n",
            "42/42 [==============================] - 5s 110ms/step - loss: 4.3177 - mae: 18.4140 - val_loss: 4.5934 - val_mae: 20.5377 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.9929 - mae: 14.8301\n",
            "Epoch 00004: val_loss improved from 4.59335 to 4.36681, saving model to /home/jupyter/IEEE_Train/fold5.h5\n",
            "42/42 [==============================] - 4s 105ms/step - loss: 3.9929 - mae: 14.8301 - val_loss: 4.3668 - val_mae: 20.1099 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.8876 - mae: 14.0511\n",
            "Epoch 00005: val_loss improved from 4.36681 to 3.94010, saving model to /home/jupyter/IEEE_Train/fold5.h5\n",
            "42/42 [==============================] - 4s 106ms/step - loss: 3.8876 - mae: 14.0511 - val_loss: 3.9401 - val_mae: 13.9658 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.7379 - mae: 13.0708\n",
            "Epoch 00006: val_loss did not improve from 3.94010\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 3.7379 - mae: 13.0708 - val_loss: 3.9830 - val_mae: 15.2027 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.7007 - mae: 12.4067\n",
            "Epoch 00007: val_loss did not improve from 3.94010\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 3.7007 - mae: 12.4067 - val_loss: 3.9629 - val_mae: 12.9691 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.5419 - mae: 10.7000\n",
            "Epoch 00008: val_loss improved from 3.94010 to 3.79705, saving model to /home/jupyter/IEEE_Train/fold5.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.5419 - mae: 10.7000 - val_loss: 3.7970 - val_mae: 12.5471 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.5067 - mae: 10.4223\n",
            "Epoch 00009: val_loss improved from 3.79705 to 3.56107, saving model to /home/jupyter/IEEE_Train/fold5.h5\n",
            "42/42 [==============================] - 4s 104ms/step - loss: 3.5067 - mae: 10.4223 - val_loss: 3.5611 - val_mae: 10.7253 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.4350 - mae: 10.1011\n",
            "Epoch 00010: val_loss did not improve from 3.56107\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.4350 - mae: 10.1011 - val_loss: 3.6051 - val_mae: 10.5234 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.4559 - mae: 9.9662\n",
            "Epoch 00011: val_loss did not improve from 3.56107\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 3.4559 - mae: 9.9662 - val_loss: 3.5852 - val_mae: 11.2565 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3334 - mae: 8.9718\n",
            "Epoch 00012: val_loss improved from 3.56107 to 3.54611, saving model to /home/jupyter/IEEE_Train/fold5.h5\n",
            "42/42 [==============================] - 4s 105ms/step - loss: 3.3334 - mae: 8.9718 - val_loss: 3.5461 - val_mae: 10.3429 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2763 - mae: 8.9926\n",
            "Epoch 00013: val_loss improved from 3.54611 to 3.53761, saving model to /home/jupyter/IEEE_Train/fold5.h5\n",
            "42/42 [==============================] - 4s 105ms/step - loss: 3.2763 - mae: 8.9926 - val_loss: 3.5376 - val_mae: 9.9589 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2599 - mae: 8.5862\n",
            "Epoch 00014: val_loss improved from 3.53761 to 3.47992, saving model to /home/jupyter/IEEE_Train/fold5.h5\n",
            "42/42 [==============================] - 4s 103ms/step - loss: 3.2599 - mae: 8.5862 - val_loss: 3.4799 - val_mae: 9.8309 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2303 - mae: 8.3799\n",
            "Epoch 00015: val_loss did not improve from 3.47992\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.2303 - mae: 8.3799 - val_loss: 3.7581 - val_mae: 10.0497 - lr: 2.5000e-04\n",
            "Epoch 16/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1918 - mae: 8.5004\n",
            "Epoch 00016: val_loss did not improve from 3.47992\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.1918 - mae: 8.5004 - val_loss: 3.5940 - val_mae: 9.7016 - lr: 2.5000e-04\n",
            "Epoch 17/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1605 - mae: 7.7946\n",
            "Epoch 00017: val_loss did not improve from 3.47992\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 3.1605 - mae: 7.7946 - val_loss: 3.4857 - val_mae: 9.6792 - lr: 1.2500e-04\n",
            "Epoch 18/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1253 - mae: 7.7899\n",
            "Epoch 00018: val_loss did not improve from 3.47992\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.1253 - mae: 7.7899 - val_loss: 3.4828 - val_mae: 9.4806 - lr: 1.2500e-04\n",
            "Epoch 19/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1102 - mae: 7.4000\n",
            "Epoch 00019: val_loss improved from 3.47992 to 3.44326, saving model to /home/jupyter/IEEE_Train/fold5.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.1102 - mae: 7.4000 - val_loss: 3.4433 - val_mae: 9.3841 - lr: 6.2500e-05\n",
            "Epoch 20/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0993 - mae: 7.4250\n",
            "Epoch 00020: val_loss did not improve from 3.44326\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0993 - mae: 7.4250 - val_loss: 3.4472 - val_mae: 9.3269 - lr: 6.2500e-05\n",
            "Epoch 21/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0964 - mae: 7.4795\n",
            "Epoch 00021: val_loss did not improve from 3.44326\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0964 - mae: 7.4795 - val_loss: 3.4653 - val_mae: 9.5627 - lr: 6.2500e-05\n",
            "Epoch 22/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0696 - mae: 7.2292\n",
            "Epoch 00022: val_loss did not improve from 3.44326\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0696 - mae: 7.2292 - val_loss: 3.4913 - val_mae: 8.4056 - lr: 3.1250e-05\n",
            "Epoch 23/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0618 - mae: 7.3324\n",
            "Epoch 00023: val_loss did not improve from 3.44326\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0618 - mae: 7.3324 - val_loss: 3.4794 - val_mae: 9.9742 - lr: 3.1250e-05\n",
            "Epoch 24/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0600 - mae: 7.0873\n",
            "Epoch 00024: val_loss improved from 3.44326 to 3.43612, saving model to /home/jupyter/IEEE_Train/fold5.h5\n",
            "42/42 [==============================] - 4s 106ms/step - loss: 3.0600 - mae: 7.0873 - val_loss: 3.4361 - val_mae: 8.4296 - lr: 1.5625e-05\n",
            "Epoch 25/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0573 - mae: 7.1600\n",
            "Epoch 00025: val_loss did not improve from 3.43612\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0573 - mae: 7.1600 - val_loss: 3.4577 - val_mae: 9.1579 - lr: 1.5625e-05\n",
            "Epoch 26/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0461 - mae: 7.2262\n",
            "Epoch 00026: val_loss did not improve from 3.43612\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0461 - mae: 7.2262 - val_loss: 3.4776 - val_mae: 8.5475 - lr: 1.5625e-05\n",
            "Epoch 27/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0438 - mae: 7.2849\n",
            "Epoch 00027: val_loss did not improve from 3.43612\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0438 - mae: 7.2849 - val_loss: 3.4714 - val_mae: 8.8798 - lr: 7.8125e-06\n",
            "Epoch 28/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0457 - mae: 7.4071\n",
            "Epoch 00028: val_loss did not improve from 3.43612\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0457 - mae: 7.4071 - val_loss: 3.4876 - val_mae: 9.1490 - lr: 7.8125e-06\n",
            "Epoch 29/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0428 - mae: 7.1626\n",
            "Epoch 00029: val_loss did not improve from 3.43612\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0428 - mae: 7.1626 - val_loss: 3.4675 - val_mae: 9.0648 - lr: 3.9063e-06\n",
            "Epoch 30/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0399 - mae: 7.2247\n",
            "Epoch 00030: val_loss did not improve from 3.43612\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0399 - mae: 7.2247 - val_loss: 3.4632 - val_mae: 8.9607 - lr: 3.9063e-06\n",
            "Epoch 31/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0440 - mae: 7.3938\n",
            "Epoch 00031: val_loss did not improve from 3.43612\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0440 - mae: 7.3938 - val_loss: 3.4940 - val_mae: 9.0213 - lr: 1.9531e-06\n",
            "Epoch 32/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0416 - mae: 7.0289\n",
            "Epoch 00032: val_loss did not improve from 3.43612\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0416 - mae: 7.0289 - val_loss: 3.4691 - val_mae: 8.9045 - lr: 1.9531e-06\n",
            "Epoch 33/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0373 - mae: 7.2978\n",
            "Epoch 00033: val_loss did not improve from 3.43612\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0373 - mae: 7.2978 - val_loss: 3.5071 - val_mae: 8.8932 - lr: 9.7656e-07\n",
            "Epoch 34/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0418 - mae: 7.2838\n",
            "Epoch 00034: val_loss did not improve from 3.43612\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0418 - mae: 7.2838 - val_loss: 3.4511 - val_mae: 8.9250 - lr: 9.7656e-07\n",
            "Epoch 35/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0329 - mae: 7.2195\n",
            "Epoch 00035: val_loss did not improve from 3.43612\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0329 - mae: 7.2195 - val_loss: 3.4540 - val_mae: 8.9794 - lr: 4.8828e-07\n",
            "Epoch 36/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0372 - mae: 7.0372\n",
            "Epoch 00036: val_loss did not improve from 3.43612\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0372 - mae: 7.0372 - val_loss: 3.4418 - val_mae: 9.0603 - lr: 4.8828e-07\n",
            "Epoch 37/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0437 - mae: 7.1415\n",
            "Epoch 00037: val_loss did not improve from 3.43612\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0437 - mae: 7.1415 - val_loss: 3.5010 - val_mae: 9.1121 - lr: 2.4414e-07\n",
            "Epoch 38/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0371 - mae: 6.9978\n",
            "Epoch 00038: val_loss did not improve from 3.43612\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0371 - mae: 6.9978 - val_loss: 3.4904 - val_mae: 8.7526 - lr: 2.4414e-07\n",
            "Epoch 39/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0442 - mae: 7.1710\n",
            "Epoch 00039: val_loss did not improve from 3.43612\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0442 - mae: 7.1710 - val_loss: 3.4908 - val_mae: 8.2091 - lr: 1.2207e-07\n",
            "Epoch 40/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0404 - mae: 7.3687\n",
            "Epoch 00040: val_loss did not improve from 3.43612\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0404 - mae: 7.3687 - val_loss: 3.4599 - val_mae: 8.7299 - lr: 1.2207e-07\n",
            "Epoch 41/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0456 - mae: 7.2631\n",
            "Epoch 00041: val_loss did not improve from 3.43612\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0456 - mae: 7.2631 - val_loss: 3.4527 - val_mae: 8.0311 - lr: 6.1035e-08\n",
            "Epoch 42/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0439 - mae: 7.1651\n",
            "Epoch 00042: val_loss did not improve from 3.43612\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0439 - mae: 7.1651 - val_loss: 3.4726 - val_mae: 9.4430 - lr: 6.1035e-08\n",
            "Epoch 43/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0333 - mae: 7.1279\n",
            "Epoch 00043: val_loss improved from 3.43612 to 3.42947, saving model to /home/jupyter/IEEE_Train/fold5.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.0333 - mae: 7.1279 - val_loss: 3.4295 - val_mae: 9.3297 - lr: 3.0518e-08\n",
            "Epoch 44/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0424 - mae: 7.2688\n",
            "Epoch 00044: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0424 - mae: 7.2688 - val_loss: 3.4521 - val_mae: 8.4951 - lr: 3.0518e-08\n",
            "Epoch 45/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0348 - mae: 7.0560\n",
            "Epoch 00045: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0348 - mae: 7.0560 - val_loss: 3.4584 - val_mae: 8.9079 - lr: 3.0518e-08\n",
            "Epoch 46/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0474 - mae: 7.3433\n",
            "Epoch 00046: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 3.0474 - mae: 7.3433 - val_loss: 3.4891 - val_mae: 9.0550 - lr: 1.5259e-08\n",
            "Epoch 47/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0379 - mae: 7.0499\n",
            "Epoch 00047: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0379 - mae: 7.0499 - val_loss: 3.4546 - val_mae: 8.8498 - lr: 1.5259e-08\n",
            "Epoch 48/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0532 - mae: 7.1565\n",
            "Epoch 00048: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0532 - mae: 7.1565 - val_loss: 3.4699 - val_mae: 9.0704 - lr: 7.6294e-09\n",
            "Epoch 49/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0384 - mae: 7.1155\n",
            "Epoch 00049: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0384 - mae: 7.1155 - val_loss: 3.4559 - val_mae: 9.2862 - lr: 7.6294e-09\n",
            "Epoch 50/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0371 - mae: 7.1770\n",
            "Epoch 00050: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0371 - mae: 7.1770 - val_loss: 3.4482 - val_mae: 8.4879 - lr: 3.8147e-09\n",
            "Epoch 51/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0386 - mae: 7.3608\n",
            "Epoch 00051: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0386 - mae: 7.3608 - val_loss: 3.4640 - val_mae: 8.3605 - lr: 3.8147e-09\n",
            "Epoch 52/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0374 - mae: 7.0612\n",
            "Epoch 00052: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0374 - mae: 7.0612 - val_loss: 3.4760 - val_mae: 8.7916 - lr: 1.9073e-09\n",
            "Epoch 53/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0475 - mae: 7.1685\n",
            "Epoch 00053: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00053: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0475 - mae: 7.1685 - val_loss: 3.4824 - val_mae: 8.3677 - lr: 1.9073e-09\n",
            "Epoch 54/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0385 - mae: 7.0808\n",
            "Epoch 00054: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0385 - mae: 7.0808 - val_loss: 3.4661 - val_mae: 9.4946 - lr: 9.5367e-10\n",
            "Epoch 55/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0484 - mae: 7.4158\n",
            "Epoch 00055: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0484 - mae: 7.4158 - val_loss: 3.4682 - val_mae: 9.0847 - lr: 9.5367e-10\n",
            "Epoch 56/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0361 - mae: 7.2632\n",
            "Epoch 00056: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0361 - mae: 7.2632 - val_loss: 3.4667 - val_mae: 8.6188 - lr: 4.7684e-10\n",
            "Epoch 57/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0428 - mae: 6.9635\n",
            "Epoch 00057: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0428 - mae: 6.9635 - val_loss: 3.4809 - val_mae: 9.3274 - lr: 4.7684e-10\n",
            "Epoch 58/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0480 - mae: 7.3111\n",
            "Epoch 00058: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0480 - mae: 7.3111 - val_loss: 3.4598 - val_mae: 8.1789 - lr: 2.3842e-10\n",
            "Epoch 59/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0454 - mae: 7.4619\n",
            "Epoch 00059: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.0454 - mae: 7.4619 - val_loss: 3.4954 - val_mae: 9.3510 - lr: 2.3842e-10\n",
            "Epoch 60/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0350 - mae: 7.2448\n",
            "Epoch 00060: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0350 - mae: 7.2448 - val_loss: 3.4644 - val_mae: 8.8370 - lr: 1.1921e-10\n",
            "Epoch 61/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0475 - mae: 7.0882\n",
            "Epoch 00061: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0475 - mae: 7.0882 - val_loss: 3.4589 - val_mae: 9.4885 - lr: 1.1921e-10\n",
            "Epoch 62/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0425 - mae: 7.0337\n",
            "Epoch 00062: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.0425 - mae: 7.0337 - val_loss: 3.4667 - val_mae: 9.6173 - lr: 5.9605e-11\n",
            "Epoch 63/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0303 - mae: 7.3525\n",
            "Epoch 00063: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00063: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0303 - mae: 7.3525 - val_loss: 3.4850 - val_mae: 7.9426 - lr: 5.9605e-11\n",
            "Epoch 64/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0387 - mae: 7.0759\n",
            "Epoch 00064: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0387 - mae: 7.0759 - val_loss: 3.4817 - val_mae: 8.8315 - lr: 2.9802e-11\n",
            "Epoch 65/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0427 - mae: 7.3752\n",
            "Epoch 00065: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0427 - mae: 7.3752 - val_loss: 3.4878 - val_mae: 9.2029 - lr: 2.9802e-11\n",
            "Epoch 66/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0409 - mae: 7.1197\n",
            "Epoch 00066: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.0409 - mae: 7.1197 - val_loss: 3.4740 - val_mae: 8.8795 - lr: 1.4901e-11\n",
            "Epoch 67/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0443 - mae: 6.9567\n",
            "Epoch 00067: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00067: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0443 - mae: 6.9567 - val_loss: 3.4861 - val_mae: 8.9727 - lr: 1.4901e-11\n",
            "Epoch 68/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0422 - mae: 6.8504\n",
            "Epoch 00068: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0422 - mae: 6.8504 - val_loss: 3.4632 - val_mae: 9.1365 - lr: 7.4506e-12\n",
            "Epoch 69/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0457 - mae: 7.2263\n",
            "Epoch 00069: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00069: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0457 - mae: 7.2263 - val_loss: 3.4850 - val_mae: 9.2439 - lr: 7.4506e-12\n",
            "Epoch 70/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0380 - mae: 7.1134\n",
            "Epoch 00070: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.0380 - mae: 7.1134 - val_loss: 3.4509 - val_mae: 8.6116 - lr: 3.7253e-12\n",
            "Epoch 71/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0407 - mae: 7.1403\n",
            "Epoch 00071: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1.8626452377018543e-12.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.0407 - mae: 7.1403 - val_loss: 3.4586 - val_mae: 8.9007 - lr: 3.7253e-12\n",
            "Epoch 72/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0455 - mae: 7.1083\n",
            "Epoch 00072: val_loss did not improve from 3.42947\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0455 - mae: 7.1083 - val_loss: 3.4623 - val_mae: 8.8535 - lr: 1.8626e-12\n",
            "Epoch 73/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0486 - mae: 7.3661Restoring model weights from the end of the best epoch: 43.\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 3.42947\n",
            "\n",
            "Epoch 00073: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0486 - mae: 7.3661 - val_loss: 3.4816 - val_mae: 8.9418 - lr: 1.8626e-12\n",
            "Epoch 00073: early stopping\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 3.2633 - mae: 7.2731\n",
            "Fit model on training data fold  6\n",
            "Epoch 1/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 63.0848 - mae: 128.8028\n",
            "Epoch 00001: val_loss improved from inf to 14.52554, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 18s 171ms/step - loss: 63.0848 - mae: 128.8028 - val_loss: 14.5255 - val_mae: 100.0260 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 7.3967 - mae: 50.3199\n",
            "Epoch 00002: val_loss improved from 14.52554 to 4.53644, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 5s 112ms/step - loss: 7.3967 - mae: 50.3199 - val_loss: 4.5364 - val_mae: 24.3445 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.3993 - mae: 22.1945\n",
            "Epoch 00003: val_loss improved from 4.53644 to 4.45230, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 5s 110ms/step - loss: 4.3993 - mae: 22.1945 - val_loss: 4.4523 - val_mae: 24.6415 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.2692 - mae: 20.9833\n",
            "Epoch 00004: val_loss improved from 4.45230 to 4.33451, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 5s 109ms/step - loss: 4.2692 - mae: 20.9833 - val_loss: 4.3345 - val_mae: 22.8578 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.2054 - mae: 19.2624\n",
            "Epoch 00005: val_loss improved from 4.33451 to 4.15845, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 5s 108ms/step - loss: 4.2054 - mae: 19.2624 - val_loss: 4.1584 - val_mae: 19.7885 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.0682 - mae: 18.0577\n",
            "Epoch 00006: val_loss improved from 4.15845 to 4.09996, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 5s 108ms/step - loss: 4.0682 - mae: 18.0577 - val_loss: 4.1000 - val_mae: 18.1829 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.0527 - mae: 16.7913\n",
            "Epoch 00007: val_loss did not improve from 4.09996\n",
            "42/42 [==============================] - 4s 103ms/step - loss: 4.0527 - mae: 16.7913 - val_loss: 4.1716 - val_mae: 19.3050 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.0430 - mae: 16.5479\n",
            "Epoch 00008: val_loss improved from 4.09996 to 3.98701, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 108ms/step - loss: 4.0430 - mae: 16.5479 - val_loss: 3.9870 - val_mae: 16.8273 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.8692 - mae: 15.0971\n",
            "Epoch 00009: val_loss did not improve from 3.98701\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.8692 - mae: 15.0971 - val_loss: 4.0813 - val_mae: 17.3093 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.7813 - mae: 14.0505\n",
            "Epoch 00010: val_loss improved from 3.98701 to 3.82115, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 103ms/step - loss: 3.7813 - mae: 14.0505 - val_loss: 3.8212 - val_mae: 14.7510 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.6707 - mae: 13.2766\n",
            "Epoch 00011: val_loss improved from 3.82115 to 3.75061, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 104ms/step - loss: 3.6707 - mae: 13.2766 - val_loss: 3.7506 - val_mae: 13.2629 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.6067 - mae: 11.9921\n",
            "Epoch 00012: val_loss improved from 3.75061 to 3.65530, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 103ms/step - loss: 3.6067 - mae: 11.9921 - val_loss: 3.6553 - val_mae: 11.4135 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.5865 - mae: 11.8752\n",
            "Epoch 00013: val_loss improved from 3.65530 to 3.61682, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.5865 - mae: 11.8752 - val_loss: 3.6168 - val_mae: 11.8498 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.4576 - mae: 10.3223\n",
            "Epoch 00014: val_loss improved from 3.61682 to 3.57004, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 103ms/step - loss: 3.4576 - mae: 10.3223 - val_loss: 3.5700 - val_mae: 10.5633 - lr: 0.0010\n",
            "Epoch 15/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3492 - mae: 9.5646\n",
            "Epoch 00015: val_loss improved from 3.57004 to 3.55657, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 3.3492 - mae: 9.5646 - val_loss: 3.5566 - val_mae: 10.2981 - lr: 0.0010\n",
            "Epoch 16/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3659 - mae: 9.4183\n",
            "Epoch 00016: val_loss did not improve from 3.55657\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.3659 - mae: 9.4183 - val_loss: 3.5922 - val_mae: 9.9390 - lr: 0.0010\n",
            "Epoch 17/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2427 - mae: 8.2088\n",
            "Epoch 00017: val_loss improved from 3.55657 to 3.42250, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 3.2427 - mae: 8.2088 - val_loss: 3.4225 - val_mae: 9.5962 - lr: 0.0010\n",
            "Epoch 18/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1461 - mae: 7.5391\n",
            "Epoch 00018: val_loss did not improve from 3.42250\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.1461 - mae: 7.5391 - val_loss: 3.5176 - val_mae: 8.6582 - lr: 0.0010\n",
            "Epoch 19/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2161 - mae: 7.6364\n",
            "Epoch 00019: val_loss improved from 3.42250 to 3.38521, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 3.2161 - mae: 7.6364 - val_loss: 3.3852 - val_mae: 7.8848 - lr: 0.0010\n",
            "Epoch 20/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1450 - mae: 6.8634\n",
            "Epoch 00020: val_loss did not improve from 3.38521\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.1450 - mae: 6.8634 - val_loss: 3.9191 - val_mae: 10.3853 - lr: 0.0010\n",
            "Epoch 21/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2769 - mae: 7.2511\n",
            "Epoch 00021: val_loss did not improve from 3.38521\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 3.2769 - mae: 7.2511 - val_loss: 3.4512 - val_mae: 8.0209 - lr: 0.0010\n",
            "Epoch 22/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.9131 - mae: 6.0242\n",
            "Epoch 00022: val_loss did not improve from 3.38521\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.9131 - mae: 6.0242 - val_loss: 3.5957 - val_mae: 8.8222 - lr: 5.0000e-04\n",
            "Epoch 23/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8008 - mae: 5.4088\n",
            "Epoch 00023: val_loss improved from 3.38521 to 3.35422, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.8008 - mae: 5.4088 - val_loss: 3.3542 - val_mae: 6.9645 - lr: 5.0000e-04\n",
            "Epoch 24/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7115 - mae: 5.0162\n",
            "Epoch 00024: val_loss improved from 3.35422 to 3.32261, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.7115 - mae: 5.0162 - val_loss: 3.3226 - val_mae: 6.8240 - lr: 5.0000e-04\n",
            "Epoch 25/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.6856 - mae: 4.7470\n",
            "Epoch 00025: val_loss improved from 3.32261 to 3.25548, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.6856 - mae: 4.7470 - val_loss: 3.2555 - val_mae: 6.3063 - lr: 5.0000e-04\n",
            "Epoch 26/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7097 - mae: 4.6751\n",
            "Epoch 00026: val_loss improved from 3.25548 to 3.21076, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.7097 - mae: 4.6751 - val_loss: 3.2108 - val_mae: 6.0608 - lr: 5.0000e-04\n",
            "Epoch 27/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.6413 - mae: 4.4612\n",
            "Epoch 00027: val_loss did not improve from 3.21076\n",
            "42/42 [==============================] - 4s 93ms/step - loss: 2.6413 - mae: 4.4612 - val_loss: 3.3569 - val_mae: 6.0160 - lr: 5.0000e-04\n",
            "Epoch 28/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.6335 - mae: 4.1215\n",
            "Epoch 00028: val_loss improved from 3.21076 to 3.02540, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.6335 - mae: 4.1215 - val_loss: 3.0254 - val_mae: 5.1805 - lr: 5.0000e-04\n",
            "Epoch 29/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.6249 - mae: 3.9698\n",
            "Epoch 00029: val_loss improved from 3.02540 to 2.99633, saving model to /home/jupyter/IEEE_Train/fold6.h5\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.6249 - mae: 3.9698 - val_loss: 2.9963 - val_mae: 4.6759 - lr: 5.0000e-04\n",
            "Epoch 30/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.5720 - mae: 3.9273\n",
            "Epoch 00030: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.5720 - mae: 3.9273 - val_loss: 3.3065 - val_mae: 5.4374 - lr: 5.0000e-04\n",
            "Epoch 31/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.5710 - mae: 3.8227\n",
            "Epoch 00031: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.5710 - mae: 3.8227 - val_loss: 3.0030 - val_mae: 4.7392 - lr: 5.0000e-04\n",
            "Epoch 32/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.1842 - mae: 2.8916\n",
            "Epoch 00032: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.1842 - mae: 2.8916 - val_loss: 3.1808 - val_mae: 4.7218 - lr: 2.5000e-04\n",
            "Epoch 33/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.1395 - mae: 2.6737\n",
            "Epoch 00033: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.1395 - mae: 2.6737 - val_loss: 3.2508 - val_mae: 4.2289 - lr: 2.5000e-04\n",
            "Epoch 34/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0276 - mae: 2.4538\n",
            "Epoch 00034: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.0276 - mae: 2.4538 - val_loss: 3.2091 - val_mae: 4.3403 - lr: 1.2500e-04\n",
            "Epoch 35/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9598 - mae: 2.2107\n",
            "Epoch 00035: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.9598 - mae: 2.2107 - val_loss: 3.4399 - val_mae: 4.1968 - lr: 1.2500e-04\n",
            "Epoch 36/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9442 - mae: 2.2679\n",
            "Epoch 00036: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.9442 - mae: 2.2679 - val_loss: 3.3387 - val_mae: 4.2415 - lr: 6.2500e-05\n",
            "Epoch 37/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8929 - mae: 2.1661\n",
            "Epoch 00037: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.8929 - mae: 2.1661 - val_loss: 3.3835 - val_mae: 4.0087 - lr: 6.2500e-05\n",
            "Epoch 38/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8512 - mae: 2.0587\n",
            "Epoch 00038: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.8512 - mae: 2.0587 - val_loss: 3.4496 - val_mae: 4.2916 - lr: 3.1250e-05\n",
            "Epoch 39/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8160 - mae: 2.0268\n",
            "Epoch 00039: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.8160 - mae: 2.0268 - val_loss: 3.2442 - val_mae: 3.7992 - lr: 3.1250e-05\n",
            "Epoch 40/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8303 - mae: 2.0605\n",
            "Epoch 00040: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 1.8303 - mae: 2.0605 - val_loss: 3.5645 - val_mae: 4.0298 - lr: 1.5625e-05\n",
            "Epoch 41/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8152 - mae: 1.9358\n",
            "Epoch 00041: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 1.8152 - mae: 1.9358 - val_loss: 3.4783 - val_mae: 4.1102 - lr: 1.5625e-05\n",
            "Epoch 42/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7784 - mae: 1.8883\n",
            "Epoch 00042: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7784 - mae: 1.8883 - val_loss: 3.4134 - val_mae: 3.8295 - lr: 7.8125e-06\n",
            "Epoch 43/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7928 - mae: 1.9211\n",
            "Epoch 00043: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7928 - mae: 1.9211 - val_loss: 3.3005 - val_mae: 3.7926 - lr: 7.8125e-06\n",
            "Epoch 44/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7789 - mae: 1.9431\n",
            "Epoch 00044: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.7789 - mae: 1.9431 - val_loss: 3.5122 - val_mae: 3.9821 - lr: 3.9063e-06\n",
            "Epoch 45/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7833 - mae: 1.9369\n",
            "Epoch 00045: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7833 - mae: 1.9369 - val_loss: 3.4954 - val_mae: 4.0608 - lr: 3.9063e-06\n",
            "Epoch 46/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7913 - mae: 1.7793\n",
            "Epoch 00046: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 1.7913 - mae: 1.7793 - val_loss: 3.4471 - val_mae: 3.8788 - lr: 1.9531e-06\n",
            "Epoch 47/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7699 - mae: 1.8681\n",
            "Epoch 00047: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 1.7699 - mae: 1.8681 - val_loss: 3.4487 - val_mae: 3.9272 - lr: 1.9531e-06\n",
            "Epoch 48/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7795 - mae: 1.9519\n",
            "Epoch 00048: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.7795 - mae: 1.9519 - val_loss: 3.4772 - val_mae: 3.8913 - lr: 9.7656e-07\n",
            "Epoch 49/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7578 - mae: 1.9697\n",
            "Epoch 00049: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7578 - mae: 1.9697 - val_loss: 3.4126 - val_mae: 3.7212 - lr: 9.7656e-07\n",
            "Epoch 50/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7911 - mae: 1.8901\n",
            "Epoch 00050: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.7911 - mae: 1.8901 - val_loss: 3.4361 - val_mae: 3.9274 - lr: 4.8828e-07\n",
            "Epoch 51/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7708 - mae: 1.8836\n",
            "Epoch 00051: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7708 - mae: 1.8836 - val_loss: 3.5313 - val_mae: 3.8568 - lr: 4.8828e-07\n",
            "Epoch 52/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7775 - mae: 1.8843\n",
            "Epoch 00052: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 1.7775 - mae: 1.8843 - val_loss: 3.5247 - val_mae: 3.8856 - lr: 2.4414e-07\n",
            "Epoch 53/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7554 - mae: 1.9256\n",
            "Epoch 00053: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7554 - mae: 1.9256 - val_loss: 3.5466 - val_mae: 4.0897 - lr: 2.4414e-07\n",
            "Epoch 54/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7794 - mae: 1.9098\n",
            "Epoch 00054: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7794 - mae: 1.9098 - val_loss: 3.4646 - val_mae: 4.0207 - lr: 1.2207e-07\n",
            "Epoch 55/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7763 - mae: 1.8941\n",
            "Epoch 00055: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7763 - mae: 1.8941 - val_loss: 3.4076 - val_mae: 3.9209 - lr: 1.2207e-07\n",
            "Epoch 56/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7591 - mae: 1.9376\n",
            "Epoch 00056: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7591 - mae: 1.9376 - val_loss: 3.4203 - val_mae: 3.9831 - lr: 6.1035e-08\n",
            "Epoch 57/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7698 - mae: 1.9395\n",
            "Epoch 00057: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.7698 - mae: 1.9395 - val_loss: 3.5008 - val_mae: 3.8242 - lr: 6.1035e-08\n",
            "Epoch 58/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7662 - mae: 1.9166\n",
            "Epoch 00058: val_loss did not improve from 2.99633\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7662 - mae: 1.9166 - val_loss: 3.5590 - val_mae: 4.0749 - lr: 3.0518e-08\n",
            "Epoch 59/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7712 - mae: 1.9002Restoring model weights from the end of the best epoch: 29.\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 2.99633\n",
            "\n",
            "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7712 - mae: 1.9002 - val_loss: 3.6178 - val_mae: 4.0415 - lr: 3.0518e-08\n",
            "Epoch 00059: early stopping\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 2.8544 - mae: 4.2011\n",
            "Fit model on training data fold  7\n",
            "Epoch 1/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 90.6050 - mae: 127.9431\n",
            "Epoch 00001: val_loss improved from inf to 17.41631, saving model to /home/jupyter/IEEE_Train/fold7.h5\n",
            "42/42 [==============================] - 17s 169ms/step - loss: 90.6050 - mae: 127.9431 - val_loss: 17.4163 - val_mae: 88.0550 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 7.6319 - mae: 38.4280\n",
            "Epoch 00002: val_loss improved from 17.41631 to 4.40989, saving model to /home/jupyter/IEEE_Train/fold7.h5\n",
            "42/42 [==============================] - 5s 113ms/step - loss: 7.6319 - mae: 38.4280 - val_loss: 4.4099 - val_mae: 17.7021 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.1779 - mae: 16.3777\n",
            "Epoch 00003: val_loss improved from 4.40989 to 4.25689, saving model to /home/jupyter/IEEE_Train/fold7.h5\n",
            "42/42 [==============================] - 5s 111ms/step - loss: 4.1779 - mae: 16.3777 - val_loss: 4.2569 - val_mae: 16.8849 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.9273 - mae: 14.6806\n",
            "Epoch 00004: val_loss improved from 4.25689 to 3.90754, saving model to /home/jupyter/IEEE_Train/fold7.h5\n",
            "42/42 [==============================] - 4s 107ms/step - loss: 3.9273 - mae: 14.6806 - val_loss: 3.9075 - val_mae: 13.7871 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.8180 - mae: 13.3233\n",
            "Epoch 00005: val_loss improved from 3.90754 to 3.90716, saving model to /home/jupyter/IEEE_Train/fold7.h5\n",
            "42/42 [==============================] - 4s 107ms/step - loss: 3.8180 - mae: 13.3233 - val_loss: 3.9072 - val_mae: 13.4230 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.6988 - mae: 12.4011\n",
            "Epoch 00006: val_loss improved from 3.90716 to 3.72325, saving model to /home/jupyter/IEEE_Train/fold7.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.6988 - mae: 12.4011 - val_loss: 3.7233 - val_mae: 13.6917 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.6783 - mae: 11.7936\n",
            "Epoch 00007: val_loss did not improve from 3.72325\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.6783 - mae: 11.7936 - val_loss: 3.7441 - val_mae: 12.0993 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.5059 - mae: 10.2209\n",
            "Epoch 00008: val_loss improved from 3.72325 to 3.58355, saving model to /home/jupyter/IEEE_Train/fold7.h5\n",
            "42/42 [==============================] - 4s 104ms/step - loss: 3.5059 - mae: 10.2209 - val_loss: 3.5835 - val_mae: 10.8702 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.4352 - mae: 9.2011\n",
            "Epoch 00009: val_loss improved from 3.58355 to 3.42647, saving model to /home/jupyter/IEEE_Train/fold7.h5\n",
            "42/42 [==============================] - 4s 104ms/step - loss: 3.4352 - mae: 9.2011 - val_loss: 3.4265 - val_mae: 9.7554 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3589 - mae: 8.6492\n",
            "Epoch 00010: val_loss did not improve from 3.42647\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.3589 - mae: 8.6492 - val_loss: 3.6121 - val_mae: 9.2648 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2522 - mae: 8.1039\n",
            "Epoch 00011: val_loss improved from 3.42647 to 3.26155, saving model to /home/jupyter/IEEE_Train/fold7.h5\n",
            "42/42 [==============================] - 4s 104ms/step - loss: 3.2522 - mae: 8.1039 - val_loss: 3.2616 - val_mae: 7.8361 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1531 - mae: 7.0183\n",
            "Epoch 00012: val_loss did not improve from 3.26155\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.1531 - mae: 7.0183 - val_loss: 3.9535 - val_mae: 9.5238 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0675 - mae: 6.2656\n",
            "Epoch 00013: val_loss improved from 3.26155 to 3.09143, saving model to /home/jupyter/IEEE_Train/fold7.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.0675 - mae: 6.2656 - val_loss: 3.0914 - val_mae: 6.3493 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1578 - mae: 6.5675\n",
            "Epoch 00014: val_loss did not improve from 3.09143\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 3.1578 - mae: 6.5675 - val_loss: 3.3888 - val_mae: 6.8432 - lr: 0.0010\n",
            "Epoch 15/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0186 - mae: 5.9095\n",
            "Epoch 00015: val_loss did not improve from 3.09143\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0186 - mae: 5.9095 - val_loss: 5.1798 - val_mae: 12.3278 - lr: 0.0010\n",
            "Epoch 16/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7861 - mae: 4.8798\n",
            "Epoch 00016: val_loss did not improve from 3.09143\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.7861 - mae: 4.8798 - val_loss: 3.2401 - val_mae: 6.5244 - lr: 5.0000e-04\n",
            "Epoch 17/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.6581 - mae: 4.1198\n",
            "Epoch 00017: val_loss did not improve from 3.09143\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.6581 - mae: 4.1198 - val_loss: 3.1973 - val_mae: 5.9113 - lr: 5.0000e-04\n",
            "Epoch 18/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.4960 - mae: 3.7778\n",
            "Epoch 00018: val_loss did not improve from 3.09143\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.4960 - mae: 3.7778 - val_loss: 3.2776 - val_mae: 5.4624 - lr: 2.5000e-04\n",
            "Epoch 19/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.4404 - mae: 3.4856\n",
            "Epoch 00019: val_loss did not improve from 3.09143\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.4404 - mae: 3.4856 - val_loss: 3.1545 - val_mae: 5.4123 - lr: 2.5000e-04\n",
            "Epoch 20/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.3103 - mae: 3.0573\n",
            "Epoch 00020: val_loss did not improve from 3.09143\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.3103 - mae: 3.0573 - val_loss: 3.3087 - val_mae: 5.4680 - lr: 1.2500e-04\n",
            "Epoch 21/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.2280 - mae: 2.9542\n",
            "Epoch 00021: val_loss did not improve from 3.09143\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.2280 - mae: 2.9542 - val_loss: 3.3747 - val_mae: 5.4456 - lr: 1.2500e-04\n",
            "Epoch 22/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.2057 - mae: 2.8827\n",
            "Epoch 00022: val_loss did not improve from 3.09143\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.2057 - mae: 2.8827 - val_loss: 3.1422 - val_mae: 4.6326 - lr: 6.2500e-05\n",
            "Epoch 23/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.1627 - mae: 2.6034\n",
            "Epoch 00023: val_loss improved from 3.09143 to 3.05439, saving model to /home/jupyter/IEEE_Train/fold7.h5\n",
            "42/42 [==============================] - 4s 103ms/step - loss: 2.1627 - mae: 2.6034 - val_loss: 3.0544 - val_mae: 4.5252 - lr: 6.2500e-05\n",
            "Epoch 24/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.1562 - mae: 2.6520\n",
            "Epoch 00024: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.1562 - mae: 2.6520 - val_loss: 3.1779 - val_mae: 4.5538 - lr: 6.2500e-05\n",
            "Epoch 25/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.1751 - mae: 2.7447\n",
            "Epoch 00025: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.1751 - mae: 2.7447 - val_loss: 3.2379 - val_mae: 4.6083 - lr: 6.2500e-05\n",
            "Epoch 26/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0679 - mae: 2.5139\n",
            "Epoch 00026: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.0679 - mae: 2.5139 - val_loss: 3.1206 - val_mae: 4.3618 - lr: 3.1250e-05\n",
            "Epoch 27/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0857 - mae: 2.5354\n",
            "Epoch 00027: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.0857 - mae: 2.5354 - val_loss: 3.1687 - val_mae: 4.5390 - lr: 3.1250e-05\n",
            "Epoch 28/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0861 - mae: 2.5214\n",
            "Epoch 00028: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.0861 - mae: 2.5214 - val_loss: 3.2744 - val_mae: 4.5004 - lr: 1.5625e-05\n",
            "Epoch 29/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0468 - mae: 2.4025\n",
            "Epoch 00029: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.0468 - mae: 2.4025 - val_loss: 3.3125 - val_mae: 4.4955 - lr: 1.5625e-05\n",
            "Epoch 30/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0214 - mae: 2.3739\n",
            "Epoch 00030: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.0214 - mae: 2.3739 - val_loss: 3.1738 - val_mae: 4.3665 - lr: 7.8125e-06\n",
            "Epoch 31/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0323 - mae: 2.3644\n",
            "Epoch 00031: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.0323 - mae: 2.3644 - val_loss: 3.2616 - val_mae: 4.5248 - lr: 7.8125e-06\n",
            "Epoch 32/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0453 - mae: 2.4720\n",
            "Epoch 00032: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.0453 - mae: 2.4720 - val_loss: 3.6060 - val_mae: 4.9192 - lr: 3.9063e-06\n",
            "Epoch 33/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0172 - mae: 2.3388\n",
            "Epoch 00033: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.0172 - mae: 2.3388 - val_loss: 3.3386 - val_mae: 4.3765 - lr: 3.9063e-06\n",
            "Epoch 34/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0555 - mae: 2.4572\n",
            "Epoch 00034: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.0555 - mae: 2.4572 - val_loss: 3.2637 - val_mae: 4.3180 - lr: 1.9531e-06\n",
            "Epoch 35/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0023 - mae: 2.3900\n",
            "Epoch 00035: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.0023 - mae: 2.3900 - val_loss: 3.1745 - val_mae: 4.3660 - lr: 1.9531e-06\n",
            "Epoch 36/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9856 - mae: 2.3897\n",
            "Epoch 00036: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 1.9856 - mae: 2.3897 - val_loss: 3.2801 - val_mae: 4.5859 - lr: 9.7656e-07\n",
            "Epoch 37/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0061 - mae: 2.4019\n",
            "Epoch 00037: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.0061 - mae: 2.4019 - val_loss: 3.2412 - val_mae: 4.1239 - lr: 9.7656e-07\n",
            "Epoch 38/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0526 - mae: 2.4481\n",
            "Epoch 00038: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.0526 - mae: 2.4481 - val_loss: 3.1925 - val_mae: 4.4247 - lr: 4.8828e-07\n",
            "Epoch 39/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0247 - mae: 2.3291\n",
            "Epoch 00039: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.0247 - mae: 2.3291 - val_loss: 3.2802 - val_mae: 4.6623 - lr: 4.8828e-07\n",
            "Epoch 40/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0400 - mae: 2.3777\n",
            "Epoch 00040: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.0400 - mae: 2.3777 - val_loss: 3.3131 - val_mae: 4.5413 - lr: 2.4414e-07\n",
            "Epoch 41/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0325 - mae: 2.3939\n",
            "Epoch 00041: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.0325 - mae: 2.3939 - val_loss: 3.1782 - val_mae: 4.3735 - lr: 2.4414e-07\n",
            "Epoch 42/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9630 - mae: 2.2737\n",
            "Epoch 00042: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 1.9630 - mae: 2.2737 - val_loss: 3.3075 - val_mae: 4.3881 - lr: 1.2207e-07\n",
            "Epoch 43/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9988 - mae: 2.4624\n",
            "Epoch 00043: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.9988 - mae: 2.4624 - val_loss: 3.2978 - val_mae: 4.4718 - lr: 1.2207e-07\n",
            "Epoch 44/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0255 - mae: 2.3982\n",
            "Epoch 00044: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.0255 - mae: 2.3982 - val_loss: 3.0935 - val_mae: 4.0764 - lr: 6.1035e-08\n",
            "Epoch 45/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9834 - mae: 2.3809\n",
            "Epoch 00045: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 1.9834 - mae: 2.3809 - val_loss: 3.4320 - val_mae: 4.8727 - lr: 6.1035e-08\n",
            "Epoch 46/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0284 - mae: 2.3361\n",
            "Epoch 00046: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.0284 - mae: 2.3361 - val_loss: 3.3423 - val_mae: 4.5112 - lr: 3.0518e-08\n",
            "Epoch 47/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9899 - mae: 2.3598\n",
            "Epoch 00047: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.9899 - mae: 2.3598 - val_loss: 3.1874 - val_mae: 3.9456 - lr: 3.0518e-08\n",
            "Epoch 48/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0217 - mae: 2.3987\n",
            "Epoch 00048: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.0217 - mae: 2.3987 - val_loss: 3.3176 - val_mae: 4.7130 - lr: 1.5259e-08\n",
            "Epoch 49/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9654 - mae: 2.3131\n",
            "Epoch 00049: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.9654 - mae: 2.3131 - val_loss: 3.3398 - val_mae: 4.5404 - lr: 1.5259e-08\n",
            "Epoch 50/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0186 - mae: 2.4452\n",
            "Epoch 00050: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.0186 - mae: 2.4452 - val_loss: 3.3806 - val_mae: 4.7173 - lr: 7.6294e-09\n",
            "Epoch 51/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0508 - mae: 2.4023\n",
            "Epoch 00051: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.0508 - mae: 2.4023 - val_loss: 3.3309 - val_mae: 4.5161 - lr: 7.6294e-09\n",
            "Epoch 52/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0486 - mae: 2.4220\n",
            "Epoch 00052: val_loss did not improve from 3.05439\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.0486 - mae: 2.4220 - val_loss: 3.2599 - val_mae: 4.3363 - lr: 3.8147e-09\n",
            "Epoch 53/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0289 - mae: 2.4804Restoring model weights from the end of the best epoch: 23.\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 3.05439\n",
            "\n",
            "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.0289 - mae: 2.4804 - val_loss: 3.3086 - val_mae: 4.3957 - lr: 3.8147e-09\n",
            "Epoch 00053: early stopping\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 3.9327 - mae: 5.1514\n",
            "Fit model on training data fold  8\n",
            "Epoch 1/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 108.8893 - mae: 145.1113\n",
            "Epoch 00001: val_loss improved from inf to 27.14511, saving model to /home/jupyter/IEEE_Train/fold8.h5\n",
            "42/42 [==============================] - 17s 170ms/step - loss: 108.8893 - mae: 145.1113 - val_loss: 27.1451 - val_mae: 140.4482 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 13.3732 - mae: 81.9176\n",
            "Epoch 00002: val_loss improved from 27.14511 to 5.73965, saving model to /home/jupyter/IEEE_Train/fold8.h5\n",
            "42/42 [==============================] - 5s 112ms/step - loss: 13.3732 - mae: 81.9176 - val_loss: 5.7397 - val_mae: 33.5477 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.5907 - mae: 22.8246\n",
            "Epoch 00003: val_loss improved from 5.73965 to 4.35524, saving model to /home/jupyter/IEEE_Train/fold8.h5\n",
            "42/42 [==============================] - 4s 107ms/step - loss: 4.5907 - mae: 22.8246 - val_loss: 4.3552 - val_mae: 20.6088 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.2778 - mae: 19.5337\n",
            "Epoch 00004: val_loss improved from 4.35524 to 4.17810, saving model to /home/jupyter/IEEE_Train/fold8.h5\n",
            "42/42 [==============================] - 5s 109ms/step - loss: 4.2778 - mae: 19.5337 - val_loss: 4.1781 - val_mae: 19.2386 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.1248 - mae: 18.1950\n",
            "Epoch 00005: val_loss improved from 4.17810 to 3.98471, saving model to /home/jupyter/IEEE_Train/fold8.h5\n",
            "42/42 [==============================] - 4s 107ms/step - loss: 4.1248 - mae: 18.1950 - val_loss: 3.9847 - val_mae: 17.0592 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.9351 - mae: 16.1923\n",
            "Epoch 00006: val_loss improved from 3.98471 to 3.82911, saving model to /home/jupyter/IEEE_Train/fold8.h5\n",
            "42/42 [==============================] - 4s 107ms/step - loss: 3.9351 - mae: 16.1923 - val_loss: 3.8291 - val_mae: 14.3790 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.8287 - mae: 14.1510\n",
            "Epoch 00007: val_loss did not improve from 3.82911\n",
            "42/42 [==============================] - 4s 103ms/step - loss: 3.8287 - mae: 14.1510 - val_loss: 3.9042 - val_mae: 13.2042 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.6886 - mae: 12.9939\n",
            "Epoch 00008: val_loss improved from 3.82911 to 3.65442, saving model to /home/jupyter/IEEE_Train/fold8.h5\n",
            "42/42 [==============================] - 4s 106ms/step - loss: 3.6886 - mae: 12.9939 - val_loss: 3.6544 - val_mae: 12.3328 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.6289 - mae: 12.0164\n",
            "Epoch 00009: val_loss improved from 3.65442 to 3.56893, saving model to /home/jupyter/IEEE_Train/fold8.h5\n",
            "42/42 [==============================] - 4s 106ms/step - loss: 3.6289 - mae: 12.0164 - val_loss: 3.5689 - val_mae: 11.4179 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.4874 - mae: 10.2989\n",
            "Epoch 00010: val_loss did not improve from 3.56893\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.4874 - mae: 10.2989 - val_loss: 3.6066 - val_mae: 11.7802 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.4259 - mae: 9.7724\n",
            "Epoch 00011: val_loss improved from 3.56893 to 3.49516, saving model to /home/jupyter/IEEE_Train/fold8.h5\n",
            "42/42 [==============================] - 4s 106ms/step - loss: 3.4259 - mae: 9.7724 - val_loss: 3.4952 - val_mae: 10.1704 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.3367 - mae: 8.6204\n",
            "Epoch 00012: val_loss improved from 3.49516 to 3.44687, saving model to /home/jupyter/IEEE_Train/fold8.h5\n",
            "42/42 [==============================] - 4s 107ms/step - loss: 3.3367 - mae: 8.6204 - val_loss: 3.4469 - val_mae: 9.2057 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.1957 - mae: 7.9384\n",
            "Epoch 00013: val_loss improved from 3.44687 to 3.29983, saving model to /home/jupyter/IEEE_Train/fold8.h5\n",
            "42/42 [==============================] - 4s 106ms/step - loss: 3.1957 - mae: 7.9384 - val_loss: 3.2998 - val_mae: 8.4892 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.2506 - mae: 7.3790\n",
            "Epoch 00014: val_loss did not improve from 3.29983\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 3.2506 - mae: 7.3790 - val_loss: 3.4243 - val_mae: 8.3760 - lr: 0.0010\n",
            "Epoch 15/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.0062 - mae: 6.2761\n",
            "Epoch 00015: val_loss did not improve from 3.29983\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.0062 - mae: 6.2761 - val_loss: 3.3432 - val_mae: 7.1396 - lr: 0.0010\n",
            "Epoch 16/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.8134 - mae: 5.2290\n",
            "Epoch 00016: val_loss improved from 3.29983 to 3.25678, saving model to /home/jupyter/IEEE_Train/fold8.h5\n",
            "42/42 [==============================] - 4s 104ms/step - loss: 2.8134 - mae: 5.2290 - val_loss: 3.2568 - val_mae: 6.4975 - lr: 5.0000e-04\n",
            "Epoch 17/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.6891 - mae: 4.6762\n",
            "Epoch 00017: val_loss improved from 3.25678 to 3.12753, saving model to /home/jupyter/IEEE_Train/fold8.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.6891 - mae: 4.6762 - val_loss: 3.1275 - val_mae: 5.7207 - lr: 5.0000e-04\n",
            "Epoch 18/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7055 - mae: 4.5694\n",
            "Epoch 00018: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.7055 - mae: 4.5694 - val_loss: 3.2441 - val_mae: 5.7768 - lr: 5.0000e-04\n",
            "Epoch 19/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.6783 - mae: 4.1388\n",
            "Epoch 00019: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.6783 - mae: 4.1388 - val_loss: 3.5558 - val_mae: 5.9340 - lr: 5.0000e-04\n",
            "Epoch 20/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.4229 - mae: 3.6220\n",
            "Epoch 00020: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.4229 - mae: 3.6220 - val_loss: 3.4615 - val_mae: 5.6184 - lr: 2.5000e-04\n",
            "Epoch 21/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.2785 - mae: 3.1195\n",
            "Epoch 00021: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.2785 - mae: 3.1195 - val_loss: 3.2323 - val_mae: 4.5377 - lr: 2.5000e-04\n",
            "Epoch 22/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.1416 - mae: 2.6479\n",
            "Epoch 00022: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.1416 - mae: 2.6479 - val_loss: 3.5177 - val_mae: 4.7531 - lr: 1.2500e-04\n",
            "Epoch 23/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0264 - mae: 2.4051\n",
            "Epoch 00023: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.0264 - mae: 2.4051 - val_loss: 3.2870 - val_mae: 4.2780 - lr: 1.2500e-04\n",
            "Epoch 24/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9564 - mae: 2.2733\n",
            "Epoch 00024: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 1.9564 - mae: 2.2733 - val_loss: 3.8052 - val_mae: 4.9812 - lr: 6.2500e-05\n",
            "Epoch 25/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8957 - mae: 2.1033\n",
            "Epoch 00025: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 1.8957 - mae: 2.1033 - val_loss: 3.7078 - val_mae: 4.4236 - lr: 6.2500e-05\n",
            "Epoch 26/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8680 - mae: 2.0575\n",
            "Epoch 00026: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 1.8680 - mae: 2.0575 - val_loss: 3.7614 - val_mae: 4.6374 - lr: 3.1250e-05\n",
            "Epoch 27/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8758 - mae: 1.9735\n",
            "Epoch 00027: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.8758 - mae: 1.9735 - val_loss: 3.8048 - val_mae: 4.3931 - lr: 3.1250e-05\n",
            "Epoch 28/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7785 - mae: 1.9180\n",
            "Epoch 00028: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.7785 - mae: 1.9180 - val_loss: 3.8799 - val_mae: 4.3809 - lr: 1.5625e-05\n",
            "Epoch 29/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8174 - mae: 1.8906\n",
            "Epoch 00029: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 1.8174 - mae: 1.8906 - val_loss: 3.8643 - val_mae: 4.3215 - lr: 1.5625e-05\n",
            "Epoch 30/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7681 - mae: 1.8825\n",
            "Epoch 00030: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7681 - mae: 1.8825 - val_loss: 3.8327 - val_mae: 4.1890 - lr: 7.8125e-06\n",
            "Epoch 31/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7471 - mae: 1.8898\n",
            "Epoch 00031: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7471 - mae: 1.8898 - val_loss: 3.7875 - val_mae: 4.0966 - lr: 7.8125e-06\n",
            "Epoch 32/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7623 - mae: 1.8906\n",
            "Epoch 00032: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7623 - mae: 1.8906 - val_loss: 3.8797 - val_mae: 4.3457 - lr: 3.9063e-06\n",
            "Epoch 33/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7504 - mae: 1.8749\n",
            "Epoch 00033: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.7504 - mae: 1.8749 - val_loss: 3.9399 - val_mae: 4.1017 - lr: 3.9063e-06\n",
            "Epoch 34/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7691 - mae: 1.8856\n",
            "Epoch 00034: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.7691 - mae: 1.8856 - val_loss: 4.1062 - val_mae: 4.7518 - lr: 1.9531e-06\n",
            "Epoch 35/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7802 - mae: 1.8858\n",
            "Epoch 00035: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7802 - mae: 1.8858 - val_loss: 4.0437 - val_mae: 4.3610 - lr: 1.9531e-06\n",
            "Epoch 36/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7728 - mae: 1.8578\n",
            "Epoch 00036: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7728 - mae: 1.8578 - val_loss: 4.0741 - val_mae: 4.4656 - lr: 9.7656e-07\n",
            "Epoch 37/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7761 - mae: 1.8735\n",
            "Epoch 00037: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7761 - mae: 1.8735 - val_loss: 3.8510 - val_mae: 4.1371 - lr: 9.7656e-07\n",
            "Epoch 38/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7632 - mae: 1.8099\n",
            "Epoch 00038: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.7632 - mae: 1.8099 - val_loss: 3.8280 - val_mae: 4.1558 - lr: 4.8828e-07\n",
            "Epoch 39/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7714 - mae: 1.8449\n",
            "Epoch 00039: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7714 - mae: 1.8449 - val_loss: 3.9257 - val_mae: 4.2512 - lr: 4.8828e-07\n",
            "Epoch 40/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7510 - mae: 1.8478\n",
            "Epoch 00040: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 1.7510 - mae: 1.8478 - val_loss: 3.8077 - val_mae: 4.3791 - lr: 2.4414e-07\n",
            "Epoch 41/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7139 - mae: 1.7373\n",
            "Epoch 00041: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.7139 - mae: 1.7373 - val_loss: 3.9407 - val_mae: 4.2067 - lr: 2.4414e-07\n",
            "Epoch 42/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7914 - mae: 1.8158\n",
            "Epoch 00042: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 1.7914 - mae: 1.8158 - val_loss: 3.9640 - val_mae: 4.2985 - lr: 1.2207e-07\n",
            "Epoch 43/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.6937 - mae: 1.8293\n",
            "Epoch 00043: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 1.6937 - mae: 1.8293 - val_loss: 3.8451 - val_mae: 4.1425 - lr: 1.2207e-07\n",
            "Epoch 44/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7456 - mae: 1.8108\n",
            "Epoch 00044: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 1.7456 - mae: 1.8108 - val_loss: 3.9846 - val_mae: 4.3292 - lr: 6.1035e-08\n",
            "Epoch 45/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7273 - mae: 1.6862\n",
            "Epoch 00045: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 1.7273 - mae: 1.6862 - val_loss: 3.9787 - val_mae: 4.1959 - lr: 6.1035e-08\n",
            "Epoch 46/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7254 - mae: 1.8089\n",
            "Epoch 00046: val_loss did not improve from 3.12753\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 1.7254 - mae: 1.8089 - val_loss: 3.9339 - val_mae: 4.3673 - lr: 3.0518e-08\n",
            "Epoch 47/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7609 - mae: 1.8700Restoring model weights from the end of the best epoch: 17.\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 3.12753\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 1.7609 - mae: 1.8700 - val_loss: 3.9193 - val_mae: 4.2442 - lr: 3.0518e-08\n",
            "Epoch 00047: early stopping\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 3.0393 - mae: 5.8603\n",
            "Fit model on training data fold  9\n",
            "Epoch 1/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 101.2698 - mae: 93.5621\n",
            "Epoch 00001: val_loss improved from inf to 6.54644, saving model to /home/jupyter/IEEE_Train/fold9.h5\n",
            "42/42 [==============================] - 17s 164ms/step - loss: 101.2698 - mae: 93.5621 - val_loss: 6.5464 - val_mae: 15.6605 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 5.1686 - mae: 13.3768\n",
            "Epoch 00002: val_loss improved from 6.54644 to 3.98672, saving model to /home/jupyter/IEEE_Train/fold9.h5\n",
            "42/42 [==============================] - 4s 105ms/step - loss: 5.1711 - mae: 13.3850 - val_loss: 3.9867 - val_mae: 10.4567 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0829 - mae: 10.5984\n",
            "Epoch 00003: val_loss did not improve from 3.98672\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 4.0860 - mae: 10.6139 - val_loss: 4.7386 - val_mae: 13.2182 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.8029 - mae: 9.8893\n",
            "Epoch 00004: val_loss did not improve from 3.98672\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.8059 - mae: 9.9059 - val_loss: 4.0137 - val_mae: 10.8805 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.4708 - mae: 8.2378\n",
            "Epoch 00005: val_loss improved from 3.98672 to 3.47899, saving model to /home/jupyter/IEEE_Train/fold9.h5\n",
            "42/42 [==============================] - 4s 105ms/step - loss: 3.4711 - mae: 8.2520 - val_loss: 3.4790 - val_mae: 8.8324 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.3098 - mae: 8.0883\n",
            "Epoch 00006: val_loss did not improve from 3.47899\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 3.3095 - mae: 8.0876 - val_loss: 3.6172 - val_mae: 8.9047 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.2474 - mae: 7.4881\n",
            "Epoch 00007: val_loss did not improve from 3.47899\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 3.2477 - mae: 7.4892 - val_loss: 4.5045 - val_mae: 12.8458 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.1953 - mae: 7.1967\n",
            "Epoch 00008: val_loss improved from 3.47899 to 3.36608, saving model to /home/jupyter/IEEE_Train/fold9.h5\n",
            "42/42 [==============================] - 4s 103ms/step - loss: 3.1970 - mae: 7.2116 - val_loss: 3.3661 - val_mae: 7.7593 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.1123 - mae: 6.7935\n",
            "Epoch 00009: val_loss did not improve from 3.36608\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.1127 - mae: 6.7995 - val_loss: 3.3724 - val_mae: 8.5168 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.0655 - mae: 6.6721\n",
            "Epoch 00010: val_loss did not improve from 3.36608\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 3.0720 - mae: 6.6975 - val_loss: 3.4063 - val_mae: 8.0312 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.0638 - mae: 6.7611\n",
            "Epoch 00011: val_loss improved from 3.36608 to 3.36446, saving model to /home/jupyter/IEEE_Train/fold9.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 3.0638 - mae: 6.7664 - val_loss: 3.3645 - val_mae: 7.7167 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.0067 - mae: 6.3972\n",
            "Epoch 00012: val_loss improved from 3.36446 to 3.08269, saving model to /home/jupyter/IEEE_Train/fold9.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.0065 - mae: 6.3952 - val_loss: 3.0827 - val_mae: 6.6283 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.0074 - mae: 6.1660\n",
            "Epoch 00013: val_loss did not improve from 3.08269\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 3.0194 - mae: 6.2092 - val_loss: 3.2448 - val_mae: 7.6943 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.9735 - mae: 6.0846\n",
            "Epoch 00014: val_loss did not improve from 3.08269\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.9734 - mae: 6.0845 - val_loss: 3.0925 - val_mae: 6.4076 - lr: 1.2500e-04\n",
            "Epoch 15/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.9286 - mae: 5.9206\n",
            "Epoch 00015: val_loss improved from 3.08269 to 3.04034, saving model to /home/jupyter/IEEE_Train/fold9.h5\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.9293 - mae: 5.9199 - val_loss: 3.0403 - val_mae: 6.5986 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.9296 - mae: 6.0061\n",
            "Epoch 00016: val_loss did not improve from 3.04034\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.9295 - mae: 6.0034 - val_loss: 3.0875 - val_mae: 6.2755 - lr: 6.2500e-05\n",
            "Epoch 17/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.9079 - mae: 6.0518\n",
            "Epoch 00017: val_loss did not improve from 3.04034\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.9116 - mae: 6.0614 - val_loss: 3.1387 - val_mae: 6.6711 - lr: 6.2500e-05\n",
            "Epoch 18/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.9095 - mae: 5.8101\n",
            "Epoch 00018: val_loss did not improve from 3.04034\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.9095 - mae: 5.8317 - val_loss: 3.0974 - val_mae: 6.5160 - lr: 3.1250e-05\n",
            "Epoch 19/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8785 - mae: 5.8912\n",
            "Epoch 00019: val_loss did not improve from 3.04034\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8812 - mae: 5.9073 - val_loss: 3.0479 - val_mae: 6.8803 - lr: 3.1250e-05\n",
            "Epoch 20/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8728 - mae: 5.8097\n",
            "Epoch 00020: val_loss did not improve from 3.04034\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8739 - mae: 5.8095 - val_loss: 3.0819 - val_mae: 6.6441 - lr: 1.5625e-05\n",
            "Epoch 21/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8706 - mae: 5.7849\n",
            "Epoch 00021: val_loss did not improve from 3.04034\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8704 - mae: 5.7838 - val_loss: 3.0458 - val_mae: 7.2976 - lr: 1.5625e-05\n",
            "Epoch 22/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8990 - mae: 5.8353\n",
            "Epoch 00022: val_loss improved from 3.04034 to 3.01449, saving model to /home/jupyter/IEEE_Train/fold9.h5\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.9019 - mae: 5.8471 - val_loss: 3.0145 - val_mae: 6.3188 - lr: 7.8125e-06\n",
            "Epoch 23/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8569 - mae: 5.7993\n",
            "Epoch 00023: val_loss did not improve from 3.01449\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8569 - mae: 5.8048 - val_loss: 3.0579 - val_mae: 6.8922 - lr: 7.8125e-06\n",
            "Epoch 24/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8530 - mae: 5.7220\n",
            "Epoch 00024: val_loss did not improve from 3.01449\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8552 - mae: 5.7302 - val_loss: 3.1136 - val_mae: 6.5482 - lr: 7.8125e-06\n",
            "Epoch 25/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8713 - mae: 5.6763\n",
            "Epoch 00025: val_loss did not improve from 3.01449\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8712 - mae: 5.6739 - val_loss: 3.0611 - val_mae: 6.5635 - lr: 3.9063e-06\n",
            "Epoch 26/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8575 - mae: 5.7000\n",
            "Epoch 00026: val_loss did not improve from 3.01449\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8595 - mae: 5.7071 - val_loss: 3.0273 - val_mae: 6.5348 - lr: 3.9063e-06\n",
            "Epoch 27/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8571 - mae: 5.6641\n",
            "Epoch 00027: val_loss did not improve from 3.01449\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8583 - mae: 5.6842 - val_loss: 3.1495 - val_mae: 6.6948 - lr: 1.9531e-06\n",
            "Epoch 28/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8422 - mae: 5.6017\n",
            "Epoch 00028: val_loss improved from 3.01449 to 3.01290, saving model to /home/jupyter/IEEE_Train/fold9.h5\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.8447 - mae: 5.6183 - val_loss: 3.0129 - val_mae: 6.7484 - lr: 1.9531e-06\n",
            "Epoch 29/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8785 - mae: 5.7495\n",
            "Epoch 00029: val_loss improved from 3.01290 to 2.98952, saving model to /home/jupyter/IEEE_Train/fold9.h5\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.8811 - mae: 5.7617 - val_loss: 2.9895 - val_mae: 6.1119 - lr: 1.9531e-06\n",
            "Epoch 30/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8584 - mae: 5.8871\n",
            "Epoch 00030: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8584 - mae: 5.8904 - val_loss: 3.1094 - val_mae: 6.5130 - lr: 1.9531e-06\n",
            "Epoch 31/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8751 - mae: 5.7621\n",
            "Epoch 00031: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8749 - mae: 5.7664 - val_loss: 3.0552 - val_mae: 5.8830 - lr: 1.9531e-06\n",
            "Epoch 32/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8565 - mae: 5.7992\n",
            "Epoch 00032: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8565 - mae: 5.7957 - val_loss: 3.1012 - val_mae: 6.7083 - lr: 9.7656e-07\n",
            "Epoch 33/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8529 - mae: 5.6532\n",
            "Epoch 00033: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8549 - mae: 5.6576 - val_loss: 3.0581 - val_mae: 6.8364 - lr: 9.7656e-07\n",
            "Epoch 34/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8388 - mae: 5.5379\n",
            "Epoch 00034: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8387 - mae: 5.5381 - val_loss: 3.0577 - val_mae: 6.3497 - lr: 4.8828e-07\n",
            "Epoch 35/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8355 - mae: 5.5241\n",
            "Epoch 00035: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8387 - mae: 5.5373 - val_loss: 3.0161 - val_mae: 6.1764 - lr: 4.8828e-07\n",
            "Epoch 36/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8439 - mae: 5.7279\n",
            "Epoch 00036: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8448 - mae: 5.7235 - val_loss: 3.0466 - val_mae: 6.3244 - lr: 2.4414e-07\n",
            "Epoch 37/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8585 - mae: 5.5684\n",
            "Epoch 00037: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8587 - mae: 5.5729 - val_loss: 3.1098 - val_mae: 6.2361 - lr: 2.4414e-07\n",
            "Epoch 38/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8710 - mae: 5.6599\n",
            "Epoch 00038: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8723 - mae: 5.6568 - val_loss: 3.0155 - val_mae: 6.6058 - lr: 1.2207e-07\n",
            "Epoch 39/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8645 - mae: 5.7518\n",
            "Epoch 00039: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8673 - mae: 5.7643 - val_loss: 3.0094 - val_mae: 5.4996 - lr: 1.2207e-07\n",
            "Epoch 40/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8579 - mae: 5.5102\n",
            "Epoch 00040: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8581 - mae: 5.5133 - val_loss: 3.0942 - val_mae: 6.4575 - lr: 6.1035e-08\n",
            "Epoch 41/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8326 - mae: 5.4899\n",
            "Epoch 00041: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8338 - mae: 5.4897 - val_loss: 3.0813 - val_mae: 6.1408 - lr: 6.1035e-08\n",
            "Epoch 42/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8447 - mae: 5.5663\n",
            "Epoch 00042: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8450 - mae: 5.5622 - val_loss: 3.0209 - val_mae: 6.3878 - lr: 3.0518e-08\n",
            "Epoch 43/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8496 - mae: 5.6811\n",
            "Epoch 00043: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8526 - mae: 5.6945 - val_loss: 2.9915 - val_mae: 5.6099 - lr: 3.0518e-08\n",
            "Epoch 44/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8463 - mae: 5.5794\n",
            "Epoch 00044: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8465 - mae: 5.5855 - val_loss: 3.0823 - val_mae: 6.3422 - lr: 1.5259e-08\n",
            "Epoch 45/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8424 - mae: 5.8562\n",
            "Epoch 00045: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8429 - mae: 5.8581 - val_loss: 3.0524 - val_mae: 6.2702 - lr: 1.5259e-08\n",
            "Epoch 46/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8572 - mae: 5.7234\n",
            "Epoch 00046: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8584 - mae: 5.7276 - val_loss: 3.0444 - val_mae: 6.3135 - lr: 7.6294e-09\n",
            "Epoch 47/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8588 - mae: 5.5566\n",
            "Epoch 00047: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8591 - mae: 5.5632 - val_loss: 3.0074 - val_mae: 6.0638 - lr: 7.6294e-09\n",
            "Epoch 48/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8692 - mae: 5.6211\n",
            "Epoch 00048: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8703 - mae: 5.6252 - val_loss: 3.0412 - val_mae: 6.3998 - lr: 3.8147e-09\n",
            "Epoch 49/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8433 - mae: 5.6053\n",
            "Epoch 00049: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8460 - mae: 5.6178 - val_loss: 3.0392 - val_mae: 5.9121 - lr: 3.8147e-09\n",
            "Epoch 50/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8586 - mae: 5.6309\n",
            "Epoch 00050: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8601 - mae: 5.6313 - val_loss: 3.0396 - val_mae: 6.6029 - lr: 1.9073e-09\n",
            "Epoch 51/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8723 - mae: 5.4369\n",
            "Epoch 00051: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8812 - mae: 5.4657 - val_loss: 3.0359 - val_mae: 5.9578 - lr: 1.9073e-09\n",
            "Epoch 52/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8531 - mae: 5.8927\n",
            "Epoch 00052: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8538 - mae: 5.8947 - val_loss: 3.0880 - val_mae: 6.6321 - lr: 9.5367e-10\n",
            "Epoch 53/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8290 - mae: 5.4619\n",
            "Epoch 00053: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00053: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8289 - mae: 5.4672 - val_loss: 3.0211 - val_mae: 6.7028 - lr: 9.5367e-10\n",
            "Epoch 54/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8538 - mae: 5.6783\n",
            "Epoch 00054: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8580 - mae: 5.7018 - val_loss: 3.0432 - val_mae: 6.2886 - lr: 4.7684e-10\n",
            "Epoch 55/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8589 - mae: 5.8446\n",
            "Epoch 00055: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.8595 - mae: 5.8565 - val_loss: 3.0923 - val_mae: 6.4057 - lr: 4.7684e-10\n",
            "Epoch 56/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8509 - mae: 5.5050\n",
            "Epoch 00056: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8565 - mae: 5.5331 - val_loss: 3.0172 - val_mae: 6.2007 - lr: 2.3842e-10\n",
            "Epoch 57/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8430 - mae: 5.4533\n",
            "Epoch 00057: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.8428 - mae: 5.4536 - val_loss: 3.1484 - val_mae: 6.8086 - lr: 2.3842e-10\n",
            "Epoch 58/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8465 - mae: 5.7959\n",
            "Epoch 00058: val_loss did not improve from 2.98952\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8465 - mae: 5.8020 - val_loss: 3.0357 - val_mae: 6.7582 - lr: 1.1921e-10\n",
            "Epoch 59/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8488 - mae: 5.8278Restoring model weights from the end of the best epoch: 29.\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 2.98952\n",
            "\n",
            "Epoch 00059: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.8486 - mae: 5.8285 - val_loss: 3.0964 - val_mae: 6.3166 - lr: 1.1921e-10\n",
            "Epoch 00059: early stopping\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 3.0920 - mae: 6.6417\n",
            "Fit model on training data fold  10\n",
            "Epoch 1/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 64.5344 - mae: 71.8888\n",
            "Epoch 00001: val_loss improved from inf to 5.58570, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 17s 168ms/step - loss: 64.4919 - mae: 71.8647 - val_loss: 5.5857 - val_mae: 17.2306 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 5.3967 - mae: 17.1229\n",
            "Epoch 00002: val_loss improved from 5.58570 to 5.34105, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 5s 110ms/step - loss: 5.3951 - mae: 17.1110 - val_loss: 5.3411 - val_mae: 14.1372 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.5871 - mae: 13.8539\n",
            "Epoch 00003: val_loss improved from 5.34105 to 4.63794, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 107ms/step - loss: 4.5874 - mae: 13.8633 - val_loss: 4.6379 - val_mae: 13.7660 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.1805 - mae: 12.3680\n",
            "Epoch 00004: val_loss improved from 4.63794 to 3.85041, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 5s 108ms/step - loss: 4.1817 - mae: 12.3755 - val_loss: 3.8504 - val_mae: 11.3038 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.8515 - mae: 10.7482\n",
            "Epoch 00005: val_loss did not improve from 3.85041\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.8572 - mae: 10.7763 - val_loss: 5.0186 - val_mae: 16.9578 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.7286 - mae: 10.6529\n",
            "Epoch 00006: val_loss improved from 3.85041 to 3.52022, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 105ms/step - loss: 3.7282 - mae: 10.6549 - val_loss: 3.5202 - val_mae: 9.8989 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.5176 - mae: 9.2885\n",
            "Epoch 00007: val_loss did not improve from 3.52022\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 3.5184 - mae: 9.2879 - val_loss: 3.8521 - val_mae: 11.7026 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.6619 - mae: 10.2735\n",
            "Epoch 00008: val_loss improved from 3.52022 to 3.47652, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 105ms/step - loss: 3.6619 - mae: 10.2707 - val_loss: 3.4765 - val_mae: 8.8932 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 3.4103 - mae: 8.8189\n",
            "Epoch 00009: val_loss did not improve from 3.47652\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.4103 - mae: 8.8189 - val_loss: 4.8752 - val_mae: 16.5010 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.3502 - mae: 8.2791\n",
            "Epoch 00010: val_loss did not improve from 3.47652\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.3499 - mae: 8.2773 - val_loss: 3.7888 - val_mae: 10.0102 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.1992 - mae: 7.6550\n",
            "Epoch 00011: val_loss improved from 3.47652 to 3.27918, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.1992 - mae: 7.6541 - val_loss: 3.2792 - val_mae: 8.0448 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.1157 - mae: 7.3710\n",
            "Epoch 00012: val_loss did not improve from 3.27918\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.1180 - mae: 7.3769 - val_loss: 3.6819 - val_mae: 9.3729 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.1167 - mae: 7.2539\n",
            "Epoch 00013: val_loss improved from 3.27918 to 3.19033, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 3.1167 - mae: 7.2517 - val_loss: 3.1903 - val_mae: 6.9333 - lr: 5.0000e-04\n",
            "Epoch 14/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.0892 - mae: 6.8555\n",
            "Epoch 00014: val_loss did not improve from 3.19033\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 3.0909 - mae: 6.8620 - val_loss: 3.5703 - val_mae: 8.3016 - lr: 5.0000e-04\n",
            "Epoch 15/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 3.0035 - mae: 6.3993\n",
            "Epoch 00015: val_loss did not improve from 3.19033\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 3.0041 - mae: 6.3983 - val_loss: 4.5735 - val_mae: 11.3783 - lr: 5.0000e-04\n",
            "Epoch 16/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.9742 - mae: 6.1922\n",
            "Epoch 00016: val_loss did not improve from 3.19033\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.9744 - mae: 6.1890 - val_loss: 3.2717 - val_mae: 7.0193 - lr: 2.5000e-04\n",
            "Epoch 17/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.9395 - mae: 5.8376\n",
            "Epoch 00017: val_loss improved from 3.19033 to 3.15511, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.9450 - mae: 5.8565 - val_loss: 3.1551 - val_mae: 6.7826 - lr: 2.5000e-04\n",
            "Epoch 18/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8907 - mae: 5.8854\n",
            "Epoch 00018: val_loss improved from 3.15511 to 3.01310, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.8906 - mae: 5.8820 - val_loss: 3.0131 - val_mae: 6.1361 - lr: 2.5000e-04\n",
            "Epoch 19/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8841 - mae: 5.8419\n",
            "Epoch 00019: val_loss did not improve from 3.01310\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.8842 - mae: 5.8427 - val_loss: 3.1584 - val_mae: 6.7536 - lr: 2.5000e-04\n",
            "Epoch 20/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.8247 - mae: 5.4829\n",
            "Epoch 00020: val_loss improved from 3.01310 to 2.81146, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 98ms/step - loss: 2.8246 - mae: 5.4872 - val_loss: 2.8115 - val_mae: 5.3443 - lr: 2.5000e-04\n",
            "Epoch 21/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.7680 - mae: 5.3607\n",
            "Epoch 00021: val_loss improved from 2.81146 to 2.80560, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.7684 - mae: 5.3568 - val_loss: 2.8056 - val_mae: 5.2685 - lr: 2.5000e-04\n",
            "Epoch 22/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.7356 - mae: 5.2193\n",
            "Epoch 00022: val_loss did not improve from 2.80560\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.7357 - mae: 5.2170 - val_loss: 2.9059 - val_mae: 5.4271 - lr: 2.5000e-04\n",
            "Epoch 23/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.6936 - mae: 4.8989\n",
            "Epoch 00023: val_loss did not improve from 2.80560\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.6988 - mae: 4.9140 - val_loss: 4.4654 - val_mae: 9.1151 - lr: 2.5000e-04\n",
            "Epoch 24/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.7491 - mae: 4.9817\n",
            "Epoch 00024: val_loss improved from 2.80560 to 2.79643, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.7492 - mae: 4.9786 - val_loss: 2.7964 - val_mae: 5.3203 - lr: 1.2500e-04\n",
            "Epoch 25/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.6272 - mae: 4.5883\n",
            "Epoch 00025: val_loss did not improve from 2.79643\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.6285 - mae: 4.5940 - val_loss: 2.8087 - val_mae: 4.9977 - lr: 1.2500e-04\n",
            "Epoch 26/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.6118 - mae: 4.5004\n",
            "Epoch 00026: val_loss improved from 2.79643 to 2.63557, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.6118 - mae: 4.5016 - val_loss: 2.6356 - val_mae: 4.3708 - lr: 1.2500e-04\n",
            "Epoch 27/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.5527 - mae: 4.1948\n",
            "Epoch 00027: val_loss did not improve from 2.63557\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.5554 - mae: 4.1945 - val_loss: 2.9963 - val_mae: 5.4277 - lr: 1.2500e-04\n",
            "Epoch 28/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.5544 - mae: 4.1720\n",
            "Epoch 00028: val_loss improved from 2.63557 to 2.60078, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.5544 - mae: 4.1707 - val_loss: 2.6008 - val_mae: 4.3317 - lr: 1.2500e-04\n",
            "Epoch 29/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.5222 - mae: 4.0000\n",
            "Epoch 00029: val_loss did not improve from 2.60078\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.5283 - mae: 4.0155 - val_loss: 2.9507 - val_mae: 5.2841 - lr: 1.2500e-04\n",
            "Epoch 30/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.5543 - mae: 4.2091\n",
            "Epoch 00030: val_loss did not improve from 2.60078\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.5682 - mae: 4.2430 - val_loss: 2.8418 - val_mae: 4.6614 - lr: 1.2500e-04\n",
            "Epoch 31/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.4913 - mae: 3.9907\n",
            "Epoch 00031: val_loss improved from 2.60078 to 2.59100, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 2.4934 - mae: 3.9927 - val_loss: 2.5910 - val_mae: 4.4941 - lr: 6.2500e-05\n",
            "Epoch 32/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.4640 - mae: 3.9287\n",
            "Epoch 00032: val_loss did not improve from 2.59100\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.4641 - mae: 3.9280 - val_loss: 2.5995 - val_mae: 4.0949 - lr: 6.2500e-05\n",
            "Epoch 33/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.4343 - mae: 3.7445\n",
            "Epoch 00033: val_loss improved from 2.59100 to 2.57879, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.4345 - mae: 3.7468 - val_loss: 2.5788 - val_mae: 4.0675 - lr: 6.2500e-05\n",
            "Epoch 34/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.4129 - mae: 3.6297\n",
            "Epoch 00034: val_loss did not improve from 2.57879\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.4286 - mae: 3.6685 - val_loss: 2.7031 - val_mae: 4.2020 - lr: 6.2500e-05\n",
            "Epoch 35/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.4232 - mae: 3.8540\n",
            "Epoch 00035: val_loss did not improve from 2.57879\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.4332 - mae: 3.8766 - val_loss: 2.5853 - val_mae: 3.8631 - lr: 6.2500e-05\n",
            "Epoch 36/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.4285 - mae: 3.5571\n",
            "Epoch 00036: val_loss did not improve from 2.57879\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.4365 - mae: 3.5773 - val_loss: 2.6189 - val_mae: 4.0948 - lr: 3.1250e-05\n",
            "Epoch 37/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.4103 - mae: 3.5647\n",
            "Epoch 00037: val_loss improved from 2.57879 to 2.53843, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.4110 - mae: 3.5641 - val_loss: 2.5384 - val_mae: 3.8929 - lr: 3.1250e-05\n",
            "Epoch 38/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3876 - mae: 3.5624\n",
            "Epoch 00038: val_loss improved from 2.53843 to 2.49392, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.3876 - mae: 3.5601 - val_loss: 2.4939 - val_mae: 4.1436 - lr: 3.1250e-05\n",
            "Epoch 39/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3668 - mae: 3.6660\n",
            "Epoch 00039: val_loss did not improve from 2.49392\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.3686 - mae: 3.6733 - val_loss: 2.5134 - val_mae: 3.8180 - lr: 3.1250e-05\n",
            "Epoch 40/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3769 - mae: 3.4838\n",
            "Epoch 00040: val_loss improved from 2.49392 to 2.45012, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 2.3774 - mae: 3.4851 - val_loss: 2.4501 - val_mae: 3.6625 - lr: 3.1250e-05\n",
            "Epoch 41/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3486 - mae: 3.4754\n",
            "Epoch 00041: val_loss did not improve from 2.45012\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.3535 - mae: 3.4890 - val_loss: 2.5004 - val_mae: 3.7120 - lr: 3.1250e-05\n",
            "Epoch 42/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3433 - mae: 3.3672\n",
            "Epoch 00042: val_loss did not improve from 2.45012\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.3457 - mae: 3.3733 - val_loss: 2.4776 - val_mae: 3.6076 - lr: 3.1250e-05\n",
            "Epoch 43/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3358 - mae: 3.5540\n",
            "Epoch 00043: val_loss improved from 2.45012 to 2.42540, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.3386 - mae: 3.5599 - val_loss: 2.4254 - val_mae: 3.3249 - lr: 1.5625e-05\n",
            "Epoch 44/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3318 - mae: 3.3256\n",
            "Epoch 00044: val_loss did not improve from 2.42540\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.3544 - mae: 3.3745 - val_loss: 2.4484 - val_mae: 3.4418 - lr: 1.5625e-05\n",
            "Epoch 45/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3219 - mae: 3.3331\n",
            "Epoch 00045: val_loss did not improve from 2.42540\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.3220 - mae: 3.3306 - val_loss: 2.4403 - val_mae: 3.6312 - lr: 1.5625e-05\n",
            "Epoch 46/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3251 - mae: 3.3922\n",
            "Epoch 00046: val_loss improved from 2.42540 to 2.41857, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.3256 - mae: 3.3925 - val_loss: 2.4186 - val_mae: 3.4397 - lr: 7.8125e-06\n",
            "Epoch 47/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3251 - mae: 3.3005\n",
            "Epoch 00047: val_loss did not improve from 2.41857\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.3291 - mae: 3.3097 - val_loss: 2.4441 - val_mae: 3.7041 - lr: 7.8125e-06\n",
            "Epoch 48/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3229 - mae: 3.4021\n",
            "Epoch 00048: val_loss did not improve from 2.41857\n",
            "\n",
            "Epoch 00048: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.3229 - mae: 3.4018 - val_loss: 2.4620 - val_mae: 3.4163 - lr: 7.8125e-06\n",
            "Epoch 49/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3032 - mae: 3.2851\n",
            "Epoch 00049: val_loss improved from 2.41857 to 2.41711, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.3032 - mae: 3.2832 - val_loss: 2.4171 - val_mae: 3.4408 - lr: 3.9063e-06\n",
            "Epoch 50/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3071 - mae: 3.4231\n",
            "Epoch 00050: val_loss improved from 2.41711 to 2.41402, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 99ms/step - loss: 2.3147 - mae: 3.4392 - val_loss: 2.4140 - val_mae: 3.2042 - lr: 3.9063e-06\n",
            "Epoch 51/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2965 - mae: 3.4790\n",
            "Epoch 00051: val_loss did not improve from 2.41402\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.3008 - mae: 3.4841 - val_loss: 2.4516 - val_mae: 3.8925 - lr: 3.9063e-06\n",
            "Epoch 52/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3055 - mae: 3.3835\n",
            "Epoch 00052: val_loss did not improve from 2.41402\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.3093 - mae: 3.3926 - val_loss: 2.4505 - val_mae: 3.6510 - lr: 3.9063e-06\n",
            "Epoch 53/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3231 - mae: 3.3268\n",
            "Epoch 00053: val_loss did not improve from 2.41402\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.3247 - mae: 3.3262 - val_loss: 2.4353 - val_mae: 3.5120 - lr: 1.9531e-06\n",
            "Epoch 54/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3113 - mae: 3.3270\n",
            "Epoch 00054: val_loss did not improve from 2.41402\n",
            "\n",
            "Epoch 00054: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.3177 - mae: 3.3397 - val_loss: 2.4345 - val_mae: 3.9211 - lr: 1.9531e-06\n",
            "Epoch 55/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3085 - mae: 3.3702\n",
            "Epoch 00055: val_loss did not improve from 2.41402\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.3087 - mae: 3.3709 - val_loss: 2.4290 - val_mae: 3.6927 - lr: 9.7656e-07\n",
            "Epoch 56/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2852 - mae: 3.2059\n",
            "Epoch 00056: val_loss did not improve from 2.41402\n",
            "\n",
            "Epoch 00056: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.2883 - mae: 3.2082 - val_loss: 2.4532 - val_mae: 3.2519 - lr: 9.7656e-07\n",
            "Epoch 57/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2968 - mae: 3.3417\n",
            "Epoch 00057: val_loss did not improve from 2.41402\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.2968 - mae: 3.3439 - val_loss: 2.4364 - val_mae: 3.5510 - lr: 4.8828e-07\n",
            "Epoch 58/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2978 - mae: 3.2601\n",
            "Epoch 00058: val_loss improved from 2.41402 to 2.38532, saving model to /home/jupyter/IEEE_Train/fold10.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 2.2990 - mae: 3.2607 - val_loss: 2.3853 - val_mae: 3.2862 - lr: 4.8828e-07\n",
            "Epoch 59/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2997 - mae: 3.3656\n",
            "Epoch 00059: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.3004 - mae: 3.3668 - val_loss: 2.4347 - val_mae: 3.4352 - lr: 4.8828e-07\n",
            "Epoch 60/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3195 - mae: 3.3243\n",
            "Epoch 00060: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00060: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.3231 - mae: 3.3333 - val_loss: 2.4206 - val_mae: 3.4954 - lr: 4.8828e-07\n",
            "Epoch 61/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2975 - mae: 3.2470\n",
            "Epoch 00061: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.2983 - mae: 3.2461 - val_loss: 2.4426 - val_mae: 3.5542 - lr: 2.4414e-07\n",
            "Epoch 62/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3016 - mae: 3.2878\n",
            "Epoch 00062: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.3209 - mae: 3.3297 - val_loss: 2.4032 - val_mae: 3.5240 - lr: 2.4414e-07\n",
            "Epoch 63/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2992 - mae: 3.2761\n",
            "Epoch 00063: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.3071 - mae: 3.2932 - val_loss: 2.4862 - val_mae: 3.7373 - lr: 1.2207e-07\n",
            "Epoch 64/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2982 - mae: 3.2549\n",
            "Epoch 00064: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.2992 - mae: 3.2631 - val_loss: 2.4372 - val_mae: 3.3111 - lr: 1.2207e-07\n",
            "Epoch 65/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3049 - mae: 3.3498\n",
            "Epoch 00065: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.3054 - mae: 3.3514 - val_loss: 2.4570 - val_mae: 3.8788 - lr: 6.1035e-08\n",
            "Epoch 66/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3018 - mae: 3.2613\n",
            "Epoch 00066: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00066: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.3018 - mae: 3.2589 - val_loss: 2.4060 - val_mae: 3.7000 - lr: 6.1035e-08\n",
            "Epoch 67/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2987 - mae: 3.4083\n",
            "Epoch 00067: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.2987 - mae: 3.4106 - val_loss: 2.4234 - val_mae: 3.3332 - lr: 3.0518e-08\n",
            "Epoch 68/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2863 - mae: 3.2353\n",
            "Epoch 00068: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.2863 - mae: 3.2350 - val_loss: 2.4425 - val_mae: 3.3986 - lr: 3.0518e-08\n",
            "Epoch 69/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2939 - mae: 3.3912\n",
            "Epoch 00069: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.2981 - mae: 3.3979 - val_loss: 2.4604 - val_mae: 3.4952 - lr: 1.5259e-08\n",
            "Epoch 70/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2921 - mae: 3.1709\n",
            "Epoch 00070: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00070: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 2.2921 - mae: 3.1755 - val_loss: 2.4527 - val_mae: 3.7220 - lr: 1.5259e-08\n",
            "Epoch 71/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3093 - mae: 3.4366\n",
            "Epoch 00071: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.3097 - mae: 3.4458 - val_loss: 2.4173 - val_mae: 3.6206 - lr: 7.6294e-09\n",
            "Epoch 72/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2953 - mae: 3.1912\n",
            "Epoch 00072: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00072: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.3178 - mae: 3.2372 - val_loss: 2.4060 - val_mae: 3.2222 - lr: 7.6294e-09\n",
            "Epoch 73/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3033 - mae: 3.2538\n",
            "Epoch 00073: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.3043 - mae: 3.2558 - val_loss: 2.4257 - val_mae: 3.6497 - lr: 3.8147e-09\n",
            "Epoch 74/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2913 - mae: 3.3582\n",
            "Epoch 00074: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.2930 - mae: 3.3648 - val_loss: 2.4323 - val_mae: 3.6757 - lr: 3.8147e-09\n",
            "Epoch 75/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2922 - mae: 3.3823\n",
            "Epoch 00075: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.2922 - mae: 3.3829 - val_loss: 2.4583 - val_mae: 3.3431 - lr: 1.9073e-09\n",
            "Epoch 76/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3073 - mae: 3.3126\n",
            "Epoch 00076: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00076: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.3084 - mae: 3.3163 - val_loss: 2.4186 - val_mae: 3.5416 - lr: 1.9073e-09\n",
            "Epoch 77/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2875 - mae: 3.2899\n",
            "Epoch 00077: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.2884 - mae: 3.2912 - val_loss: 2.4108 - val_mae: 3.4350 - lr: 9.5367e-10\n",
            "Epoch 78/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2912 - mae: 3.3195\n",
            "Epoch 00078: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00078: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.2912 - mae: 3.3187 - val_loss: 2.4522 - val_mae: 3.7195 - lr: 9.5367e-10\n",
            "Epoch 79/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3032 - mae: 3.3462\n",
            "Epoch 00079: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.3032 - mae: 3.3478 - val_loss: 2.4155 - val_mae: 3.4808 - lr: 4.7684e-10\n",
            "Epoch 80/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2885 - mae: 3.4078\n",
            "Epoch 00080: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00080: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.2901 - mae: 3.4099 - val_loss: 2.4404 - val_mae: 3.5918 - lr: 4.7684e-10\n",
            "Epoch 81/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2986 - mae: 3.2659\n",
            "Epoch 00081: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.3017 - mae: 3.2762 - val_loss: 2.4417 - val_mae: 3.4576 - lr: 2.3842e-10\n",
            "Epoch 82/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2963 - mae: 3.3717\n",
            "Epoch 00082: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00082: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.2963 - mae: 3.3706 - val_loss: 2.4090 - val_mae: 3.4139 - lr: 2.3842e-10\n",
            "Epoch 83/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3097 - mae: 3.3309\n",
            "Epoch 00083: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 2.3276 - mae: 3.3746 - val_loss: 2.4493 - val_mae: 3.6545 - lr: 1.1921e-10\n",
            "Epoch 84/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2975 - mae: 3.3134\n",
            "Epoch 00084: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00084: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.2983 - mae: 3.3148 - val_loss: 2.4549 - val_mae: 3.4947 - lr: 1.1921e-10\n",
            "Epoch 85/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2917 - mae: 3.3028\n",
            "Epoch 00085: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.2917 - mae: 3.3007 - val_loss: 2.4062 - val_mae: 3.0427 - lr: 5.9605e-11\n",
            "Epoch 86/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2942 - mae: 3.1760\n",
            "Epoch 00086: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00086: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.2946 - mae: 3.1743 - val_loss: 2.4120 - val_mae: 3.4284 - lr: 5.9605e-11\n",
            "Epoch 87/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.2928 - mae: 3.3758\n",
            "Epoch 00087: val_loss did not improve from 2.38532\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 2.2927 - mae: 3.3772 - val_loss: 2.4634 - val_mae: 3.7281 - lr: 2.9802e-11\n",
            "Epoch 88/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 2.3128 - mae: 3.2973Restoring model weights from the end of the best epoch: 58.\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 2.38532\n",
            "\n",
            "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 2.3129 - mae: 3.2974 - val_loss: 2.4383 - val_mae: 3.5562 - lr: 2.9802e-11\n",
            "Epoch 00088: early stopping\n",
            "5/5 [==============================] - 2s 27ms/step - loss: 2.8290 - mae: 4.3797\n",
            "Fit model on training data fold  11\n",
            "Epoch 1/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 73.5233 - mae: 102.1219\n",
            "Epoch 00001: val_loss improved from inf to 6.19336, saving model to /home/jupyter/IEEE_Train/fold11.h5\n",
            "42/42 [==============================] - 17s 165ms/step - loss: 73.5233 - mae: 102.1219 - val_loss: 6.1934 - val_mae: 26.3946 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 5.2811 - mae: 22.5911\n",
            "Epoch 00002: val_loss improved from 6.19336 to 5.09177, saving model to /home/jupyter/IEEE_Train/fold11.h5\n",
            "42/42 [==============================] - 5s 113ms/step - loss: 5.2811 - mae: 22.5911 - val_loss: 5.0918 - val_mae: 23.0987 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.8074 - mae: 20.1809\n",
            "Epoch 00003: val_loss improved from 5.09177 to 4.90694, saving model to /home/jupyter/IEEE_Train/fold11.h5\n",
            "42/42 [==============================] - 5s 110ms/step - loss: 4.8066 - mae: 20.1848 - val_loss: 4.9069 - val_mae: 21.6410 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.6981 - mae: 19.8172\n",
            "Epoch 00004: val_loss improved from 4.90694 to 4.67996, saving model to /home/jupyter/IEEE_Train/fold11.h5\n",
            "42/42 [==============================] - 4s 107ms/step - loss: 4.6974 - mae: 19.8198 - val_loss: 4.6800 - val_mae: 20.5319 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.4918 - mae: 18.9299\n",
            "Epoch 00005: val_loss improved from 4.67996 to 4.18138, saving model to /home/jupyter/IEEE_Train/fold11.h5\n",
            "42/42 [==============================] - 4s 104ms/step - loss: 4.4911 - mae: 18.9294 - val_loss: 4.1814 - val_mae: 14.4438 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.8002 - mae: 19.3716\n",
            "Epoch 00006: val_loss did not improve from 4.18138\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.8025 - mae: 19.3827 - val_loss: 4.2300 - val_mae: 17.1711 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.5015 - mae: 19.1942\n",
            "Epoch 00007: val_loss improved from 4.18138 to 4.09632, saving model to /home/jupyter/IEEE_Train/fold11.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 4.5022 - mae: 19.1897 - val_loss: 4.0963 - val_mae: 15.1730 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.4268 - mae: 19.4320\n",
            "Epoch 00008: val_loss improved from 4.09632 to 4.08903, saving model to /home/jupyter/IEEE_Train/fold11.h5\n",
            "42/42 [==============================] - 4s 102ms/step - loss: 4.4276 - mae: 19.4180 - val_loss: 4.0890 - val_mae: 16.1796 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.3293 - mae: 18.7622\n",
            "Epoch 00009: val_loss did not improve from 4.08903\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.3291 - mae: 18.7748 - val_loss: 4.2684 - val_mae: 17.9587 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.5505 - mae: 18.2518\n",
            "Epoch 00010: val_loss did not improve from 4.08903\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.5500 - mae: 18.2496 - val_loss: 4.3649 - val_mae: 18.8495 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.4453 - mae: 19.6755\n",
            "Epoch 00011: val_loss did not improve from 4.08903\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.4452 - mae: 19.6690 - val_loss: 4.2959 - val_mae: 18.3734 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.4082 - mae: 19.3107\n",
            "Epoch 00012: val_loss did not improve from 4.08903\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.4078 - mae: 19.3209 - val_loss: 4.4410 - val_mae: 19.7351 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.3740 - mae: 19.9107\n",
            "Epoch 00013: val_loss improved from 4.08903 to 4.08858, saving model to /home/jupyter/IEEE_Train/fold11.h5\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 4.3741 - mae: 19.9049 - val_loss: 4.0886 - val_mae: 16.7801 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.3428 - mae: 19.5535\n",
            "Epoch 00014: val_loss did not improve from 4.08858\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.3432 - mae: 19.5746 - val_loss: 4.1029 - val_mae: 17.7782 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.3057 - mae: 19.1785\n",
            "Epoch 00015: val_loss did not improve from 4.08858\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.3052 - mae: 19.1841 - val_loss: 4.0997 - val_mae: 16.5145 - lr: 2.5000e-04\n",
            "Epoch 16/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.2804 - mae: 18.5414\n",
            "Epoch 00016: val_loss did not improve from 4.08858\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.2815 - mae: 18.5397 - val_loss: 4.1498 - val_mae: 17.4115 - lr: 1.2500e-04\n",
            "Epoch 17/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.2314 - mae: 17.8568\n",
            "Epoch 00017: val_loss did not improve from 4.08858\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.2329 - mae: 17.8514 - val_loss: 4.1160 - val_mae: 17.4791 - lr: 1.2500e-04\n",
            "Epoch 18/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.2197 - mae: 18.2339\n",
            "Epoch 00018: val_loss improved from 4.08858 to 4.05509, saving model to /home/jupyter/IEEE_Train/fold11.h5\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 4.2193 - mae: 18.2285 - val_loss: 4.0551 - val_mae: 14.9722 - lr: 6.2500e-05\n",
            "Epoch 19/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.1923 - mae: 18.0323\n",
            "Epoch 00019: val_loss did not improve from 4.05509\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.1924 - mae: 18.0293 - val_loss: 4.1064 - val_mae: 16.1769 - lr: 6.2500e-05\n",
            "Epoch 20/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.1702 - mae: 17.2947\n",
            "Epoch 00020: val_loss did not improve from 4.05509\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.1707 - mae: 17.2972 - val_loss: 4.0824 - val_mae: 17.2838 - lr: 6.2500e-05\n",
            "Epoch 21/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.1521 - mae: 17.6699\n",
            "Epoch 00021: val_loss did not improve from 4.05509\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.1522 - mae: 17.6659 - val_loss: 4.0870 - val_mae: 16.9425 - lr: 3.1250e-05\n",
            "Epoch 22/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.1179 - mae: 17.4914\n",
            "Epoch 00022: val_loss did not improve from 4.05509\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.1179 - mae: 17.4914 - val_loss: 4.1755 - val_mae: 18.1821 - lr: 3.1250e-05\n",
            "Epoch 23/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.1105 - mae: 17.0699\n",
            "Epoch 00023: val_loss improved from 4.05509 to 4.04341, saving model to /home/jupyter/IEEE_Train/fold11.h5\n",
            "42/42 [==============================] - 4s 100ms/step - loss: 4.1102 - mae: 17.0733 - val_loss: 4.0434 - val_mae: 15.7801 - lr: 1.5625e-05\n",
            "Epoch 24/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.1281 - mae: 17.0063\n",
            "Epoch 00024: val_loss did not improve from 4.04341\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.1288 - mae: 16.9994 - val_loss: 4.1580 - val_mae: 16.4978 - lr: 1.5625e-05\n",
            "Epoch 25/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0905 - mae: 16.9661\n",
            "Epoch 00025: val_loss did not improve from 4.04341\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0910 - mae: 16.9973 - val_loss: 4.0809 - val_mae: 17.4027 - lr: 1.5625e-05\n",
            "Epoch 26/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0924 - mae: 16.9475\n",
            "Epoch 00026: val_loss did not improve from 4.04341\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0925 - mae: 16.9546 - val_loss: 4.0937 - val_mae: 16.7995 - lr: 7.8125e-06\n",
            "Epoch 27/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0821 - mae: 17.0515\n",
            "Epoch 00027: val_loss improved from 4.04341 to 4.02513, saving model to /home/jupyter/IEEE_Train/fold11.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 4.0819 - mae: 17.0499 - val_loss: 4.0251 - val_mae: 15.9487 - lr: 7.8125e-06\n",
            "Epoch 28/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.1016 - mae: 16.9744\n",
            "Epoch 00028: val_loss did not improve from 4.02513\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.1015 - mae: 16.9975 - val_loss: 4.1018 - val_mae: 16.3094 - lr: 7.8125e-06\n",
            "Epoch 29/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0717 - mae: 17.0808\n",
            "Epoch 00029: val_loss did not improve from 4.02513\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.0720 - mae: 17.0772 - val_loss: 4.1094 - val_mae: 16.0771 - lr: 7.8125e-06\n",
            "Epoch 30/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0918 - mae: 16.8471\n",
            "Epoch 00030: val_loss did not improve from 4.02513\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0917 - mae: 16.8497 - val_loss: 4.0515 - val_mae: 17.1961 - lr: 3.9063e-06\n",
            "Epoch 31/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0747 - mae: 16.6601\n",
            "Epoch 00031: val_loss did not improve from 4.02513\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0744 - mae: 16.6487 - val_loss: 4.0997 - val_mae: 15.0792 - lr: 3.9063e-06\n",
            "Epoch 32/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0661 - mae: 16.6029\n",
            "Epoch 00032: val_loss did not improve from 4.02513\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 4.0659 - mae: 16.5941 - val_loss: 4.0636 - val_mae: 16.3367 - lr: 1.9531e-06\n",
            "Epoch 33/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0841 - mae: 16.5637\n",
            "Epoch 00033: val_loss did not improve from 4.02513\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 4.0845 - mae: 16.5751 - val_loss: 4.0487 - val_mae: 15.2778 - lr: 1.9531e-06\n",
            "Epoch 34/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0630 - mae: 16.4065\n",
            "Epoch 00034: val_loss did not improve from 4.02513\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0629 - mae: 16.4450 - val_loss: 4.0368 - val_mae: 16.3522 - lr: 9.7656e-07\n",
            "Epoch 35/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0558 - mae: 16.7311\n",
            "Epoch 00035: val_loss did not improve from 4.02513\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 4.0567 - mae: 16.7622 - val_loss: 4.2116 - val_mae: 17.1839 - lr: 9.7656e-07\n",
            "Epoch 36/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0554 - mae: 16.8954\n",
            "Epoch 00036: val_loss did not improve from 4.02513\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0551 - mae: 16.8896 - val_loss: 4.0554 - val_mae: 15.8864 - lr: 4.8828e-07\n",
            "Epoch 37/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0636 - mae: 16.3951\n",
            "Epoch 00037: val_loss did not improve from 4.02513\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.0635 - mae: 16.4038 - val_loss: 4.0306 - val_mae: 15.8828 - lr: 4.8828e-07\n",
            "Epoch 38/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0649 - mae: 16.5417\n",
            "Epoch 00038: val_loss did not improve from 4.02513\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 4.0654 - mae: 16.5966 - val_loss: 4.1283 - val_mae: 17.2699 - lr: 2.4414e-07\n",
            "Epoch 39/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0537 - mae: 16.0983\n",
            "Epoch 00039: val_loss did not improve from 4.02513\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 4.0566 - mae: 16.1147 - val_loss: 4.1358 - val_mae: 17.4058 - lr: 2.4414e-07\n",
            "Epoch 40/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0583 - mae: 16.3563\n",
            "Epoch 00040: val_loss did not improve from 4.02513\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 4.0585 - mae: 16.3518 - val_loss: 4.0640 - val_mae: 17.6329 - lr: 1.2207e-07\n",
            "Epoch 41/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0572 - mae: 16.0953\n",
            "Epoch 00041: val_loss did not improve from 4.02513\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0605 - mae: 16.1484 - val_loss: 4.2038 - val_mae: 17.6659 - lr: 1.2207e-07\n",
            "Epoch 42/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0537 - mae: 16.4284\n",
            "Epoch 00042: val_loss did not improve from 4.02513\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0535 - mae: 16.4316 - val_loss: 4.0338 - val_mae: 16.0330 - lr: 6.1035e-08\n",
            "Epoch 43/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0629 - mae: 16.7190\n",
            "Epoch 00043: val_loss did not improve from 4.02513\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0641 - mae: 16.7293 - val_loss: 4.1439 - val_mae: 17.5358 - lr: 6.1035e-08\n",
            "Epoch 44/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0423 - mae: 16.5966\n",
            "Epoch 00044: val_loss did not improve from 4.02513\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0420 - mae: 16.5840 - val_loss: 4.0323 - val_mae: 16.9771 - lr: 3.0518e-08\n",
            "Epoch 45/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0990 - mae: 17.2048\n",
            "Epoch 00045: val_loss did not improve from 4.02513\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.0987 - mae: 17.2112 - val_loss: 4.0349 - val_mae: 15.8787 - lr: 3.0518e-08\n",
            "Epoch 46/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0745 - mae: 16.6388\n",
            "Epoch 00046: val_loss did not improve from 4.02513\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0751 - mae: 16.6569 - val_loss: 4.0328 - val_mae: 14.6257 - lr: 1.5259e-08\n",
            "Epoch 47/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0551 - mae: 15.6833\n",
            "Epoch 00047: val_loss did not improve from 4.02513\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0548 - mae: 15.6737 - val_loss: 4.0668 - val_mae: 16.9235 - lr: 1.5259e-08\n",
            "Epoch 48/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0483 - mae: 16.0737\n",
            "Epoch 00048: val_loss improved from 4.02513 to 4.01320, saving model to /home/jupyter/IEEE_Train/fold11.h5\n",
            "42/42 [==============================] - 4s 101ms/step - loss: 4.0492 - mae: 16.0972 - val_loss: 4.0132 - val_mae: 16.0136 - lr: 7.6294e-09\n",
            "Epoch 49/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0677 - mae: 16.0633\n",
            "Epoch 00049: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0691 - mae: 16.0986 - val_loss: 4.1033 - val_mae: 16.4996 - lr: 7.6294e-09\n",
            "Epoch 50/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0597 - mae: 15.9606\n",
            "Epoch 00050: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 4.0594 - mae: 15.9546 - val_loss: 4.0536 - val_mae: 16.2109 - lr: 7.6294e-09\n",
            "Epoch 51/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0524 - mae: 16.4982\n",
            "Epoch 00051: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 4.0568 - mae: 16.5416 - val_loss: 4.1929 - val_mae: 17.2827 - lr: 3.8147e-09\n",
            "Epoch 52/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0641 - mae: 16.6581\n",
            "Epoch 00052: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0639 - mae: 16.6506 - val_loss: 4.0410 - val_mae: 15.2035 - lr: 3.8147e-09\n",
            "Epoch 53/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0612 - mae: 16.1505\n",
            "Epoch 00053: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0625 - mae: 16.1709 - val_loss: 4.1578 - val_mae: 18.9107 - lr: 1.9073e-09\n",
            "Epoch 54/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0736 - mae: 16.3860\n",
            "Epoch 00054: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00054: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0739 - mae: 16.3799 - val_loss: 4.0639 - val_mae: 16.4340 - lr: 1.9073e-09\n",
            "Epoch 55/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0522 - mae: 15.7670\n",
            "Epoch 00055: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0527 - mae: 15.7954 - val_loss: 4.1252 - val_mae: 17.2510 - lr: 9.5367e-10\n",
            "Epoch 56/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0688 - mae: 16.2793\n",
            "Epoch 00056: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00056: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
            "42/42 [==============================] - 4s 94ms/step - loss: 4.0720 - mae: 16.3102 - val_loss: 4.1905 - val_mae: 17.0520 - lr: 9.5367e-10\n",
            "Epoch 57/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0496 - mae: 16.6857\n",
            "Epoch 00057: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0517 - mae: 16.6995 - val_loss: 4.2414 - val_mae: 17.7950 - lr: 4.7684e-10\n",
            "Epoch 58/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0614 - mae: 16.6645\n",
            "Epoch 00058: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00058: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0616 - mae: 16.6747 - val_loss: 4.1521 - val_mae: 17.7942 - lr: 4.7684e-10\n",
            "Epoch 59/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0742 - mae: 16.8405\n",
            "Epoch 00059: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0741 - mae: 16.8432 - val_loss: 4.0557 - val_mae: 15.9372 - lr: 2.3842e-10\n",
            "Epoch 60/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0743 - mae: 15.9638\n",
            "Epoch 00060: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0755 - mae: 15.9755 - val_loss: 4.0284 - val_mae: 15.3632 - lr: 2.3842e-10\n",
            "Epoch 61/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0678 - mae: 16.2422\n",
            "Epoch 00061: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.0676 - mae: 16.2322 - val_loss: 4.0713 - val_mae: 17.4064 - lr: 1.1921e-10\n",
            "Epoch 62/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0782 - mae: 16.3594\n",
            "Epoch 00062: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00062: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0790 - mae: 16.3642 - val_loss: 4.0305 - val_mae: 16.2501 - lr: 1.1921e-10\n",
            "Epoch 63/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0538 - mae: 16.4567\n",
            "Epoch 00063: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0543 - mae: 16.4596 - val_loss: 4.0458 - val_mae: 17.8349 - lr: 5.9605e-11\n",
            "Epoch 64/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0692 - mae: 16.8655\n",
            "Epoch 00064: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0707 - mae: 16.8821 - val_loss: 4.1195 - val_mae: 17.2979 - lr: 5.9605e-11\n",
            "Epoch 65/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0668 - mae: 16.3623\n",
            "Epoch 00065: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.0684 - mae: 16.3835 - val_loss: 4.1345 - val_mae: 16.8893 - lr: 2.9802e-11\n",
            "Epoch 66/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0467 - mae: 16.6696\n",
            "Epoch 00066: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0467 - mae: 16.6736 - val_loss: 4.0676 - val_mae: 15.7641 - lr: 2.9802e-11\n",
            "Epoch 67/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0480 - mae: 16.1906\n",
            "Epoch 00067: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.0502 - mae: 16.2221 - val_loss: 4.1133 - val_mae: 16.7043 - lr: 1.4901e-11\n",
            "Epoch 68/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0602 - mae: 16.3591\n",
            "Epoch 00068: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00068: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0598 - mae: 16.3595 - val_loss: 4.0289 - val_mae: 17.0984 - lr: 1.4901e-11\n",
            "Epoch 69/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0739 - mae: 15.9899\n",
            "Epoch 00069: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.0745 - mae: 15.9843 - val_loss: 4.0651 - val_mae: 17.2564 - lr: 7.4506e-12\n",
            "Epoch 70/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0642 - mae: 16.7245\n",
            "Epoch 00070: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00070: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0642 - mae: 16.7349 - val_loss: 4.0988 - val_mae: 15.9007 - lr: 7.4506e-12\n",
            "Epoch 71/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.0778 - mae: 16.2637\n",
            "Epoch 00071: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0778 - mae: 16.2637 - val_loss: 4.0580 - val_mae: 16.9394 - lr: 3.7253e-12\n",
            "Epoch 72/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0592 - mae: 16.6728\n",
            "Epoch 00072: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.8626452377018543e-12.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0598 - mae: 16.6808 - val_loss: 4.0717 - val_mae: 16.0929 - lr: 3.7253e-12\n",
            "Epoch 73/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0644 - mae: 16.6274\n",
            "Epoch 00073: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0661 - mae: 16.6326 - val_loss: 4.0462 - val_mae: 15.1392 - lr: 1.8626e-12\n",
            "Epoch 74/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0881 - mae: 17.1473\n",
            "Epoch 00074: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00074: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0878 - mae: 17.1460 - val_loss: 4.0553 - val_mae: 15.8674 - lr: 1.8626e-12\n",
            "Epoch 75/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0521 - mae: 16.5729\n",
            "Epoch 00075: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 97ms/step - loss: 4.0519 - mae: 16.5651 - val_loss: 4.0695 - val_mae: 16.2403 - lr: 9.3132e-13\n",
            "Epoch 76/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0679 - mae: 16.6960\n",
            "Epoch 00076: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00076: ReduceLROnPlateau reducing learning rate to 4.656613094254636e-13.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0676 - mae: 16.6894 - val_loss: 4.0595 - val_mae: 17.3182 - lr: 9.3132e-13\n",
            "Epoch 77/200\n",
            "41/42 [============================>.] - ETA: 0s - loss: 4.0625 - mae: 16.2332\n",
            "Epoch 00077: val_loss did not improve from 4.01320\n",
            "42/42 [==============================] - 4s 96ms/step - loss: 4.0631 - mae: 16.2329 - val_loss: 4.0927 - val_mae: 17.2865 - lr: 4.6566e-13\n",
            "Epoch 78/200\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.0733 - mae: 17.1215Restoring model weights from the end of the best epoch: 48.\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 4.01320\n",
            "\n",
            "Epoch 00078: ReduceLROnPlateau reducing learning rate to 2.328306547127318e-13.\n",
            "42/42 [==============================] - 4s 95ms/step - loss: 4.0733 - mae: 17.1215 - val_loss: 4.0465 - val_mae: 15.2607 - lr: 4.6566e-13\n",
            "Epoch 00078: early stopping\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 4.0145 - mae: 16.0348\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2160x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval Mean 9.452842766250225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IEEE_Test_Data_PPG, IEEE_Test_Data_ACC, IEEE_Test_Data_truth = preprocess_dataset(IEEE_Test_Data)\n",
        "train_model_loso(IEEE_Test_Data, IEEE_Test_Data_PPG, IEEE_Test_Data_ACC, IEEE_Test_Data_truth, \"IEEE_Test\")"
      ],
      "metadata": {
        "id": "wgHEUxJSZmNT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87678967-f0ac-4b65-8a30-64b648b82d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fit model on training data fold  0\n",
            "Epoch 1/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 92.9015 - mae: 92.8783\n",
            "Epoch 00001: val_loss improved from inf to 12.25428, saving model to /home/jupyter/IEEE_Test/fold0.h5\n",
            "29/29 [==============================] - 15s 206ms/step - loss: 92.9015 - mae: 92.8783 - val_loss: 12.2543 - val_mae: 33.5581 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 8.3999 - mae: 28.3633\n",
            "Epoch 00002: val_loss improved from 12.25428 to 7.77322, saving model to /home/jupyter/IEEE_Test/fold0.h5\n",
            "29/29 [==============================] - 4s 124ms/step - loss: 8.3999 - mae: 28.3633 - val_loss: 7.7732 - val_mae: 31.5837 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 5.5469 - mae: 19.7593\n",
            "Epoch 00003: val_loss improved from 7.77322 to 6.25871, saving model to /home/jupyter/IEEE_Test/fold0.h5\n",
            "29/29 [==============================] - 3s 121ms/step - loss: 5.5469 - mae: 19.7593 - val_loss: 6.2587 - val_mae: 24.6977 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 4.8094 - mae: 16.6969\n",
            "Epoch 00004: val_loss did not improve from 6.25871\n",
            "29/29 [==============================] - 3s 117ms/step - loss: 4.8094 - mae: 16.6969 - val_loss: 6.9895 - val_mae: 30.5301 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 4.4196 - mae: 15.2824\n",
            "Epoch 00005: val_loss did not improve from 6.25871\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "29/29 [==============================] - 3s 116ms/step - loss: 4.4196 - mae: 15.2824 - val_loss: 6.3204 - val_mae: 26.9547 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 4.1345 - mae: 14.1062\n",
            "Epoch 00006: val_loss improved from 6.25871 to 5.71168, saving model to /home/jupyter/IEEE_Test/fold0.h5\n",
            "29/29 [==============================] - 3s 120ms/step - loss: 4.1345 - mae: 14.1062 - val_loss: 5.7117 - val_mae: 21.8874 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 4.0468 - mae: 13.2251\n",
            "Epoch 00007: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 115ms/step - loss: 4.0468 - mae: 13.2251 - val_loss: 6.6350 - val_mae: 29.7855 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.8483 - mae: 12.5392\n",
            "Epoch 00008: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "29/29 [==============================] - 3s 114ms/step - loss: 3.8483 - mae: 12.5392 - val_loss: 6.5121 - val_mae: 27.5853 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.7804 - mae: 11.9165\n",
            "Epoch 00009: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 116ms/step - loss: 3.7804 - mae: 11.9165 - val_loss: 6.3079 - val_mae: 26.9228 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.6829 - mae: 11.5251\n",
            "Epoch 00010: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "29/29 [==============================] - 3s 113ms/step - loss: 3.6829 - mae: 11.5251 - val_loss: 6.9780 - val_mae: 33.3755 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.6559 - mae: 11.2527\n",
            "Epoch 00011: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 110ms/step - loss: 3.6559 - mae: 11.2527 - val_loss: 6.3253 - val_mae: 26.5001 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.6115 - mae: 11.1052\n",
            "Epoch 00012: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "29/29 [==============================] - 3s 111ms/step - loss: 3.6115 - mae: 11.1052 - val_loss: 5.9491 - val_mae: 24.3504 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.4851 - mae: 9.8825\n",
            "Epoch 00013: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 108ms/step - loss: 3.4851 - mae: 9.8825 - val_loss: 6.2512 - val_mae: 25.8486 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.4570 - mae: 9.7651\n",
            "Epoch 00014: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "29/29 [==============================] - 3s 112ms/step - loss: 3.4570 - mae: 9.7651 - val_loss: 6.2568 - val_mae: 25.5809 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.4577 - mae: 9.6917\n",
            "Epoch 00015: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 112ms/step - loss: 3.4577 - mae: 9.6917 - val_loss: 6.2272 - val_mae: 24.5941 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.4326 - mae: 10.0171\n",
            "Epoch 00016: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "29/29 [==============================] - 3s 112ms/step - loss: 3.4326 - mae: 10.0171 - val_loss: 6.0901 - val_mae: 24.4231 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3914 - mae: 9.5784\n",
            "Epoch 00017: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 112ms/step - loss: 3.3914 - mae: 9.5784 - val_loss: 6.2231 - val_mae: 25.2899 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.4125 - mae: 9.5844\n",
            "Epoch 00018: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "29/29 [==============================] - 3s 106ms/step - loss: 3.4125 - mae: 9.5844 - val_loss: 6.3997 - val_mae: 27.1658 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3987 - mae: 9.6648\n",
            "Epoch 00019: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 110ms/step - loss: 3.3987 - mae: 9.6648 - val_loss: 6.3968 - val_mae: 26.3467 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.4007 - mae: 9.7433\n",
            "Epoch 00020: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 3.4007 - mae: 9.7433 - val_loss: 6.2086 - val_mae: 25.7931 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3938 - mae: 9.4925\n",
            "Epoch 00021: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 3.3938 - mae: 9.4925 - val_loss: 6.3911 - val_mae: 25.6826 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3963 - mae: 9.5674\n",
            "Epoch 00022: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "29/29 [==============================] - 3s 103ms/step - loss: 3.3963 - mae: 9.5674 - val_loss: 6.2323 - val_mae: 25.3286 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3890 - mae: 9.0739\n",
            "Epoch 00023: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 3.3890 - mae: 9.0739 - val_loss: 6.2571 - val_mae: 25.1650 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3745 - mae: 9.4487\n",
            "Epoch 00024: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 3.3745 - mae: 9.4487 - val_loss: 6.2117 - val_mae: 24.8416 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.4215 - mae: 9.7078\n",
            "Epoch 00025: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 3.4215 - mae: 9.7078 - val_loss: 6.1776 - val_mae: 24.5152 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.4122 - mae: 9.9951 \n",
            "Epoch 00026: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 3.4122 - mae: 9.9951 - val_loss: 6.4644 - val_mae: 26.5067 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3784 - mae: 9.4978\n",
            "Epoch 00027: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 3.3784 - mae: 9.4978 - val_loss: 6.1536 - val_mae: 24.9548 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3924 - mae: 9.6054\n",
            "Epoch 00028: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "29/29 [==============================] - 3s 108ms/step - loss: 3.3924 - mae: 9.6054 - val_loss: 6.1903 - val_mae: 25.1592 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.4070 - mae: 9.4660\n",
            "Epoch 00029: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 3.4070 - mae: 9.4660 - val_loss: 6.1240 - val_mae: 24.8300 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3950 - mae: 9.0126\n",
            "Epoch 00030: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 3.3950 - mae: 9.0126 - val_loss: 6.3411 - val_mae: 25.4621 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3784 - mae: 9.5038\n",
            "Epoch 00031: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 108ms/step - loss: 3.3784 - mae: 9.5038 - val_loss: 6.1632 - val_mae: 25.0973 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3697 - mae: 9.1729\n",
            "Epoch 00032: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "29/29 [==============================] - 3s 108ms/step - loss: 3.3697 - mae: 9.1729 - val_loss: 6.2294 - val_mae: 25.3361 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3922 - mae: 9.8483\n",
            "Epoch 00033: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 105ms/step - loss: 3.3922 - mae: 9.8483 - val_loss: 6.1786 - val_mae: 23.8252 - lr: 6.1035e-08\n",
            "Epoch 34/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3821 - mae: 9.3962\n",
            "Epoch 00034: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 3.3821 - mae: 9.3962 - val_loss: 6.3244 - val_mae: 26.2220 - lr: 6.1035e-08\n",
            "Epoch 35/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3642 - mae: 9.5017\n",
            "Epoch 00035: val_loss did not improve from 5.71168\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 3.3642 - mae: 9.5017 - val_loss: 6.2382 - val_mae: 26.0963 - lr: 3.0518e-08\n",
            "Epoch 36/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.4286 - mae: 9.3892Restoring model weights from the end of the best epoch: 6.\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 5.71168\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "29/29 [==============================] - 3s 105ms/step - loss: 3.4286 - mae: 9.3892 - val_loss: 6.3600 - val_mae: 26.1376 - lr: 3.0518e-08\n",
            "Epoch 00036: early stopping\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 5.6271 - mae: 20.6681\n",
            "Fit model on training data fold  1\n",
            "Epoch 1/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 112.5852 - mae: 101.2802\n",
            "Epoch 00001: val_loss improved from inf to 26.55236, saving model to /home/jupyter/IEEE_Test/fold1.h5\n",
            "29/29 [==============================] - 16s 208ms/step - loss: 112.5852 - mae: 101.2802 - val_loss: 26.5524 - val_mae: 66.4302 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 9.2210 - mae: 28.2727\n",
            "Epoch 00002: val_loss improved from 26.55236 to 5.08726, saving model to /home/jupyter/IEEE_Test/fold1.h5\n",
            "29/29 [==============================] - 4s 122ms/step - loss: 9.2210 - mae: 28.2727 - val_loss: 5.0873 - val_mae: 16.9599 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 4.8256 - mae: 15.8028\n",
            "Epoch 00003: val_loss did not improve from 5.08726\n",
            "29/29 [==============================] - 3s 115ms/step - loss: 4.8256 - mae: 15.8028 - val_loss: 5.3553 - val_mae: 18.1600 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 4.1288 - mae: 12.8760\n",
            "Epoch 00004: val_loss did not improve from 5.08726\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "29/29 [==============================] - 3s 117ms/step - loss: 4.1288 - mae: 12.8760 - val_loss: 6.0270 - val_mae: 21.3088 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.8754 - mae: 11.7530\n",
            "Epoch 00005: val_loss improved from 5.08726 to 4.58274, saving model to /home/jupyter/IEEE_Test/fold1.h5\n",
            "29/29 [==============================] - 4s 124ms/step - loss: 3.8754 - mae: 11.7530 - val_loss: 4.5827 - val_mae: 14.8084 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.6746 - mae: 10.3901\n",
            "Epoch 00006: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 118ms/step - loss: 3.6746 - mae: 10.3901 - val_loss: 5.0598 - val_mae: 18.1135 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.5635 - mae: 10.4182\n",
            "Epoch 00007: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "29/29 [==============================] - 3s 117ms/step - loss: 3.5635 - mae: 10.4182 - val_loss: 5.1717 - val_mae: 18.7460 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3640 - mae: 8.8619\n",
            "Epoch 00008: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 117ms/step - loss: 3.3640 - mae: 8.8619 - val_loss: 5.1971 - val_mae: 17.7732 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3813 - mae: 8.8430\n",
            "Epoch 00009: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "29/29 [==============================] - 3s 114ms/step - loss: 3.3813 - mae: 8.8430 - val_loss: 5.2492 - val_mae: 18.7118 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3019 - mae: 8.8113\n",
            "Epoch 00010: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 115ms/step - loss: 3.3019 - mae: 8.8113 - val_loss: 5.0590 - val_mae: 17.1667 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.2217 - mae: 7.7617\n",
            "Epoch 00011: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "29/29 [==============================] - 3s 112ms/step - loss: 3.2217 - mae: 7.7617 - val_loss: 5.2533 - val_mae: 17.4221 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.2140 - mae: 7.6335\n",
            "Epoch 00012: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 112ms/step - loss: 3.2140 - mae: 7.6335 - val_loss: 5.1458 - val_mae: 18.3194 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1626 - mae: 7.9571\n",
            "Epoch 00013: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "29/29 [==============================] - 3s 112ms/step - loss: 3.1626 - mae: 7.9571 - val_loss: 5.1629 - val_mae: 17.1924 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1753 - mae: 8.1623\n",
            "Epoch 00014: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 109ms/step - loss: 3.1753 - mae: 8.1623 - val_loss: 5.2151 - val_mae: 18.8557 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1383 - mae: 7.9259\n",
            "Epoch 00015: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "29/29 [==============================] - 3s 109ms/step - loss: 3.1383 - mae: 7.9259 - val_loss: 5.2464 - val_mae: 18.7408 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1137 - mae: 7.7580\n",
            "Epoch 00016: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 108ms/step - loss: 3.1137 - mae: 7.7580 - val_loss: 5.1001 - val_mae: 16.6661 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1470 - mae: 7.8582\n",
            "Epoch 00017: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "29/29 [==============================] - 3s 111ms/step - loss: 3.1470 - mae: 7.8582 - val_loss: 5.2381 - val_mae: 17.8748 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1601 - mae: 7.5859\n",
            "Epoch 00018: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 109ms/step - loss: 3.1601 - mae: 7.5859 - val_loss: 5.1810 - val_mae: 18.1033 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1772 - mae: 8.2345\n",
            "Epoch 00019: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "29/29 [==============================] - 3s 111ms/step - loss: 3.1772 - mae: 8.2345 - val_loss: 5.1662 - val_mae: 17.4039 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1467 - mae: 7.7423\n",
            "Epoch 00020: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 110ms/step - loss: 3.1467 - mae: 7.7423 - val_loss: 5.3281 - val_mae: 18.4717 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1329 - mae: 7.5221\n",
            "Epoch 00021: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 3.1329 - mae: 7.5221 - val_loss: 5.0774 - val_mae: 17.4274 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1043 - mae: 7.1011\n",
            "Epoch 00022: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 109ms/step - loss: 3.1043 - mae: 7.1011 - val_loss: 5.1173 - val_mae: 16.5390 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1361 - mae: 7.5775\n",
            "Epoch 00023: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "29/29 [==============================] - 3s 106ms/step - loss: 3.1361 - mae: 7.5775 - val_loss: 5.2056 - val_mae: 17.6041 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1164 - mae: 7.7117\n",
            "Epoch 00024: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 3.1164 - mae: 7.7117 - val_loss: 5.2714 - val_mae: 17.8334 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.0973 - mae: 7.5306\n",
            "Epoch 00025: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "29/29 [==============================] - 3s 111ms/step - loss: 3.0973 - mae: 7.5306 - val_loss: 5.2007 - val_mae: 17.1699 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1108 - mae: 7.9539\n",
            "Epoch 00026: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 110ms/step - loss: 3.1108 - mae: 7.9539 - val_loss: 5.2520 - val_mae: 18.2044 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1310 - mae: 8.0334\n",
            "Epoch 00027: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "29/29 [==============================] - 3s 110ms/step - loss: 3.1310 - mae: 8.0334 - val_loss: 5.2352 - val_mae: 17.8199 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1084 - mae: 6.9116\n",
            "Epoch 00028: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 110ms/step - loss: 3.1084 - mae: 6.9116 - val_loss: 5.2335 - val_mae: 17.3097 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1328 - mae: 7.6337\n",
            "Epoch 00029: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 3.1328 - mae: 7.6337 - val_loss: 5.2450 - val_mae: 18.9136 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1158 - mae: 7.6263\n",
            "Epoch 00030: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 109ms/step - loss: 3.1158 - mae: 7.6263 - val_loss: 5.1332 - val_mae: 18.2487 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1208 - mae: 7.4951\n",
            "Epoch 00031: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "29/29 [==============================] - 3s 106ms/step - loss: 3.1208 - mae: 7.4951 - val_loss: 5.0137 - val_mae: 16.8707 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1129 - mae: 7.8702\n",
            "Epoch 00032: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 3.1129 - mae: 7.8702 - val_loss: 5.0146 - val_mae: 17.0876 - lr: 6.1035e-08\n",
            "Epoch 33/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1082 - mae: 6.9885\n",
            "Epoch 00033: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "29/29 [==============================] - 3s 106ms/step - loss: 3.1082 - mae: 6.9885 - val_loss: 5.2821 - val_mae: 18.2625 - lr: 6.1035e-08\n",
            "Epoch 34/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1306 - mae: 7.5378\n",
            "Epoch 00034: val_loss did not improve from 4.58274\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 3.1306 - mae: 7.5378 - val_loss: 5.2363 - val_mae: 18.7166 - lr: 3.0518e-08\n",
            "Epoch 35/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1602 - mae: 7.9110Restoring model weights from the end of the best epoch: 5.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 4.58274\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "29/29 [==============================] - 3s 108ms/step - loss: 3.1602 - mae: 7.9110 - val_loss: 5.3219 - val_mae: 18.0375 - lr: 3.0518e-08\n",
            "Epoch 00035: early stopping\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 9.7433 - mae: 41.5820\n",
            "Fit model on training data fold  2\n",
            "Epoch 1/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 134.5832 - mae: 108.3480\n",
            "Epoch 00001: val_loss improved from inf to 33.67804, saving model to /home/jupyter/IEEE_Test/fold2.h5\n",
            "29/29 [==============================] - 16s 207ms/step - loss: 134.5832 - mae: 108.3480 - val_loss: 33.6780 - val_mae: 63.6360 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 12.1748 - mae: 34.0948\n",
            "Epoch 00002: val_loss improved from 33.67804 to 7.93310, saving model to /home/jupyter/IEEE_Test/fold2.h5\n",
            "29/29 [==============================] - 4s 126ms/step - loss: 12.1748 - mae: 34.0948 - val_loss: 7.9331 - val_mae: 29.3324 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 5.3474 - mae: 17.8618\n",
            "Epoch 00003: val_loss improved from 7.93310 to 7.36803, saving model to /home/jupyter/IEEE_Test/fold2.h5\n",
            "29/29 [==============================] - 3s 120ms/step - loss: 5.3474 - mae: 17.8618 - val_loss: 7.3680 - val_mae: 27.1543 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 4.4807 - mae: 14.8976\n",
            "Epoch 00004: val_loss improved from 7.36803 to 7.24974, saving model to /home/jupyter/IEEE_Test/fold2.h5\n",
            "29/29 [==============================] - 3s 120ms/step - loss: 4.4807 - mae: 14.8976 - val_loss: 7.2497 - val_mae: 26.8820 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 4.0428 - mae: 12.7577\n",
            "Epoch 00005: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 113ms/step - loss: 4.0428 - mae: 12.7577 - val_loss: 8.0597 - val_mae: 33.0953 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.9365 - mae: 12.2373\n",
            "Epoch 00006: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "29/29 [==============================] - 3s 112ms/step - loss: 3.9365 - mae: 12.2373 - val_loss: 7.6385 - val_mae: 31.9427 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.5834 - mae: 10.7917\n",
            "Epoch 00007: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 106ms/step - loss: 3.5834 - mae: 10.7917 - val_loss: 7.4744 - val_mae: 30.8533 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.5418 - mae: 10.1440\n",
            "Epoch 00008: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 3.5418 - mae: 10.1440 - val_loss: 7.8594 - val_mae: 32.0118 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3554 - mae: 8.8193\n",
            "Epoch 00009: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 105ms/step - loss: 3.3554 - mae: 8.8193 - val_loss: 7.5815 - val_mae: 31.2720 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.3471 - mae: 9.3519\n",
            "Epoch 00010: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 3.3471 - mae: 9.3519 - val_loss: 7.5324 - val_mae: 31.4651 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.2510 - mae: 8.3461\n",
            "Epoch 00011: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 3.2510 - mae: 8.3461 - val_loss: 7.6390 - val_mae: 31.8987 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.2355 - mae: 8.0768\n",
            "Epoch 00012: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 3.2355 - mae: 8.0768 - val_loss: 7.7976 - val_mae: 30.9237 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.2037 - mae: 7.9453\n",
            "Epoch 00013: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 3.2037 - mae: 7.9453 - val_loss: 7.8147 - val_mae: 32.9045 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.2105 - mae: 8.1248\n",
            "Epoch 00014: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 3.2105 - mae: 8.1248 - val_loss: 7.7360 - val_mae: 31.2563 - lr: 6.2500e-05\n",
            "Epoch 15/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1896 - mae: 8.0968\n",
            "Epoch 00015: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 108ms/step - loss: 3.1896 - mae: 8.0968 - val_loss: 7.7367 - val_mae: 30.8308 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.2239 - mae: 8.0757\n",
            "Epoch 00016: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 3.2239 - mae: 8.0757 - val_loss: 7.6238 - val_mae: 30.3463 - lr: 3.1250e-05\n",
            "Epoch 17/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1534 - mae: 7.4805\n",
            "Epoch 00017: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 3.1534 - mae: 7.4805 - val_loss: 7.5912 - val_mae: 29.5503 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1643 - mae: 7.9250\n",
            "Epoch 00018: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "29/29 [==============================] - 3s 103ms/step - loss: 3.1643 - mae: 7.9250 - val_loss: 7.6947 - val_mae: 30.6387 - lr: 1.5625e-05\n",
            "Epoch 19/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1494 - mae: 7.8104\n",
            "Epoch 00019: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 103ms/step - loss: 3.1494 - mae: 7.8104 - val_loss: 7.6724 - val_mae: 29.9360 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1321 - mae: 7.2770\n",
            "Epoch 00020: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "29/29 [==============================] - 3s 103ms/step - loss: 3.1321 - mae: 7.2770 - val_loss: 7.6232 - val_mae: 30.3883 - lr: 7.8125e-06\n",
            "Epoch 21/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1807 - mae: 8.0425\n",
            "Epoch 00021: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 3.1807 - mae: 8.0425 - val_loss: 7.6545 - val_mae: 29.7741 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1263 - mae: 7.4945\n",
            "Epoch 00022: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 3.1263 - mae: 7.4945 - val_loss: 7.5843 - val_mae: 29.8699 - lr: 3.9063e-06\n",
            "Epoch 23/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1461 - mae: 7.9982\n",
            "Epoch 00023: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 3.1461 - mae: 7.9982 - val_loss: 7.6323 - val_mae: 30.9543 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1646 - mae: 7.6247\n",
            "Epoch 00024: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "29/29 [==============================] - 3s 108ms/step - loss: 3.1646 - mae: 7.6247 - val_loss: 7.6586 - val_mae: 30.8628 - lr: 1.9531e-06\n",
            "Epoch 25/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1441 - mae: 7.7709\n",
            "Epoch 00025: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 105ms/step - loss: 3.1441 - mae: 7.7709 - val_loss: 7.6533 - val_mae: 29.7986 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1515 - mae: 7.7613\n",
            "Epoch 00026: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "29/29 [==============================] - 3s 105ms/step - loss: 3.1515 - mae: 7.7613 - val_loss: 7.7121 - val_mae: 30.6047 - lr: 9.7656e-07\n",
            "Epoch 27/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1271 - mae: 7.7831\n",
            "Epoch 00027: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 105ms/step - loss: 3.1271 - mae: 7.7831 - val_loss: 7.5774 - val_mae: 29.2065 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1424 - mae: 7.9309\n",
            "Epoch 00028: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "29/29 [==============================] - 3s 108ms/step - loss: 3.1424 - mae: 7.9309 - val_loss: 7.5601 - val_mae: 29.6032 - lr: 4.8828e-07\n",
            "Epoch 29/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1407 - mae: 7.9503\n",
            "Epoch 00029: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 105ms/step - loss: 3.1407 - mae: 7.9503 - val_loss: 7.6633 - val_mae: 29.1562 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1406 - mae: 7.8920\n",
            "Epoch 00030: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "29/29 [==============================] - 3s 108ms/step - loss: 3.1406 - mae: 7.8920 - val_loss: 7.6791 - val_mae: 30.2644 - lr: 2.4414e-07\n",
            "Epoch 31/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1431 - mae: 8.0634\n",
            "Epoch 00031: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 108ms/step - loss: 3.1431 - mae: 8.0634 - val_loss: 7.7066 - val_mae: 31.4566 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1428 - mae: 7.3244\n",
            "Epoch 00032: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 3.1428 - mae: 7.3244 - val_loss: 7.6120 - val_mae: 30.0875 - lr: 1.2207e-07\n",
            "Epoch 33/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1353 - mae: 7.6602\n",
            "Epoch 00033: val_loss did not improve from 7.24974\n",
            "29/29 [==============================] - 3s 105ms/step - loss: 3.1353 - mae: 7.6602 - val_loss: 7.7053 - val_mae: 30.6482 - lr: 6.1035e-08\n",
            "Epoch 34/200\n",
            "29/29 [==============================] - ETA: 0s - loss: 3.1286 - mae: 7.8039Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 7.24974\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "29/29 [==============================] - 3s 109ms/step - loss: 3.1286 - mae: 7.8039 - val_loss: 7.6869 - val_mae: 30.5137 - lr: 6.1035e-08\n",
            "Epoch 00034: early stopping\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 5.4824 - mae: 19.0394\n",
            "Fit model on training data fold  3\n",
            "Epoch 1/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 109.0702 - mae: 101.8987\n",
            "Epoch 00001: val_loss improved from inf to 39.06916, saving model to /home/jupyter/IEEE_Test/fold3.h5\n",
            "30/30 [==============================] - 16s 215ms/step - loss: 109.0702 - mae: 101.8987 - val_loss: 39.0692 - val_mae: 65.3614 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 10.1171 - mae: 32.4084\n",
            "Epoch 00002: val_loss improved from 39.06916 to 7.46138, saving model to /home/jupyter/IEEE_Test/fold3.h5\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 10.1171 - mae: 32.4084 - val_loss: 7.4614 - val_mae: 29.9288 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 5.8886 - mae: 21.2503\n",
            "Epoch 00003: val_loss improved from 7.46138 to 6.87926, saving model to /home/jupyter/IEEE_Test/fold3.h5\n",
            "30/30 [==============================] - 3s 110ms/step - loss: 5.8886 - mae: 21.2503 - val_loss: 6.8793 - val_mae: 27.8810 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.8764 - mae: 17.0526\n",
            "Epoch 00004: val_loss improved from 6.87926 to 6.67544, saving model to /home/jupyter/IEEE_Test/fold3.h5\n",
            "30/30 [==============================] - 3s 110ms/step - loss: 4.8764 - mae: 17.0526 - val_loss: 6.6754 - val_mae: 32.0387 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.5427 - mae: 17.1667\n",
            "Epoch 00005: val_loss improved from 6.67544 to 6.32007, saving model to /home/jupyter/IEEE_Test/fold3.h5\n",
            "30/30 [==============================] - 3s 109ms/step - loss: 4.5427 - mae: 17.1667 - val_loss: 6.3201 - val_mae: 31.4263 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.4024 - mae: 16.1934\n",
            "Epoch 00006: val_loss did not improve from 6.32007\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 4.4024 - mae: 16.1934 - val_loss: 6.4630 - val_mae: 33.5439 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.2783 - mae: 16.1101\n",
            "Epoch 00007: val_loss did not improve from 6.32007\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 4.2783 - mae: 16.1101 - val_loss: 6.4354 - val_mae: 32.5682 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.1519 - mae: 15.5976\n",
            "Epoch 00008: val_loss improved from 6.32007 to 6.09969, saving model to /home/jupyter/IEEE_Test/fold3.h5\n",
            "30/30 [==============================] - 3s 114ms/step - loss: 4.1519 - mae: 15.5976 - val_loss: 6.0997 - val_mae: 29.2706 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.0749 - mae: 14.2868\n",
            "Epoch 00009: val_loss improved from 6.09969 to 5.94323, saving model to /home/jupyter/IEEE_Test/fold3.h5\n",
            "30/30 [==============================] - 3s 113ms/step - loss: 4.0749 - mae: 14.2868 - val_loss: 5.9432 - val_mae: 30.0586 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.9835 - mae: 14.3468\n",
            "Epoch 00010: val_loss improved from 5.94323 to 5.91473, saving model to /home/jupyter/IEEE_Test/fold3.h5\n",
            "30/30 [==============================] - 3s 114ms/step - loss: 3.9835 - mae: 14.3468 - val_loss: 5.9147 - val_mae: 30.5168 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.0284 - mae: 14.3382\n",
            "Epoch 00011: val_loss improved from 5.91473 to 5.87045, saving model to /home/jupyter/IEEE_Test/fold3.h5\n",
            "30/30 [==============================] - 3s 114ms/step - loss: 4.0284 - mae: 14.3382 - val_loss: 5.8704 - val_mae: 29.4516 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.1538 - mae: 15.3175\n",
            "Epoch 00012: val_loss did not improve from 5.87045\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 4.1538 - mae: 15.3175 - val_loss: 5.9147 - val_mae: 28.4018 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.9097 - mae: 13.5324\n",
            "Epoch 00013: val_loss improved from 5.87045 to 5.74540, saving model to /home/jupyter/IEEE_Test/fold3.h5\n",
            "30/30 [==============================] - 3s 113ms/step - loss: 3.9097 - mae: 13.5324 - val_loss: 5.7454 - val_mae: 26.8597 - lr: 5.0000e-04\n",
            "Epoch 14/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.9767 - mae: 14.7590\n",
            "Epoch 00014: val_loss improved from 5.74540 to 5.74131, saving model to /home/jupyter/IEEE_Test/fold3.h5\n",
            "30/30 [==============================] - 3s 114ms/step - loss: 3.9767 - mae: 14.7590 - val_loss: 5.7413 - val_mae: 28.0875 - lr: 5.0000e-04\n",
            "Epoch 15/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.9110 - mae: 13.4290\n",
            "Epoch 00015: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 3.9110 - mae: 13.4290 - val_loss: 6.0458 - val_mae: 26.7594 - lr: 5.0000e-04\n",
            "Epoch 16/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.8333 - mae: 13.3677\n",
            "Epoch 00016: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "30/30 [==============================] - 3s 113ms/step - loss: 3.8333 - mae: 13.3677 - val_loss: 5.9455 - val_mae: 29.3469 - lr: 5.0000e-04\n",
            "Epoch 17/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.6859 - mae: 12.0213\n",
            "Epoch 00017: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 3.6859 - mae: 12.0213 - val_loss: 5.9560 - val_mae: 29.7377 - lr: 2.5000e-04\n",
            "Epoch 18/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.6156 - mae: 11.0442\n",
            "Epoch 00018: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.6156 - mae: 11.0442 - val_loss: 5.8310 - val_mae: 28.8831 - lr: 2.5000e-04\n",
            "Epoch 19/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.5488 - mae: 10.5631\n",
            "Epoch 00019: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 3.5488 - mae: 10.5631 - val_loss: 5.8960 - val_mae: 26.4165 - lr: 1.2500e-04\n",
            "Epoch 20/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.5800 - mae: 11.2040\n",
            "Epoch 00020: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 3.5800 - mae: 11.2040 - val_loss: 6.0524 - val_mae: 28.4268 - lr: 1.2500e-04\n",
            "Epoch 21/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.5497 - mae: 10.7906\n",
            "Epoch 00021: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.5497 - mae: 10.7906 - val_loss: 5.8926 - val_mae: 27.6231 - lr: 6.2500e-05\n",
            "Epoch 22/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.5503 - mae: 10.8547\n",
            "Epoch 00022: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 3.5503 - mae: 10.8547 - val_loss: 5.8555 - val_mae: 26.6981 - lr: 6.2500e-05\n",
            "Epoch 23/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.5080 - mae: 10.5290\n",
            "Epoch 00023: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.5080 - mae: 10.5290 - val_loss: 5.9499 - val_mae: 27.6891 - lr: 3.1250e-05\n",
            "Epoch 24/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.5106 - mae: 10.4085\n",
            "Epoch 00024: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.5106 - mae: 10.4085 - val_loss: 6.0321 - val_mae: 27.9472 - lr: 3.1250e-05\n",
            "Epoch 25/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.5547 - mae: 10.9874\n",
            "Epoch 00025: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 3.5547 - mae: 10.9874 - val_loss: 5.9477 - val_mae: 28.9067 - lr: 1.5625e-05\n",
            "Epoch 26/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4598 - mae: 10.6084\n",
            "Epoch 00026: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 3.4598 - mae: 10.6084 - val_loss: 6.0151 - val_mae: 28.1775 - lr: 1.5625e-05\n",
            "Epoch 27/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4876 - mae: 10.6136\n",
            "Epoch 00027: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.4876 - mae: 10.6136 - val_loss: 5.9804 - val_mae: 27.7907 - lr: 7.8125e-06\n",
            "Epoch 28/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4767 - mae: 11.0092\n",
            "Epoch 00028: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 3.4767 - mae: 11.0092 - val_loss: 6.0818 - val_mae: 28.8618 - lr: 7.8125e-06\n",
            "Epoch 29/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4885 - mae: 10.0897\n",
            "Epoch 00029: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.4885 - mae: 10.0897 - val_loss: 5.9605 - val_mae: 27.9307 - lr: 3.9063e-06\n",
            "Epoch 30/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4928 - mae: 10.3122\n",
            "Epoch 00030: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.4928 - mae: 10.3122 - val_loss: 6.1050 - val_mae: 28.3126 - lr: 3.9063e-06\n",
            "Epoch 31/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.5025 - mae: 10.7648\n",
            "Epoch 00031: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.5025 - mae: 10.7648 - val_loss: 6.0955 - val_mae: 28.8968 - lr: 1.9531e-06\n",
            "Epoch 32/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4655 - mae: 10.3683\n",
            "Epoch 00032: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 3.4655 - mae: 10.3683 - val_loss: 6.0402 - val_mae: 27.4518 - lr: 1.9531e-06\n",
            "Epoch 33/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4729 - mae: 10.6295\n",
            "Epoch 00033: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.4729 - mae: 10.6295 - val_loss: 5.9876 - val_mae: 27.6294 - lr: 9.7656e-07\n",
            "Epoch 34/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4758 - mae: 10.3933\n",
            "Epoch 00034: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 3.4758 - mae: 10.3933 - val_loss: 5.9554 - val_mae: 27.5123 - lr: 9.7656e-07\n",
            "Epoch 35/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4654 - mae: 10.4043\n",
            "Epoch 00035: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.4654 - mae: 10.4043 - val_loss: 6.1181 - val_mae: 29.2252 - lr: 4.8828e-07\n",
            "Epoch 36/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4586 - mae: 10.6517\n",
            "Epoch 00036: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.4586 - mae: 10.6517 - val_loss: 5.9924 - val_mae: 27.4164 - lr: 4.8828e-07\n",
            "Epoch 37/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4847 - mae: 10.7273\n",
            "Epoch 00037: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 3.4847 - mae: 10.7273 - val_loss: 6.0502 - val_mae: 29.0839 - lr: 2.4414e-07\n",
            "Epoch 38/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4766 - mae: 9.8045\n",
            "Epoch 00038: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 3.4766 - mae: 9.8045 - val_loss: 5.8824 - val_mae: 26.4501 - lr: 2.4414e-07\n",
            "Epoch 39/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4715 - mae: 11.0435\n",
            "Epoch 00039: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 3.4715 - mae: 11.0435 - val_loss: 6.0320 - val_mae: 30.0635 - lr: 1.2207e-07\n",
            "Epoch 40/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.5806 - mae: 11.3752\n",
            "Epoch 00040: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 3.5806 - mae: 11.3752 - val_loss: 5.9756 - val_mae: 27.8206 - lr: 1.2207e-07\n",
            "Epoch 41/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4653 - mae: 10.5482\n",
            "Epoch 00041: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 3.4653 - mae: 10.5482 - val_loss: 6.0379 - val_mae: 28.1236 - lr: 6.1035e-08\n",
            "Epoch 42/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4772 - mae: 10.4668\n",
            "Epoch 00042: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 3.4772 - mae: 10.4668 - val_loss: 5.9214 - val_mae: 26.0945 - lr: 6.1035e-08\n",
            "Epoch 43/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4670 - mae: 10.4096\n",
            "Epoch 00043: val_loss did not improve from 5.74131\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 3.4670 - mae: 10.4096 - val_loss: 5.9728 - val_mae: 27.9469 - lr: 3.0518e-08\n",
            "Epoch 44/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.5203 - mae: 10.0005Restoring model weights from the end of the best epoch: 14.\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 5.74131\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 3.5203 - mae: 10.0005 - val_loss: 6.0321 - val_mae: 29.2911 - lr: 3.0518e-08\n",
            "Epoch 00044: early stopping\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 4.9414 - mae: 21.0524\n",
            "Fit model on training data fold  4\n",
            "Epoch 1/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 128.5336 - mae: 103.2085\n",
            "Epoch 00001: val_loss improved from inf to 29.69765, saving model to /home/jupyter/IEEE_Test/fold4.h5\n",
            "30/30 [==============================] - 15s 183ms/step - loss: 128.5336 - mae: 103.2085 - val_loss: 29.6976 - val_mae: 57.0877 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 8.9915 - mae: 22.9750\n",
            "Epoch 00002: val_loss improved from 29.69765 to 6.91416, saving model to /home/jupyter/IEEE_Test/fold4.h5\n",
            "30/30 [==============================] - 4s 127ms/step - loss: 8.9915 - mae: 22.9750 - val_loss: 6.9142 - val_mae: 20.3114 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 5.0870 - mae: 14.5790\n",
            "Epoch 00003: val_loss improved from 6.91416 to 6.29167, saving model to /home/jupyter/IEEE_Test/fold4.h5\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 5.0870 - mae: 14.5790 - val_loss: 6.2917 - val_mae: 20.2072 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.5664 - mae: 13.6965\n",
            "Epoch 00004: val_loss improved from 6.29167 to 5.74467, saving model to /home/jupyter/IEEE_Test/fold4.h5\n",
            "30/30 [==============================] - 3s 115ms/step - loss: 4.5664 - mae: 13.6965 - val_loss: 5.7447 - val_mae: 17.8523 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.3068 - mae: 12.7863\n",
            "Epoch 00005: val_loss improved from 5.74467 to 5.04359, saving model to /home/jupyter/IEEE_Test/fold4.h5\n",
            "30/30 [==============================] - 3s 114ms/step - loss: 4.3068 - mae: 12.7863 - val_loss: 5.0436 - val_mae: 17.3192 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.9125 - mae: 11.2651\n",
            "Epoch 00006: val_loss did not improve from 5.04359\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.9125 - mae: 11.2651 - val_loss: 5.2511 - val_mae: 17.6923 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.8071 - mae: 11.0422\n",
            "Epoch 00007: val_loss did not improve from 5.04359\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 3.8071 - mae: 11.0422 - val_loss: 5.6554 - val_mae: 18.2443 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.6001 - mae: 9.6312\n",
            "Epoch 00008: val_loss did not improve from 5.04359\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 3.6001 - mae: 9.6312 - val_loss: 5.6551 - val_mae: 18.4223 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.3672 - mae: 8.5989\n",
            "Epoch 00009: val_loss did not improve from 5.04359\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 3.3672 - mae: 8.5989 - val_loss: 5.1988 - val_mae: 15.9362 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.3357 - mae: 8.3850\n",
            "Epoch 00010: val_loss did not improve from 5.04359\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 3.3357 - mae: 8.3850 - val_loss: 5.2057 - val_mae: 17.2053 - lr: 2.5000e-04\n",
            "Epoch 11/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2853 - mae: 8.3448\n",
            "Epoch 00011: val_loss did not improve from 5.04359\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 3.2853 - mae: 8.3448 - val_loss: 5.3803 - val_mae: 17.0040 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.1970 - mae: 7.5635\n",
            "Epoch 00012: val_loss improved from 5.04359 to 5.01144, saving model to /home/jupyter/IEEE_Test/fold4.h5\n",
            "30/30 [==============================] - 3s 109ms/step - loss: 3.1970 - mae: 7.5635 - val_loss: 5.0114 - val_mae: 15.1409 - lr: 1.2500e-04\n",
            "Epoch 13/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.1913 - mae: 8.2670\n",
            "Epoch 00013: val_loss did not improve from 5.01144\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 3.1913 - mae: 8.2670 - val_loss: 5.0471 - val_mae: 16.0756 - lr: 1.2500e-04\n",
            "Epoch 14/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.1670 - mae: 7.5211\n",
            "Epoch 00014: val_loss did not improve from 5.01144\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "30/30 [==============================] - 3s 101ms/step - loss: 3.1670 - mae: 7.5211 - val_loss: 5.0827 - val_mae: 15.7306 - lr: 1.2500e-04\n",
            "Epoch 15/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0938 - mae: 7.2837\n",
            "Epoch 00015: val_loss improved from 5.01144 to 4.96500, saving model to /home/jupyter/IEEE_Test/fold4.h5\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 3.0938 - mae: 7.2837 - val_loss: 4.9650 - val_mae: 15.2503 - lr: 6.2500e-05\n",
            "Epoch 16/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.1327 - mae: 7.4608\n",
            "Epoch 00016: val_loss did not improve from 4.96500\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 3.1327 - mae: 7.4608 - val_loss: 5.0005 - val_mae: 15.3190 - lr: 6.2500e-05\n",
            "Epoch 17/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0672 - mae: 6.8547\n",
            "Epoch 00017: val_loss improved from 4.96500 to 4.75615, saving model to /home/jupyter/IEEE_Test/fold4.h5\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 3.0672 - mae: 6.8547 - val_loss: 4.7561 - val_mae: 14.6721 - lr: 6.2500e-05\n",
            "Epoch 18/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0897 - mae: 7.2944\n",
            "Epoch 00018: val_loss did not improve from 4.75615\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 3.0897 - mae: 7.2944 - val_loss: 4.8274 - val_mae: 15.5929 - lr: 6.2500e-05\n",
            "Epoch 19/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0447 - mae: 6.8588\n",
            "Epoch 00019: val_loss improved from 4.75615 to 4.74895, saving model to /home/jupyter/IEEE_Test/fold4.h5\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 3.0447 - mae: 6.8588 - val_loss: 4.7490 - val_mae: 14.1484 - lr: 6.2500e-05\n",
            "Epoch 20/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0715 - mae: 7.3168\n",
            "Epoch 00020: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 101ms/step - loss: 3.0715 - mae: 7.3168 - val_loss: 4.8717 - val_mae: 14.8067 - lr: 6.2500e-05\n",
            "Epoch 21/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0599 - mae: 7.1248\n",
            "Epoch 00021: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 3.0599 - mae: 7.1248 - val_loss: 5.0208 - val_mae: 15.2065 - lr: 6.2500e-05\n",
            "Epoch 22/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0342 - mae: 7.1376\n",
            "Epoch 00022: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 3.0342 - mae: 7.1376 - val_loss: 4.9169 - val_mae: 15.0490 - lr: 3.1250e-05\n",
            "Epoch 23/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0359 - mae: 6.9726\n",
            "Epoch 00023: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 3.0359 - mae: 6.9726 - val_loss: 5.0388 - val_mae: 16.0420 - lr: 3.1250e-05\n",
            "Epoch 24/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0009 - mae: 6.4848\n",
            "Epoch 00024: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 3.0009 - mae: 6.4848 - val_loss: 4.8640 - val_mae: 14.9814 - lr: 1.5625e-05\n",
            "Epoch 25/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0179 - mae: 7.1497\n",
            "Epoch 00025: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 3.0179 - mae: 7.1497 - val_loss: 4.8452 - val_mae: 14.7625 - lr: 1.5625e-05\n",
            "Epoch 26/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0171 - mae: 6.9170\n",
            "Epoch 00026: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 3.0171 - mae: 6.9170 - val_loss: 4.8167 - val_mae: 14.4967 - lr: 7.8125e-06\n",
            "Epoch 27/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0055 - mae: 6.6102\n",
            "Epoch 00027: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 3.0055 - mae: 6.6102 - val_loss: 4.9327 - val_mae: 14.9115 - lr: 7.8125e-06\n",
            "Epoch 28/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0356 - mae: 6.9142\n",
            "Epoch 00028: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 3.0356 - mae: 6.9142 - val_loss: 4.9682 - val_mae: 14.9521 - lr: 3.9063e-06\n",
            "Epoch 29/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0265 - mae: 7.2165\n",
            "Epoch 00029: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 3.0265 - mae: 7.2165 - val_loss: 4.8057 - val_mae: 14.5762 - lr: 3.9063e-06\n",
            "Epoch 30/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.9891 - mae: 6.9796\n",
            "Epoch 00030: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 98ms/step - loss: 2.9891 - mae: 6.9796 - val_loss: 4.8608 - val_mae: 14.3306 - lr: 1.9531e-06\n",
            "Epoch 31/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.9859 - mae: 6.5734\n",
            "Epoch 00031: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 2.9859 - mae: 6.5734 - val_loss: 4.8813 - val_mae: 14.8493 - lr: 1.9531e-06\n",
            "Epoch 32/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0038 - mae: 6.7502\n",
            "Epoch 00032: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.0038 - mae: 6.7502 - val_loss: 4.9857 - val_mae: 15.4383 - lr: 9.7656e-07\n",
            "Epoch 33/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.9939 - mae: 6.6106\n",
            "Epoch 00033: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "30/30 [==============================] - 3s 99ms/step - loss: 2.9939 - mae: 6.6106 - val_loss: 5.0172 - val_mae: 15.4627 - lr: 9.7656e-07\n",
            "Epoch 34/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.9965 - mae: 6.5101\n",
            "Epoch 00034: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 2.9965 - mae: 6.5101 - val_loss: 4.7541 - val_mae: 14.6849 - lr: 4.8828e-07\n",
            "Epoch 35/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.9914 - mae: 6.6730\n",
            "Epoch 00035: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "30/30 [==============================] - 3s 98ms/step - loss: 2.9914 - mae: 6.6730 - val_loss: 4.9833 - val_mae: 15.1594 - lr: 4.8828e-07\n",
            "Epoch 36/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0041 - mae: 7.0176\n",
            "Epoch 00036: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 3.0041 - mae: 7.0176 - val_loss: 4.8308 - val_mae: 14.7286 - lr: 2.4414e-07\n",
            "Epoch 37/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0115 - mae: 6.9083\n",
            "Epoch 00037: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "30/30 [==============================] - 3s 99ms/step - loss: 3.0115 - mae: 6.9083 - val_loss: 4.8802 - val_mae: 14.5871 - lr: 2.4414e-07\n",
            "Epoch 38/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.9920 - mae: 7.1059\n",
            "Epoch 00038: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 2.9920 - mae: 7.1059 - val_loss: 4.9875 - val_mae: 15.5658 - lr: 1.2207e-07\n",
            "Epoch 39/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.9956 - mae: 6.7207\n",
            "Epoch 00039: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 2.9956 - mae: 6.7207 - val_loss: 4.8339 - val_mae: 14.1815 - lr: 1.2207e-07\n",
            "Epoch 40/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.9999 - mae: 6.6741\n",
            "Epoch 00040: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 2.9999 - mae: 6.6741 - val_loss: 4.7881 - val_mae: 13.9548 - lr: 6.1035e-08\n",
            "Epoch 41/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0123 - mae: 6.6457\n",
            "Epoch 00041: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "30/30 [==============================] - 3s 99ms/step - loss: 3.0123 - mae: 6.6457 - val_loss: 4.8356 - val_mae: 15.0001 - lr: 6.1035e-08\n",
            "Epoch 42/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0099 - mae: 6.7278\n",
            "Epoch 00042: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.0099 - mae: 6.7278 - val_loss: 4.8866 - val_mae: 14.3347 - lr: 3.0518e-08\n",
            "Epoch 43/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0081 - mae: 6.8043\n",
            "Epoch 00043: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "30/30 [==============================] - 3s 99ms/step - loss: 3.0081 - mae: 6.8043 - val_loss: 4.9919 - val_mae: 15.1625 - lr: 3.0518e-08\n",
            "Epoch 44/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.9824 - mae: 6.7560\n",
            "Epoch 00044: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 100ms/step - loss: 2.9824 - mae: 6.7560 - val_loss: 4.8985 - val_mae: 14.1398 - lr: 1.5259e-08\n",
            "Epoch 45/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0073 - mae: 6.8578\n",
            "Epoch 00045: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.0073 - mae: 6.8578 - val_loss: 4.9612 - val_mae: 14.9002 - lr: 1.5259e-08\n",
            "Epoch 46/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0102 - mae: 7.2310\n",
            "Epoch 00046: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.0102 - mae: 7.2310 - val_loss: 4.8906 - val_mae: 14.8729 - lr: 7.6294e-09\n",
            "Epoch 47/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.9903 - mae: 6.6506\n",
            "Epoch 00047: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 2.9903 - mae: 6.6506 - val_loss: 4.9035 - val_mae: 15.1827 - lr: 7.6294e-09\n",
            "Epoch 48/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.9955 - mae: 6.2458\n",
            "Epoch 00048: val_loss did not improve from 4.74895\n",
            "30/30 [==============================] - 3s 99ms/step - loss: 2.9955 - mae: 6.2458 - val_loss: 4.9094 - val_mae: 14.8864 - lr: 3.8147e-09\n",
            "Epoch 49/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.0413 - mae: 7.1234Restoring model weights from the end of the best epoch: 19.\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 4.74895\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
            "30/30 [==============================] - 3s 100ms/step - loss: 3.0413 - mae: 7.1234 - val_loss: 4.8622 - val_mae: 14.5059 - lr: 3.8147e-09\n",
            "Epoch 00049: early stopping\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 7.7897 - mae: 31.6194\n",
            "Fit model on training data fold  5\n",
            "Epoch 1/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 142.1533 - mae: 113.2477\n",
            "Epoch 00001: val_loss improved from inf to 53.64318, saving model to /home/jupyter/IEEE_Test/fold5.h5\n",
            "30/30 [==============================] - 16s 202ms/step - loss: 142.1533 - mae: 113.2477 - val_loss: 53.6432 - val_mae: 87.4783 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 18.1709 - mae: 49.2271\n",
            "Epoch 00002: val_loss improved from 53.64318 to 6.24099, saving model to /home/jupyter/IEEE_Test/fold5.h5\n",
            "30/30 [==============================] - 4s 122ms/step - loss: 18.1709 - mae: 49.2271 - val_loss: 6.2410 - val_mae: 25.9454 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 5.3102 - mae: 20.8687\n",
            "Epoch 00003: val_loss improved from 6.24099 to 5.43952, saving model to /home/jupyter/IEEE_Test/fold5.h5\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 5.3102 - mae: 20.8687 - val_loss: 5.4395 - val_mae: 24.1650 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.6047 - mae: 17.2993\n",
            "Epoch 00004: val_loss did not improve from 5.43952\n",
            "30/30 [==============================] - 3s 112ms/step - loss: 4.6047 - mae: 17.2993 - val_loss: 5.7262 - val_mae: 26.4471 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.2706 - mae: 15.9167\n",
            "Epoch 00005: val_loss improved from 5.43952 to 5.41445, saving model to /home/jupyter/IEEE_Test/fold5.h5\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 4.2706 - mae: 15.9167 - val_loss: 5.4145 - val_mae: 22.9559 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.1280 - mae: 14.8750\n",
            "Epoch 00006: val_loss improved from 5.41445 to 5.23661, saving model to /home/jupyter/IEEE_Test/fold5.h5\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 4.1280 - mae: 14.8750 - val_loss: 5.2366 - val_mae: 25.6008 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.0986 - mae: 15.4788\n",
            "Epoch 00007: val_loss did not improve from 5.23661\n",
            "30/30 [==============================] - 3s 111ms/step - loss: 4.0986 - mae: 15.4788 - val_loss: 5.6228 - val_mae: 27.2240 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.0629 - mae: 15.5605\n",
            "Epoch 00008: val_loss did not improve from 5.23661\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 4.0629 - mae: 15.5605 - val_loss: 5.2935 - val_mae: 26.5352 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.8764 - mae: 14.1722\n",
            "Epoch 00009: val_loss did not improve from 5.23661\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 3.8764 - mae: 14.1722 - val_loss: 5.2376 - val_mae: 24.2943 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.8228 - mae: 13.1971\n",
            "Epoch 00010: val_loss did not improve from 5.23661\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 3.8228 - mae: 13.1971 - val_loss: 5.5367 - val_mae: 26.0952 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.8630 - mae: 13.5633\n",
            "Epoch 00011: val_loss improved from 5.23661 to 5.06577, saving model to /home/jupyter/IEEE_Test/fold5.h5\n",
            "30/30 [==============================] - 3s 112ms/step - loss: 3.8630 - mae: 13.5633 - val_loss: 5.0658 - val_mae: 23.4388 - lr: 2.5000e-04\n",
            "Epoch 12/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.8056 - mae: 13.1159\n",
            "Epoch 00012: val_loss improved from 5.06577 to 4.83083, saving model to /home/jupyter/IEEE_Test/fold5.h5\n",
            "30/30 [==============================] - 3s 112ms/step - loss: 3.8056 - mae: 13.1159 - val_loss: 4.8308 - val_mae: 20.2664 - lr: 2.5000e-04\n",
            "Epoch 13/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.7093 - mae: 12.9183\n",
            "Epoch 00013: val_loss did not improve from 4.83083\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.7093 - mae: 12.9183 - val_loss: 4.9636 - val_mae: 21.0347 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.6359 - mae: 12.3340\n",
            "Epoch 00014: val_loss improved from 4.83083 to 4.76524, saving model to /home/jupyter/IEEE_Test/fold5.h5\n",
            "30/30 [==============================] - 3s 109ms/step - loss: 3.6359 - mae: 12.3340 - val_loss: 4.7652 - val_mae: 19.3634 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.5933 - mae: 11.8160\n",
            "Epoch 00015: val_loss did not improve from 4.76524\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.5933 - mae: 11.8160 - val_loss: 4.8781 - val_mae: 19.4977 - lr: 2.5000e-04\n",
            "Epoch 16/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.5632 - mae: 11.9344\n",
            "Epoch 00016: val_loss improved from 4.76524 to 4.74873, saving model to /home/jupyter/IEEE_Test/fold5.h5\n",
            "30/30 [==============================] - 3s 109ms/step - loss: 3.5632 - mae: 11.9344 - val_loss: 4.7487 - val_mae: 20.3679 - lr: 2.5000e-04\n",
            "Epoch 17/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.5917 - mae: 11.9695\n",
            "Epoch 00017: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 3.5917 - mae: 11.9695 - val_loss: 4.9842 - val_mae: 19.9777 - lr: 2.5000e-04\n",
            "Epoch 18/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.5780 - mae: 11.1617\n",
            "Epoch 00018: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.5780 - mae: 11.1617 - val_loss: 4.9626 - val_mae: 20.1372 - lr: 2.5000e-04\n",
            "Epoch 19/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4475 - mae: 10.2849\n",
            "Epoch 00019: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.4475 - mae: 10.2849 - val_loss: 5.0459 - val_mae: 20.1742 - lr: 1.2500e-04\n",
            "Epoch 20/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.4386 - mae: 10.5060\n",
            "Epoch 00020: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.4386 - mae: 10.5060 - val_loss: 5.1290 - val_mae: 20.2040 - lr: 1.2500e-04\n",
            "Epoch 21/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.3791 - mae: 9.8234\n",
            "Epoch 00021: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.3791 - mae: 9.8234 - val_loss: 5.0234 - val_mae: 19.8514 - lr: 6.2500e-05\n",
            "Epoch 22/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.3457 - mae: 9.5279\n",
            "Epoch 00022: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 3.3457 - mae: 9.5279 - val_loss: 5.1057 - val_mae: 21.3202 - lr: 6.2500e-05\n",
            "Epoch 23/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.3676 - mae: 9.6949\n",
            "Epoch 00023: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 101ms/step - loss: 3.3676 - mae: 9.6949 - val_loss: 5.2163 - val_mae: 21.3934 - lr: 3.1250e-05\n",
            "Epoch 24/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.3204 - mae: 8.9690\n",
            "Epoch 00024: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 3.3204 - mae: 8.9690 - val_loss: 5.1531 - val_mae: 20.8343 - lr: 3.1250e-05\n",
            "Epoch 25/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.3248 - mae: 9.0689\n",
            "Epoch 00025: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 101ms/step - loss: 3.3248 - mae: 9.0689 - val_loss: 5.0595 - val_mae: 20.6907 - lr: 1.5625e-05\n",
            "Epoch 26/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.3019 - mae: 9.5184\n",
            "Epoch 00026: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "30/30 [==============================] - 3s 101ms/step - loss: 3.3019 - mae: 9.5184 - val_loss: 5.0982 - val_mae: 19.3850 - lr: 1.5625e-05\n",
            "Epoch 27/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.3176 - mae: 9.1349\n",
            "Epoch 00027: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.3176 - mae: 9.1349 - val_loss: 5.1292 - val_mae: 21.0052 - lr: 7.8125e-06\n",
            "Epoch 28/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.3000 - mae: 9.7150\n",
            "Epoch 00028: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "30/30 [==============================] - 3s 101ms/step - loss: 3.3000 - mae: 9.7150 - val_loss: 5.1570 - val_mae: 20.9503 - lr: 7.8125e-06\n",
            "Epoch 29/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2925 - mae: 9.0123\n",
            "Epoch 00029: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 101ms/step - loss: 3.2925 - mae: 9.0123 - val_loss: 5.0855 - val_mae: 20.4906 - lr: 3.9063e-06\n",
            "Epoch 30/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.3001 - mae: 9.2511\n",
            "Epoch 00030: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.3001 - mae: 9.2511 - val_loss: 5.0715 - val_mae: 20.6389 - lr: 3.9063e-06\n",
            "Epoch 31/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2823 - mae: 9.2608\n",
            "Epoch 00031: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.2823 - mae: 9.2608 - val_loss: 5.1334 - val_mae: 20.4914 - lr: 1.9531e-06\n",
            "Epoch 32/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.3145 - mae: 9.5167\n",
            "Epoch 00032: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 3.3145 - mae: 9.5167 - val_loss: 5.1911 - val_mae: 19.9227 - lr: 1.9531e-06\n",
            "Epoch 33/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2747 - mae: 9.2825\n",
            "Epoch 00033: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.2747 - mae: 9.2825 - val_loss: 5.0262 - val_mae: 19.4673 - lr: 9.7656e-07\n",
            "Epoch 34/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.3158 - mae: 9.6156\n",
            "Epoch 00034: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 3.3158 - mae: 9.6156 - val_loss: 5.0989 - val_mae: 20.6121 - lr: 9.7656e-07\n",
            "Epoch 35/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2801 - mae: 9.2354\n",
            "Epoch 00035: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 100ms/step - loss: 3.2801 - mae: 9.2354 - val_loss: 5.0925 - val_mae: 20.5740 - lr: 4.8828e-07\n",
            "Epoch 36/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2911 - mae: 9.5641\n",
            "Epoch 00036: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.2911 - mae: 9.5641 - val_loss: 5.0616 - val_mae: 20.6059 - lr: 4.8828e-07\n",
            "Epoch 37/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2954 - mae: 9.5803\n",
            "Epoch 00037: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.2954 - mae: 9.5803 - val_loss: 5.1220 - val_mae: 19.8451 - lr: 2.4414e-07\n",
            "Epoch 38/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2737 - mae: 8.8662\n",
            "Epoch 00038: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "30/30 [==============================] - 3s 100ms/step - loss: 3.2737 - mae: 8.8662 - val_loss: 5.2079 - val_mae: 20.8599 - lr: 2.4414e-07\n",
            "Epoch 39/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2773 - mae: 9.3372\n",
            "Epoch 00039: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 109ms/step - loss: 3.2773 - mae: 9.3372 - val_loss: 5.0283 - val_mae: 19.9338 - lr: 1.2207e-07\n",
            "Epoch 40/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2978 - mae: 8.9412\n",
            "Epoch 00040: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "30/30 [==============================] - 3s 100ms/step - loss: 3.2978 - mae: 8.9412 - val_loss: 5.0416 - val_mae: 21.2155 - lr: 1.2207e-07\n",
            "Epoch 41/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2930 - mae: 9.2734\n",
            "Epoch 00041: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 100ms/step - loss: 3.2930 - mae: 9.2734 - val_loss: 5.1964 - val_mae: 22.0350 - lr: 6.1035e-08\n",
            "Epoch 42/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2887 - mae: 9.1115\n",
            "Epoch 00042: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "30/30 [==============================] - 3s 100ms/step - loss: 3.2887 - mae: 9.1115 - val_loss: 5.1077 - val_mae: 21.4293 - lr: 6.1035e-08\n",
            "Epoch 43/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2865 - mae: 8.9794\n",
            "Epoch 00043: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 100ms/step - loss: 3.2865 - mae: 8.9794 - val_loss: 5.0704 - val_mae: 20.0255 - lr: 3.0518e-08\n",
            "Epoch 44/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2922 - mae: 9.2183\n",
            "Epoch 00044: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.2922 - mae: 9.2183 - val_loss: 5.0454 - val_mae: 20.1694 - lr: 3.0518e-08\n",
            "Epoch 45/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2972 - mae: 9.4509\n",
            "Epoch 00045: val_loss did not improve from 4.74873\n",
            "30/30 [==============================] - 3s 100ms/step - loss: 3.2972 - mae: 9.4509 - val_loss: 5.2360 - val_mae: 22.3868 - lr: 1.5259e-08\n",
            "Epoch 46/200\n",
            "30/30 [==============================] - ETA: 0s - loss: 3.2887 - mae: 8.8187Restoring model weights from the end of the best epoch: 16.\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 4.74873\n",
            "\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 3.2887 - mae: 8.8187 - val_loss: 5.0221 - val_mae: 19.9117 - lr: 1.5259e-08\n",
            "Epoch 00046: early stopping\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 5.4401 - mae: 22.3104\n",
            "Fit model on training data fold  6\n",
            "Epoch 1/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 152.4881 - mae: 112.1758\n",
            "Epoch 00001: val_loss improved from inf to 110.88754, saving model to /home/jupyter/IEEE_Test/fold6.h5\n",
            "25/25 [==============================] - 16s 229ms/step - loss: 152.4881 - mae: 112.1758 - val_loss: 110.8875 - val_mae: 104.2975 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.1875 - mae: 49.0649\n",
            "Epoch 00002: val_loss improved from 110.88754 to 9.45881, saving model to /home/jupyter/IEEE_Test/fold6.h5\n",
            "25/25 [==============================] - 3s 130ms/step - loss: 26.1875 - mae: 49.0649 - val_loss: 9.4588 - val_mae: 30.9955 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 5.9862 - mae: 18.1417\n",
            "Epoch 00003: val_loss improved from 9.45881 to 9.14153, saving model to /home/jupyter/IEEE_Test/fold6.h5\n",
            "25/25 [==============================] - 3s 130ms/step - loss: 5.9862 - mae: 18.1417 - val_loss: 9.1415 - val_mae: 29.7278 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.7171 - mae: 13.7579\n",
            "Epoch 00004: val_loss improved from 9.14153 to 8.27790, saving model to /home/jupyter/IEEE_Test/fold6.h5\n",
            "25/25 [==============================] - 3s 130ms/step - loss: 4.7171 - mae: 13.7579 - val_loss: 8.2779 - val_mae: 28.6688 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.1711 - mae: 12.2003\n",
            "Epoch 00005: val_loss improved from 8.27790 to 7.89266, saving model to /home/jupyter/IEEE_Test/fold6.h5\n",
            "25/25 [==============================] - 3s 128ms/step - loss: 4.1711 - mae: 12.2003 - val_loss: 7.8927 - val_mae: 27.3601 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.9079 - mae: 10.9604\n",
            "Epoch 00006: val_loss did not improve from 7.89266\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 3.9079 - mae: 10.9604 - val_loss: 8.0443 - val_mae: 28.4987 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.6691 - mae: 10.4915\n",
            "Epoch 00007: val_loss improved from 7.89266 to 7.78473, saving model to /home/jupyter/IEEE_Test/fold6.h5\n",
            "25/25 [==============================] - 3s 128ms/step - loss: 3.6691 - mae: 10.4915 - val_loss: 7.7847 - val_mae: 28.3629 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5586 - mae: 9.5414\n",
            "Epoch 00008: val_loss did not improve from 7.78473\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.5586 - mae: 9.5414 - val_loss: 9.3414 - val_mae: 31.6564 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5313 - mae: 9.7810\n",
            "Epoch 00009: val_loss improved from 7.78473 to 6.83752, saving model to /home/jupyter/IEEE_Test/fold6.h5\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 3.5313 - mae: 9.7810 - val_loss: 6.8375 - val_mae: 24.5877 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.4409 - mae: 8.6500\n",
            "Epoch 00010: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 3.4409 - mae: 8.6500 - val_loss: 7.9262 - val_mae: 27.2863 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.2809 - mae: 8.4736\n",
            "Epoch 00011: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.2809 - mae: 8.4736 - val_loss: 9.3328 - val_mae: 30.2014 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.2029 - mae: 7.8673\n",
            "Epoch 00012: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 3.2029 - mae: 7.8673 - val_loss: 8.1697 - val_mae: 26.3171 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1234 - mae: 7.2446\n",
            "Epoch 00013: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.1234 - mae: 7.2446 - val_loss: 9.0037 - val_mae: 27.4627 - lr: 5.0000e-04\n",
            "Epoch 14/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0600 - mae: 7.0145\n",
            "Epoch 00014: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 3.0600 - mae: 7.0145 - val_loss: 8.7116 - val_mae: 27.1977 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0332 - mae: 6.6872\n",
            "Epoch 00015: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.0332 - mae: 6.6872 - val_loss: 9.0974 - val_mae: 27.9488 - lr: 2.5000e-04\n",
            "Epoch 16/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0079 - mae: 6.9234\n",
            "Epoch 00016: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.0079 - mae: 6.9234 - val_loss: 8.9132 - val_mae: 26.8123 - lr: 1.2500e-04\n",
            "Epoch 17/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9655 - mae: 6.5676\n",
            "Epoch 00017: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 2.9655 - mae: 6.5676 - val_loss: 9.2377 - val_mae: 27.8298 - lr: 1.2500e-04\n",
            "Epoch 18/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9753 - mae: 6.6172\n",
            "Epoch 00018: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 2.9753 - mae: 6.6172 - val_loss: 9.1371 - val_mae: 27.3611 - lr: 6.2500e-05\n",
            "Epoch 19/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9284 - mae: 6.0995\n",
            "Epoch 00019: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "25/25 [==============================] - 3s 111ms/step - loss: 2.9284 - mae: 6.0995 - val_loss: 9.2623 - val_mae: 27.7291 - lr: 6.2500e-05\n",
            "Epoch 20/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9172 - mae: 5.9527\n",
            "Epoch 00020: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 2.9172 - mae: 5.9527 - val_loss: 9.4004 - val_mae: 27.8733 - lr: 3.1250e-05\n",
            "Epoch 21/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9197 - mae: 6.2541\n",
            "Epoch 00021: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "25/25 [==============================] - 3s 122ms/step - loss: 2.9197 - mae: 6.2541 - val_loss: 9.2663 - val_mae: 27.8515 - lr: 3.1250e-05\n",
            "Epoch 22/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9333 - mae: 6.3195\n",
            "Epoch 00022: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 2.9333 - mae: 6.3195 - val_loss: 9.2964 - val_mae: 27.7713 - lr: 1.5625e-05\n",
            "Epoch 23/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9215 - mae: 6.5224\n",
            "Epoch 00023: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 2.9215 - mae: 6.5224 - val_loss: 9.3327 - val_mae: 27.6557 - lr: 1.5625e-05\n",
            "Epoch 24/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9508 - mae: 5.9880\n",
            "Epoch 00024: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 2.9508 - mae: 5.9880 - val_loss: 9.2821 - val_mae: 27.4597 - lr: 7.8125e-06\n",
            "Epoch 25/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9188 - mae: 6.1070\n",
            "Epoch 00025: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 2.9188 - mae: 6.1070 - val_loss: 9.2832 - val_mae: 27.1368 - lr: 7.8125e-06\n",
            "Epoch 26/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9371 - mae: 6.0563\n",
            "Epoch 00026: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 2.9371 - mae: 6.0563 - val_loss: 9.1773 - val_mae: 27.3411 - lr: 3.9063e-06\n",
            "Epoch 27/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9156 - mae: 6.3871\n",
            "Epoch 00027: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 2.9156 - mae: 6.3871 - val_loss: 9.3984 - val_mae: 28.0007 - lr: 3.9063e-06\n",
            "Epoch 28/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9110 - mae: 5.7777\n",
            "Epoch 00028: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 2.9110 - mae: 5.7777 - val_loss: 9.1999 - val_mae: 27.8082 - lr: 1.9531e-06\n",
            "Epoch 29/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9519 - mae: 6.1293\n",
            "Epoch 00029: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 2.9519 - mae: 6.1293 - val_loss: 9.2375 - val_mae: 27.1548 - lr: 1.9531e-06\n",
            "Epoch 30/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9242 - mae: 5.9192\n",
            "Epoch 00030: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 110ms/step - loss: 2.9242 - mae: 5.9192 - val_loss: 9.1631 - val_mae: 27.0478 - lr: 9.7656e-07\n",
            "Epoch 31/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9097 - mae: 6.0884\n",
            "Epoch 00031: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 2.9097 - mae: 6.0884 - val_loss: 9.2915 - val_mae: 27.2414 - lr: 9.7656e-07\n",
            "Epoch 32/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9001 - mae: 6.2073\n",
            "Epoch 00032: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 2.9001 - mae: 6.2073 - val_loss: 9.3410 - val_mae: 27.9721 - lr: 4.8828e-07\n",
            "Epoch 33/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9045 - mae: 5.8966\n",
            "Epoch 00033: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 2.9045 - mae: 5.8966 - val_loss: 9.2668 - val_mae: 27.0353 - lr: 4.8828e-07\n",
            "Epoch 34/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9158 - mae: 6.3206\n",
            "Epoch 00034: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 2.9158 - mae: 6.3206 - val_loss: 9.3248 - val_mae: 27.3649 - lr: 2.4414e-07\n",
            "Epoch 35/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9115 - mae: 6.1361\n",
            "Epoch 00035: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 2.9115 - mae: 6.1361 - val_loss: 9.1417 - val_mae: 27.3239 - lr: 2.4414e-07\n",
            "Epoch 36/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9258 - mae: 6.2470\n",
            "Epoch 00036: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 2.9258 - mae: 6.2470 - val_loss: 9.2741 - val_mae: 27.2601 - lr: 1.2207e-07\n",
            "Epoch 37/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9165 - mae: 6.0134\n",
            "Epoch 00037: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 2.9165 - mae: 6.0134 - val_loss: 9.3410 - val_mae: 27.3226 - lr: 1.2207e-07\n",
            "Epoch 38/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.8919 - mae: 6.1157\n",
            "Epoch 00038: val_loss did not improve from 6.83752\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 2.8919 - mae: 6.1157 - val_loss: 9.2571 - val_mae: 26.9346 - lr: 6.1035e-08\n",
            "Epoch 39/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9527 - mae: 6.2935Restoring model weights from the end of the best epoch: 9.\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 6.83752\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 2.9527 - mae: 6.2935 - val_loss: 9.0986 - val_mae: 26.8973 - lr: 6.1035e-08\n",
            "Epoch 00039: early stopping\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 3.3801 - mae: 6.7621\n",
            "Fit model on training data fold  7\n",
            "Epoch 1/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 101.9581 - mae: 112.3117\n",
            "Epoch 00001: val_loss improved from inf to 29.78974, saving model to /home/jupyter/IEEE_Test/fold7.h5\n",
            "25/25 [==============================] - 16s 217ms/step - loss: 101.9581 - mae: 112.3117 - val_loss: 29.7897 - val_mae: 91.7635 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.5930 - mae: 63.4219\n",
            "Epoch 00002: val_loss improved from 29.78974 to 7.11584, saving model to /home/jupyter/IEEE_Test/fold7.h5\n",
            "25/25 [==============================] - 4s 144ms/step - loss: 15.5930 - mae: 63.4219 - val_loss: 7.1158 - val_mae: 35.0028 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 5.6436 - mae: 26.4333\n",
            "Epoch 00003: val_loss did not improve from 7.11584\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 5.6436 - mae: 26.4333 - val_loss: 7.1263 - val_mae: 39.5252 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 5.2873 - mae: 26.6185\n",
            "Epoch 00004: val_loss improved from 7.11584 to 6.90133, saving model to /home/jupyter/IEEE_Test/fold7.h5\n",
            "25/25 [==============================] - 3s 138ms/step - loss: 5.2873 - mae: 26.6185 - val_loss: 6.9013 - val_mae: 38.7890 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 5.1968 - mae: 25.6559\n",
            "Epoch 00005: val_loss improved from 6.90133 to 6.72293, saving model to /home/jupyter/IEEE_Test/fold7.h5\n",
            "25/25 [==============================] - 3s 125ms/step - loss: 5.1968 - mae: 25.6559 - val_loss: 6.7229 - val_mae: 39.5265 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 5.0056 - mae: 24.7648\n",
            "Epoch 00006: val_loss did not improve from 6.72293\n",
            "25/25 [==============================] - 3s 122ms/step - loss: 5.0056 - mae: 24.7648 - val_loss: 7.3167 - val_mae: 39.0306 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.6041 - mae: 20.4509\n",
            "Epoch 00007: val_loss improved from 6.72293 to 6.24723, saving model to /home/jupyter/IEEE_Test/fold7.h5\n",
            "25/25 [==============================] - 3s 134ms/step - loss: 4.6041 - mae: 20.4509 - val_loss: 6.2472 - val_mae: 32.3005 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.2648 - mae: 17.6547\n",
            "Epoch 00008: val_loss did not improve from 6.24723\n",
            "25/25 [==============================] - 3s 126ms/step - loss: 4.2648 - mae: 17.6547 - val_loss: 6.3112 - val_mae: 29.5811 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.1255 - mae: 16.2189\n",
            "Epoch 00009: val_loss improved from 6.24723 to 6.01710, saving model to /home/jupyter/IEEE_Test/fold7.h5\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 4.1255 - mae: 16.2189 - val_loss: 6.0171 - val_mae: 28.9867 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.0779 - mae: 15.8521\n",
            "Epoch 00010: val_loss did not improve from 6.01710\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 4.0779 - mae: 15.8521 - val_loss: 6.6508 - val_mae: 31.8255 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.0620 - mae: 15.2779\n",
            "Epoch 00011: val_loss did not improve from 6.01710\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "25/25 [==============================] - 3s 110ms/step - loss: 4.0620 - mae: 15.2779 - val_loss: 6.8297 - val_mae: 32.5838 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.8856 - mae: 14.0933\n",
            "Epoch 00012: val_loss improved from 6.01710 to 5.87733, saving model to /home/jupyter/IEEE_Test/fold7.h5\n",
            "25/25 [==============================] - 3s 114ms/step - loss: 3.8856 - mae: 14.0933 - val_loss: 5.8773 - val_mae: 26.9067 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.1356 - mae: 16.1881\n",
            "Epoch 00013: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 4.1356 - mae: 16.1881 - val_loss: 7.2014 - val_mae: 33.9651 - lr: 5.0000e-04\n",
            "Epoch 14/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.8452 - mae: 12.9619\n",
            "Epoch 00014: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.8452 - mae: 12.9619 - val_loss: 6.7868 - val_mae: 31.5268 - lr: 5.0000e-04\n",
            "Epoch 15/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.7156 - mae: 12.7302\n",
            "Epoch 00015: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.7156 - mae: 12.7302 - val_loss: 7.2193 - val_mae: 33.6806 - lr: 2.5000e-04\n",
            "Epoch 16/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.6991 - mae: 12.3802\n",
            "Epoch 00016: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 3.6991 - mae: 12.3802 - val_loss: 7.2525 - val_mae: 33.0788 - lr: 2.5000e-04\n",
            "Epoch 17/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.6490 - mae: 12.9285\n",
            "Epoch 00017: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 3.6490 - mae: 12.9285 - val_loss: 7.0814 - val_mae: 32.0770 - lr: 1.2500e-04\n",
            "Epoch 18/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5977 - mae: 11.9192\n",
            "Epoch 00018: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.5977 - mae: 11.9192 - val_loss: 7.1805 - val_mae: 31.5560 - lr: 1.2500e-04\n",
            "Epoch 19/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5804 - mae: 12.1122\n",
            "Epoch 00019: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 3.5804 - mae: 12.1122 - val_loss: 7.1469 - val_mae: 30.8374 - lr: 6.2500e-05\n",
            "Epoch 20/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5877 - mae: 11.7416\n",
            "Epoch 00020: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.5877 - mae: 11.7416 - val_loss: 7.2341 - val_mae: 32.1701 - lr: 6.2500e-05\n",
            "Epoch 21/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5616 - mae: 11.6869\n",
            "Epoch 00021: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.5616 - mae: 11.6869 - val_loss: 7.3370 - val_mae: 31.3540 - lr: 3.1250e-05\n",
            "Epoch 22/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5836 - mae: 12.0086\n",
            "Epoch 00022: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.5836 - mae: 12.0086 - val_loss: 7.2316 - val_mae: 31.5639 - lr: 3.1250e-05\n",
            "Epoch 23/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5726 - mae: 11.4748\n",
            "Epoch 00023: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.5726 - mae: 11.4748 - val_loss: 7.3032 - val_mae: 32.4718 - lr: 1.5625e-05\n",
            "Epoch 24/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5619 - mae: 11.4093\n",
            "Epoch 00024: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.5619 - mae: 11.4093 - val_loss: 7.1975 - val_mae: 32.5712 - lr: 1.5625e-05\n",
            "Epoch 25/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5767 - mae: 11.5728\n",
            "Epoch 00025: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.5767 - mae: 11.5728 - val_loss: 7.3292 - val_mae: 32.5556 - lr: 7.8125e-06\n",
            "Epoch 26/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5360 - mae: 11.6408\n",
            "Epoch 00026: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.5360 - mae: 11.6408 - val_loss: 7.2935 - val_mae: 32.9278 - lr: 7.8125e-06\n",
            "Epoch 27/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5303 - mae: 11.2328\n",
            "Epoch 00027: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.5303 - mae: 11.2328 - val_loss: 7.2428 - val_mae: 31.2297 - lr: 3.9063e-06\n",
            "Epoch 28/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5640 - mae: 11.7450\n",
            "Epoch 00028: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.5640 - mae: 11.7450 - val_loss: 7.3317 - val_mae: 32.3142 - lr: 3.9063e-06\n",
            "Epoch 29/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5444 - mae: 11.4137\n",
            "Epoch 00029: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.5444 - mae: 11.4137 - val_loss: 7.2769 - val_mae: 31.6085 - lr: 1.9531e-06\n",
            "Epoch 30/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5476 - mae: 11.6499\n",
            "Epoch 00030: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.5476 - mae: 11.6499 - val_loss: 7.2763 - val_mae: 31.7941 - lr: 1.9531e-06\n",
            "Epoch 31/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5385 - mae: 11.1847\n",
            "Epoch 00031: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.5385 - mae: 11.1847 - val_loss: 7.3007 - val_mae: 32.1239 - lr: 9.7656e-07\n",
            "Epoch 32/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5605 - mae: 12.0127\n",
            "Epoch 00032: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.5605 - mae: 12.0127 - val_loss: 7.3590 - val_mae: 32.5497 - lr: 9.7656e-07\n",
            "Epoch 33/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5606 - mae: 11.2505\n",
            "Epoch 00033: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.5606 - mae: 11.2505 - val_loss: 7.2761 - val_mae: 32.0708 - lr: 4.8828e-07\n",
            "Epoch 34/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5448 - mae: 11.5119\n",
            "Epoch 00034: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.5448 - mae: 11.5119 - val_loss: 7.2952 - val_mae: 31.9502 - lr: 4.8828e-07\n",
            "Epoch 35/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5513 - mae: 11.9227\n",
            "Epoch 00035: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.5513 - mae: 11.9227 - val_loss: 7.2733 - val_mae: 31.3624 - lr: 2.4414e-07\n",
            "Epoch 36/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5574 - mae: 11.3514\n",
            "Epoch 00036: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.5574 - mae: 11.3514 - val_loss: 7.2390 - val_mae: 31.9350 - lr: 2.4414e-07\n",
            "Epoch 37/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5510 - mae: 11.9506\n",
            "Epoch 00037: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.5510 - mae: 11.9506 - val_loss: 7.1864 - val_mae: 32.3341 - lr: 1.2207e-07\n",
            "Epoch 38/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5545 - mae: 10.5744\n",
            "Epoch 00038: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.5545 - mae: 10.5744 - val_loss: 7.3384 - val_mae: 32.0214 - lr: 1.2207e-07\n",
            "Epoch 39/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5312 - mae: 11.2150\n",
            "Epoch 00039: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 115ms/step - loss: 3.5312 - mae: 11.2150 - val_loss: 7.3401 - val_mae: 33.0241 - lr: 6.1035e-08\n",
            "Epoch 40/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5315 - mae: 11.8534\n",
            "Epoch 00040: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 3.5315 - mae: 11.8534 - val_loss: 7.2244 - val_mae: 31.4380 - lr: 6.1035e-08\n",
            "Epoch 41/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5452 - mae: 11.4879\n",
            "Epoch 00041: val_loss did not improve from 5.87733\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.5452 - mae: 11.4879 - val_loss: 7.2175 - val_mae: 32.4305 - lr: 3.0518e-08\n",
            "Epoch 42/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5406 - mae: 11.3502Restoring model weights from the end of the best epoch: 12.\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 5.87733\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.5406 - mae: 11.3502 - val_loss: 7.3234 - val_mae: 31.8419 - lr: 3.0518e-08\n",
            "Epoch 00042: early stopping\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 4.3573 - mae: 18.3033\n",
            "Fit model on training data fold  8\n",
            "Epoch 1/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 101.3953 - mae: 99.9111 \n",
            "Epoch 00001: val_loss improved from inf to 18.47413, saving model to /home/jupyter/IEEE_Test/fold8.h5\n",
            "25/25 [==============================] - 16s 223ms/step - loss: 101.3953 - mae: 99.9111 - val_loss: 18.4741 - val_mae: 43.4687 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 10.2242 - mae: 32.3094\n",
            "Epoch 00002: val_loss improved from 18.47413 to 8.21383, saving model to /home/jupyter/IEEE_Test/fold8.h5\n",
            "25/25 [==============================] - 4s 145ms/step - loss: 10.2242 - mae: 32.3094 - val_loss: 8.2138 - val_mae: 31.4883 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 5.6678 - mae: 20.3627\n",
            "Epoch 00003: val_loss improved from 8.21383 to 5.46971, saving model to /home/jupyter/IEEE_Test/fold8.h5\n",
            "25/25 [==============================] - 3s 127ms/step - loss: 5.6678 - mae: 20.3627 - val_loss: 5.4697 - val_mae: 19.8518 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.6729 - mae: 16.0646\n",
            "Epoch 00004: val_loss improved from 5.46971 to 4.57964, saving model to /home/jupyter/IEEE_Test/fold8.h5\n",
            "25/25 [==============================] - 3s 137ms/step - loss: 4.6729 - mae: 16.0646 - val_loss: 4.5796 - val_mae: 15.6497 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.2459 - mae: 13.4541\n",
            "Epoch 00005: val_loss did not improve from 4.57964\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 4.2459 - mae: 13.4541 - val_loss: 4.7367 - val_mae: 14.2771 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.9235 - mae: 12.5750\n",
            "Epoch 00006: val_loss improved from 4.57964 to 4.43037, saving model to /home/jupyter/IEEE_Test/fold8.h5\n",
            "25/25 [==============================] - 3s 135ms/step - loss: 3.9235 - mae: 12.5750 - val_loss: 4.4304 - val_mae: 13.0531 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.6956 - mae: 10.4080\n",
            "Epoch 00007: val_loss did not improve from 4.43037\n",
            "25/25 [==============================] - 3s 125ms/step - loss: 3.6956 - mae: 10.4080 - val_loss: 4.5190 - val_mae: 14.9179 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5467 - mae: 10.1743\n",
            "Epoch 00008: val_loss improved from 4.43037 to 4.09356, saving model to /home/jupyter/IEEE_Test/fold8.h5\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 3.5467 - mae: 10.1743 - val_loss: 4.0936 - val_mae: 12.1322 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.4652 - mae: 10.0855\n",
            "Epoch 00009: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 3.4652 - mae: 10.0855 - val_loss: 4.7082 - val_mae: 13.8977 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.4971 - mae: 9.2820\n",
            "Epoch 00010: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.4971 - mae: 9.2820 - val_loss: 4.2708 - val_mae: 13.1569 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.2773 - mae: 8.4385\n",
            "Epoch 00011: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.2773 - mae: 8.4385 - val_loss: 4.2769 - val_mae: 12.9502 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.2406 - mae: 8.3648\n",
            "Epoch 00012: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.2406 - mae: 8.3648 - val_loss: 4.3393 - val_mae: 13.1225 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1598 - mae: 7.8380\n",
            "Epoch 00013: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.1598 - mae: 7.8380 - val_loss: 4.2430 - val_mae: 12.4823 - lr: 2.5000e-04\n",
            "Epoch 14/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1521 - mae: 7.8268\n",
            "Epoch 00014: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.1521 - mae: 7.8268 - val_loss: 4.2299 - val_mae: 12.3280 - lr: 2.5000e-04\n",
            "Epoch 15/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0885 - mae: 7.3698\n",
            "Epoch 00015: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.0885 - mae: 7.3698 - val_loss: 4.2338 - val_mae: 12.4951 - lr: 1.2500e-04\n",
            "Epoch 16/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0685 - mae: 7.4885\n",
            "Epoch 00016: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.0685 - mae: 7.4885 - val_loss: 4.2852 - val_mae: 11.5548 - lr: 1.2500e-04\n",
            "Epoch 17/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0528 - mae: 7.3184\n",
            "Epoch 00017: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.0528 - mae: 7.3184 - val_loss: 4.2822 - val_mae: 12.0425 - lr: 6.2500e-05\n",
            "Epoch 18/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0359 - mae: 7.4079\n",
            "Epoch 00018: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.0359 - mae: 7.4079 - val_loss: 4.2890 - val_mae: 11.7066 - lr: 6.2500e-05\n",
            "Epoch 19/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0213 - mae: 7.0945\n",
            "Epoch 00019: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.0213 - mae: 7.0945 - val_loss: 4.3596 - val_mae: 12.3269 - lr: 3.1250e-05\n",
            "Epoch 20/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0274 - mae: 6.5421\n",
            "Epoch 00020: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.0274 - mae: 6.5421 - val_loss: 4.3068 - val_mae: 12.3494 - lr: 3.1250e-05\n",
            "Epoch 21/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0309 - mae: 6.8725\n",
            "Epoch 00021: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.0309 - mae: 6.8725 - val_loss: 4.3516 - val_mae: 12.6289 - lr: 1.5625e-05\n",
            "Epoch 22/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0303 - mae: 6.8499\n",
            "Epoch 00022: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.0303 - mae: 6.8499 - val_loss: 4.2864 - val_mae: 12.0180 - lr: 1.5625e-05\n",
            "Epoch 23/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0376 - mae: 6.8863\n",
            "Epoch 00023: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.0376 - mae: 6.8863 - val_loss: 4.3468 - val_mae: 12.1099 - lr: 7.8125e-06\n",
            "Epoch 24/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0124 - mae: 7.2241\n",
            "Epoch 00024: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 3.0124 - mae: 7.2241 - val_loss: 4.2769 - val_mae: 12.2192 - lr: 7.8125e-06\n",
            "Epoch 25/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0111 - mae: 7.1930\n",
            "Epoch 00025: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.0111 - mae: 7.1930 - val_loss: 4.3105 - val_mae: 11.8682 - lr: 3.9063e-06\n",
            "Epoch 26/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0155 - mae: 6.6459\n",
            "Epoch 00026: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.0155 - mae: 6.6459 - val_loss: 4.3412 - val_mae: 12.2280 - lr: 3.9063e-06\n",
            "Epoch 27/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0005 - mae: 6.6695\n",
            "Epoch 00027: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.0005 - mae: 6.6695 - val_loss: 4.3004 - val_mae: 12.0694 - lr: 1.9531e-06\n",
            "Epoch 28/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0136 - mae: 6.9337\n",
            "Epoch 00028: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.0136 - mae: 6.9337 - val_loss: 4.2754 - val_mae: 11.6962 - lr: 1.9531e-06\n",
            "Epoch 29/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9977 - mae: 6.6726\n",
            "Epoch 00029: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 2.9977 - mae: 6.6726 - val_loss: 4.2835 - val_mae: 11.7169 - lr: 9.7656e-07\n",
            "Epoch 30/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0224 - mae: 6.8327\n",
            "Epoch 00030: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.0224 - mae: 6.8327 - val_loss: 4.2822 - val_mae: 12.2161 - lr: 9.7656e-07\n",
            "Epoch 31/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0329 - mae: 6.9123\n",
            "Epoch 00031: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.0329 - mae: 6.9123 - val_loss: 4.2761 - val_mae: 11.5372 - lr: 4.8828e-07\n",
            "Epoch 32/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0264 - mae: 7.0026\n",
            "Epoch 00032: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.0264 - mae: 7.0026 - val_loss: 4.3249 - val_mae: 11.4302 - lr: 4.8828e-07\n",
            "Epoch 33/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0077 - mae: 6.9022\n",
            "Epoch 00033: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.0077 - mae: 6.9022 - val_loss: 4.3071 - val_mae: 11.6336 - lr: 2.4414e-07\n",
            "Epoch 34/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0179 - mae: 6.6890\n",
            "Epoch 00034: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.0179 - mae: 6.6890 - val_loss: 4.3018 - val_mae: 12.3367 - lr: 2.4414e-07\n",
            "Epoch 35/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0026 - mae: 6.7338\n",
            "Epoch 00035: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.0026 - mae: 6.7338 - val_loss: 4.2828 - val_mae: 11.2981 - lr: 1.2207e-07\n",
            "Epoch 36/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0066 - mae: 7.0013\n",
            "Epoch 00036: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.0066 - mae: 7.0013 - val_loss: 4.3168 - val_mae: 12.2988 - lr: 1.2207e-07\n",
            "Epoch 37/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.9969 - mae: 6.7407\n",
            "Epoch 00037: val_loss did not improve from 4.09356\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 2.9969 - mae: 6.7407 - val_loss: 4.2790 - val_mae: 11.7273 - lr: 6.1035e-08\n",
            "Epoch 38/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0113 - mae: 6.9058Restoring model weights from the end of the best epoch: 8.\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 4.09356\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.0113 - mae: 6.9058 - val_loss: 4.3492 - val_mae: 11.9280 - lr: 6.1035e-08\n",
            "Epoch 00038: early stopping\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 15.9260 - mae: 58.1296\n",
            "Fit model on training data fold  9\n",
            "Epoch 1/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 156.5745 - mae: 113.3356\n",
            "Epoch 00001: val_loss improved from inf to 153.52913, saving model to /home/jupyter/IEEE_Test/fold9.h5\n",
            "25/25 [==============================] - 15s 219ms/step - loss: 156.5745 - mae: 113.3356 - val_loss: 153.5291 - val_mae: 128.2260 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 50.8225 - mae: 74.3937\n",
            "Epoch 00002: val_loss improved from 153.52913 to 9.05679, saving model to /home/jupyter/IEEE_Test/fold9.h5\n",
            "25/25 [==============================] - 3s 126ms/step - loss: 50.8225 - mae: 74.3937 - val_loss: 9.0568 - val_mae: 23.0478 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 5.9565 - mae: 16.5464\n",
            "Epoch 00003: val_loss improved from 9.05679 to 7.11789, saving model to /home/jupyter/IEEE_Test/fold9.h5\n",
            "25/25 [==============================] - 3s 126ms/step - loss: 5.9565 - mae: 16.5464 - val_loss: 7.1179 - val_mae: 23.6829 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.4851 - mae: 13.1761\n",
            "Epoch 00004: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 115ms/step - loss: 4.4851 - mae: 13.1761 - val_loss: 7.6531 - val_mae: 26.8611 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.0920 - mae: 12.0045\n",
            "Epoch 00005: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "25/25 [==============================] - 3s 128ms/step - loss: 4.0920 - mae: 12.0045 - val_loss: 7.8716 - val_mae: 29.0789 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.6838 - mae: 9.8473\n",
            "Epoch 00006: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 126ms/step - loss: 3.6838 - mae: 9.8473 - val_loss: 7.6252 - val_mae: 28.2217 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.5480 - mae: 9.1223\n",
            "Epoch 00007: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "25/25 [==============================] - 3s 123ms/step - loss: 3.5480 - mae: 9.1223 - val_loss: 7.2575 - val_mae: 25.8399 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.4806 - mae: 9.2445\n",
            "Epoch 00008: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 110ms/step - loss: 3.4806 - mae: 9.2445 - val_loss: 8.1317 - val_mae: 29.6529 - lr: 2.5000e-04\n",
            "Epoch 9/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.3381 - mae: 8.3874\n",
            "Epoch 00009: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "25/25 [==============================] - 3s 124ms/step - loss: 3.3381 - mae: 8.3874 - val_loss: 7.6754 - val_mae: 27.2230 - lr: 2.5000e-04\n",
            "Epoch 10/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.3012 - mae: 7.7647\n",
            "Epoch 00010: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 124ms/step - loss: 3.3012 - mae: 7.7647 - val_loss: 7.7802 - val_mae: 27.8704 - lr: 1.2500e-04\n",
            "Epoch 11/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.2783 - mae: 8.2067\n",
            "Epoch 00011: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "25/25 [==============================] - 3s 110ms/step - loss: 3.2783 - mae: 8.2067 - val_loss: 7.6506 - val_mae: 27.2438 - lr: 1.2500e-04\n",
            "Epoch 12/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.2192 - mae: 7.4940\n",
            "Epoch 00012: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 123ms/step - loss: 3.2192 - mae: 7.4940 - val_loss: 7.7235 - val_mae: 27.7150 - lr: 6.2500e-05\n",
            "Epoch 13/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.2279 - mae: 7.5717\n",
            "Epoch 00013: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "25/25 [==============================] - 3s 125ms/step - loss: 3.2279 - mae: 7.5717 - val_loss: 7.7912 - val_mae: 28.1117 - lr: 6.2500e-05\n",
            "Epoch 14/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.2002 - mae: 7.2757\n",
            "Epoch 00014: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 123ms/step - loss: 3.2002 - mae: 7.2757 - val_loss: 7.7628 - val_mae: 27.9875 - lr: 3.1250e-05\n",
            "Epoch 15/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1946 - mae: 7.7549\n",
            "Epoch 00015: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "25/25 [==============================] - 3s 123ms/step - loss: 3.1946 - mae: 7.7549 - val_loss: 7.7647 - val_mae: 28.2058 - lr: 3.1250e-05\n",
            "Epoch 16/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.2120 - mae: 7.6175\n",
            "Epoch 00016: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 123ms/step - loss: 3.2120 - mae: 7.6175 - val_loss: 7.8752 - val_mae: 28.4370 - lr: 1.5625e-05\n",
            "Epoch 17/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.2099 - mae: 7.4966\n",
            "Epoch 00017: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "25/25 [==============================] - 3s 124ms/step - loss: 3.2099 - mae: 7.4966 - val_loss: 7.8947 - val_mae: 27.6323 - lr: 1.5625e-05\n",
            "Epoch 18/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1823 - mae: 8.3529\n",
            "Epoch 00018: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 3.1823 - mae: 8.3529 - val_loss: 7.8103 - val_mae: 27.6676 - lr: 7.8125e-06\n",
            "Epoch 19/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1807 - mae: 7.3821\n",
            "Epoch 00019: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 3.1807 - mae: 7.3821 - val_loss: 7.7615 - val_mae: 27.8801 - lr: 7.8125e-06\n",
            "Epoch 20/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.2050 - mae: 7.5864\n",
            "Epoch 00020: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 123ms/step - loss: 3.2050 - mae: 7.5864 - val_loss: 7.7471 - val_mae: 27.6094 - lr: 3.9063e-06\n",
            "Epoch 21/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1924 - mae: 7.7759\n",
            "Epoch 00021: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "25/25 [==============================] - 3s 123ms/step - loss: 3.1924 - mae: 7.7759 - val_loss: 7.6934 - val_mae: 26.8915 - lr: 3.9063e-06\n",
            "Epoch 22/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1912 - mae: 7.3792\n",
            "Epoch 00022: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 3.1912 - mae: 7.3792 - val_loss: 7.8121 - val_mae: 27.6730 - lr: 1.9531e-06\n",
            "Epoch 23/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.2036 - mae: 7.5568\n",
            "Epoch 00023: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 3.2036 - mae: 7.5568 - val_loss: 7.9264 - val_mae: 28.2090 - lr: 1.9531e-06\n",
            "Epoch 24/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1993 - mae: 7.8583\n",
            "Epoch 00024: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 3.1993 - mae: 7.8583 - val_loss: 7.7222 - val_mae: 26.3742 - lr: 9.7656e-07\n",
            "Epoch 25/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1907 - mae: 7.9915\n",
            "Epoch 00025: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.1907 - mae: 7.9915 - val_loss: 7.7915 - val_mae: 27.2048 - lr: 9.7656e-07\n",
            "Epoch 26/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1897 - mae: 7.7860\n",
            "Epoch 00026: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.1897 - mae: 7.7860 - val_loss: 7.7435 - val_mae: 27.2978 - lr: 4.8828e-07\n",
            "Epoch 27/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1822 - mae: 7.7503\n",
            "Epoch 00027: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.1822 - mae: 7.7503 - val_loss: 7.7742 - val_mae: 27.3203 - lr: 4.8828e-07\n",
            "Epoch 28/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.2142 - mae: 8.1691\n",
            "Epoch 00028: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.2142 - mae: 8.1691 - val_loss: 7.7517 - val_mae: 27.4993 - lr: 2.4414e-07\n",
            "Epoch 29/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1702 - mae: 7.5926\n",
            "Epoch 00029: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.1702 - mae: 7.5926 - val_loss: 7.8127 - val_mae: 27.6412 - lr: 2.4414e-07\n",
            "Epoch 30/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1523 - mae: 7.1834\n",
            "Epoch 00030: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.1523 - mae: 7.1834 - val_loss: 7.6895 - val_mae: 27.3697 - lr: 1.2207e-07\n",
            "Epoch 31/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1800 - mae: 7.1797\n",
            "Epoch 00031: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 3.1800 - mae: 7.1797 - val_loss: 7.6823 - val_mae: 26.7143 - lr: 1.2207e-07\n",
            "Epoch 32/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1761 - mae: 7.5092\n",
            "Epoch 00032: val_loss did not improve from 7.11789\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 3.1761 - mae: 7.5092 - val_loss: 7.6354 - val_mae: 27.0743 - lr: 6.1035e-08\n",
            "Epoch 33/200\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1774 - mae: 7.6202Restoring model weights from the end of the best epoch: 3.\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 7.11789\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 3.1774 - mae: 7.6202 - val_loss: 7.7683 - val_mae: 27.6590 - lr: 6.1035e-08\n",
            "Epoch 00033: early stopping\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 7.5675 - mae: 24.6569\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2160x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval Mean 26.272119064523718\n"
          ]
        }
      ]
    }
  ]
}